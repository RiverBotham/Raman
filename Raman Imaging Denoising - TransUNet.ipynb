{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "mount_file_id": "1GHdCjqh84Xexa4GTzb8qntXQ08wgcIgd",
      "authorship_tag": "ABX9TyMJAfDDUtfpmZ6TDR3SYLl7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiverBotham/Raman/blob/main/Raman%20Imaging%20Denoising%20-%20TransUNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO:\n",
        "\n",
        "\n",
        "*   Test TransUNet1D\n",
        "*   If any good try adding it to ResUNet\n",
        "\n"
      ],
      "metadata": {
        "id": "D077tOtY-b6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To save forst clone the repo\n",
        "!git config --global user.name \"RiverBotham\"\n",
        "!git config --global user.email \"river.botham@gmail.com\"\n",
        "!git config --global user.password \"MY_PASSWORD\"\n",
        "\n",
        "token = 'MY_TOKEN'\n",
        "username = 'RiverBotham'\n",
        "repo = 'Raman'\n",
        "\n",
        "!git clone https://{token}@github.com/{username}/{repo}"
      ],
      "metadata": {
        "id": "q-lUBSc7BMNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9139a17-6ef1-4bc3-e028-2b178c1b9516"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Raman'...\n",
            "remote: Enumerating objects: 107, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
            "remote: Total 107 (delta 60), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (107/107), 7.50 MiB | 7.89 MiB/s, done.\n",
            "Resolving deltas: 100% (60/60), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move into the cloned repo, then File -> Save copy in GitHub\n",
        "%cd {repo}/Denoising"
      ],
      "metadata": {
        "id": "zzLakAJlDBnY",
        "outputId": "6abe14ee-9974-442c-fbcc-9be40c53e910",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Raman/Denoising\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import datetime\n",
        "import time\n",
        "import shutil\n",
        "import argparse\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import scipy.signal\n",
        "import math\n",
        "from skimage.metrics import structural_similarity as sk_ssim\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.distributed as dist\n",
        "import torch.utils.data.distributed\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.cuda.amp as amp\n",
        "from torchvision import transforms, utils\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "# import model, dataset, utilities"
      ],
      "metadata": {
        "id": "KWeXO7nYDF8j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class MultiHeadAttention(nn.Module):\n",
        "#     def __init__(self, embedding_dim, head_num):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.head_num = head_num\n",
        "#         self.dk = (embedding_dim // head_num) ** (1 / 2)\n",
        "\n",
        "#         self.qkv_layer = nn.Linear(embedding_dim, embedding_dim * 3, bias=False)\n",
        "#         self.out_attention = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "\n",
        "#     def forward(self, x, mask=None):\n",
        "#         qkv = self.qkv_layer(x)\n",
        "\n",
        "#         query, key, value = tuple(rearrange(qkv, 'b t (d k h) -> k b h t d', k=3, h=self.head_num))\n",
        "#         energy = torch.einsum(\"... i d, ... j d -> ... i j\", query, key) * self.dk\n",
        "\n",
        "#         if mask is not None:\n",
        "#             energy = energy.masked_fill(mask, -float('inf'))\n",
        "\n",
        "#         attention = torch.softmax(energy, dim=-1)\n",
        "\n",
        "#         x = torch.einsum(\"... i j, ... j d -> ... i d\", attention, value)\n",
        "\n",
        "#         x = rearrange(x, \"b h t d -> b t (h d)\")\n",
        "#         x = self.out_attention(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class MLP(nn.Module):\n",
        "#     def __init__(self, embedding_dim, mlp_dim):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.mlp_layers = nn.Sequential(\n",
        "#             nn.Linear(embedding_dim, mlp_dim),\n",
        "#             nn.GELU(),\n",
        "#             nn.Dropout(0.1),\n",
        "#             nn.Linear(mlp_dim, embedding_dim),\n",
        "#             nn.Dropout(0.1)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.mlp_layers(x)\n",
        "\n",
        "\n",
        "# class TransformerEncoderBlock(nn.Module):\n",
        "#     def __init__(self, embedding_dim, head_num, mlp_dim):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.multi_head_attention = MultiHeadAttention(embedding_dim, head_num)\n",
        "#         self.mlp = MLP(embedding_dim, mlp_dim)\n",
        "\n",
        "#         self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
        "#         self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "#         self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         _x = self.multi_head_attention(x)\n",
        "#         _x = self.dropout(_x)\n",
        "#         x = x + _x\n",
        "#         x = self.layer_norm1(x)\n",
        "\n",
        "#         _x = self.mlp(x)\n",
        "#         x = x + _x\n",
        "#         x = self.layer_norm2(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class Transformer1D(nn.Module):\n",
        "#     def __init__(self, input_dim, embedding_dim, head_num, mlp_dim, block_num):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.embedding = nn.Linear(input_dim, embedding_dim)  # Embedding for 1D input\n",
        "#         self.transformer_blocks = nn.ModuleList([\n",
        "#             TransformerEncoderBlock(embedding_dim, head_num, mlp_dim) for _ in range(block_num)\n",
        "#         ])\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x shape: (batch_size, seq_length, input_dim)\n",
        "#         x = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
        "#         for block in self.transformer_blocks:\n",
        "#             x = block(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class EncoderBottleneck(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, stride=1, base_width=64):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.downsample = nn.Sequential(\n",
        "#             nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "#             nn.BatchNorm1d(out_channels)\n",
        "#         )\n",
        "\n",
        "#         width = int(out_channels * (base_width / 64))\n",
        "\n",
        "#         self.conv1 = nn.Conv1d(in_channels, width, kernel_size=1, stride=1, bias=False)\n",
        "#         self.norm1 = nn.BatchNorm1d(width)\n",
        "\n",
        "#         self.conv2 = nn.Conv1d(width, width, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "#         self.norm2 = nn.BatchNorm1d(width)\n",
        "\n",
        "#         self.conv3 = nn.Conv1d(width, out_channels, kernel_size=1, stride=1, bias=False)\n",
        "#         self.norm3 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x_down = self.downsample(x)\n",
        "\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.norm1(x)\n",
        "#         x = self.relu(x)\n",
        "\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.norm2(x)\n",
        "#         x = self.relu(x)\n",
        "\n",
        "#         x = self.conv3(x)\n",
        "#         x = self.norm3(x)\n",
        "#         x = x + x_down\n",
        "#         x = self.relu(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class DecoderBottleneck(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, scale_factor=2):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.upsample = nn.Upsample(scale_factor=scale_factor, mode='linear', align_corners=True)\n",
        "#         self.layer = nn.Sequential(\n",
        "#             nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm1d(out_channels),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm1d(out_channels),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x, x_concat=None):\n",
        "#         x = self.upsample(x)\n",
        "\n",
        "#         if x_concat is not None:\n",
        "#             x = torch.cat([x_concat, x], dim=1)\n",
        "\n",
        "#         x = self.layer(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, signal_length, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "#         self.norm1 = nn.BatchNorm1d(out_channels)\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "#         self.encoder1 = EncoderBottleneck(out_channels, out_channels * 2, stride=2)\n",
        "#         self.encoder2 = EncoderBottleneck(out_channels * 2, out_channels * 4, stride=2)\n",
        "#         self.encoder3 = EncoderBottleneck(out_channels * 4, out_channels * 8, stride=2)\n",
        "\n",
        "#         self.vit_signal_length = signal_length // patch_dim\n",
        "#         self.vit = Transformer1D(self.vit_signal_length, out_channels * 8, head_num, mlp_dim, block_num)\n",
        "\n",
        "#         self.conv2 = nn.Conv1d(out_channels * 8, 512, kernel_size=3, stride=1, padding=1)\n",
        "#         self.norm2 = nn.BatchNorm1d(512)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv1(x)  # Shape: (batch_size, out_channels, reduced_length)\n",
        "#         x = self.norm1(x)\n",
        "#         x1 = self.relu(x)\n",
        "\n",
        "#         x2 = self.encoder1(x1)\n",
        "#         x3 = self.encoder2(x2)\n",
        "#         x = self.encoder3(x3)\n",
        "\n",
        "#         # Reshape for transformer input\n",
        "#         x = rearrange(x, 'b c l -> b l c')  # Change to (batch_size, length, channels)\n",
        "#         x = self.vit(x)  # Pass through the transformer\n",
        "#         x = rearrange(x, 'b l c -> b c l')  # Change back to (batch_size, channels, length)\n",
        "\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.norm2(x)\n",
        "#         x = self.relu(x)\n",
        "\n",
        "#         return x, x1, x2, x3\n",
        "\n",
        "\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, out_channels, class_num):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.decoder1 = DecoderBottleneck(out_channels * 8, out_channels * 4)\n",
        "#         self.decoder2 = DecoderBottleneck(out_channels * 4, out_channels * 2)\n",
        "#         self.decoder3 = DecoderBottleneck(out_channels * 2, int(out_channels * 1 / 2))\n",
        "#         self.decoder4 = DecoderBottleneck(int(out_channels * 1 / 2), int(out_channels * 1 / 8))\n",
        "\n",
        "#         self.conv1 = nn.Conv1d(int(out_channels * 1 / 8), class_num, kernel_size=1)\n",
        "\n",
        "#     def forward(self, x, x1, x2, x3):\n",
        "#         x = self.decoder1(x, x3)\n",
        "#         x = self.decoder2(x, x2)\n",
        "#         x = self.decoder3(x, x1)\n",
        "#         x = self.decoder4(x)\n",
        "#         x = self.conv1(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class TransUNet(nn.Module):\n",
        "#     def __init__(self, signal_length, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim, class_num):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.encoder = Encoder(signal_length, in_channels, out_channels,\n",
        "#                                head_num, mlp_dim, block_num, patch_dim)\n",
        "\n",
        "#         self.decoder = Decoder(out_channels, class_num)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x, x1, x2, x3 = self.encoder(x)\n",
        "#         x = self.decoder(x, x1, x2, x3)\n",
        "\n",
        "#         return x"
      ],
      "metadata": {
        "id": "krq1LO2Sd2yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Trans U net\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import numpy as np\n",
        "# from einops import rearrange, repeat\n",
        "\n",
        "# # Multi-Head Attention for 1D\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "#     def __init__(self, embedding_dim, head_num):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.head_num = head_num\n",
        "#         self.dk = (embedding_dim // head_num) ** (1 / 2)\n",
        "\n",
        "#         self.qkv_layer = nn.Linear(embedding_dim, embedding_dim * 3, bias=False)\n",
        "#         self.out_attention = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "\n",
        "#     def forward(self, x, mask=None):\n",
        "#         qkv = self.qkv_layer(x)\n",
        "\n",
        "#         query, key, value = tuple(rearrange(qkv, 'b t (d k h ) -> k b h t d ', k=3, h=self.head_num))\n",
        "#         energy = torch.einsum(\"... i d , ... j d -> ... i j\", query, key) * self.dk\n",
        "\n",
        "#         if mask is not None:\n",
        "#             energy = energy.masked_fill(mask, -np.inf)\n",
        "\n",
        "#         attention = torch.softmax(energy, dim=-1)\n",
        "\n",
        "#         x = torch.einsum(\"... i j , ... j d -> ... i d\", attention, value)\n",
        "\n",
        "#         x = rearrange(x, \"b h t d -> b t (h d)\")\n",
        "#         x = self.out_attention(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class MLP(nn.Module):\n",
        "#     def __init__(self, embedding_dim, mlp_dim):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.mlp_layers = nn.Sequential(\n",
        "#             nn.Linear(embedding_dim, mlp_dim),\n",
        "#             nn.GELU(),\n",
        "#             nn.Dropout(0.1),\n",
        "#             nn.Linear(mlp_dim, embedding_dim),\n",
        "#             nn.Dropout(0.1)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.mlp_layers(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class TransformerEncoderBlock(nn.Module):\n",
        "#     def __init__(self, embedding_dim, head_num, mlp_dim):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.multi_head_attention = MultiHeadAttention(embedding_dim, head_num)\n",
        "#         self.mlp = MLP(embedding_dim, mlp_dim)\n",
        "\n",
        "#         self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
        "#         self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "#         self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         _x = self.multi_head_attention(x)\n",
        "#         _x = self.dropout(_x)\n",
        "#         x = x + _x\n",
        "#         x = self.layer_norm1(x)\n",
        "\n",
        "#         _x = self.mlp(x)\n",
        "#         x = x + _x\n",
        "#         x = self.layer_norm2(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class TransformerEncoder(nn.Module):\n",
        "#     def __init__(self, embedding_dim, head_num, mlp_dim, block_num=12):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.layer_blocks = nn.ModuleList(\n",
        "#             [TransformerEncoderBlock(embedding_dim, head_num, mlp_dim) for _ in range(block_num)])\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         for layer_block in self.layer_blocks:\n",
        "#             x = layer_block(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class ViT(nn.Module):\n",
        "#     def __init__(self, seq_len, in_channels, embedding_dim, head_num, mlp_dim,\n",
        "#                  block_num, patch_dim, classification=False, num_classes=1):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.patch_dim = patch_dim\n",
        "#         self.classification = classification\n",
        "#         self.num_tokens = (seq_len // patch_dim) ** 2\n",
        "#         self.token_dim = in_channels * (patch_dim ** 2)\n",
        "\n",
        "#         self.projection = nn.Linear(self.token_dim, embedding_dim)\n",
        "#         self.embedding = nn.Parameter(torch.rand(self.num_tokens + 1, embedding_dim))\n",
        "\n",
        "#         self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
        "\n",
        "#         self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "#         self.transformer = TransformerEncoder(embedding_dim, head_num, mlp_dim, block_num)\n",
        "\n",
        "#         if self.classification:\n",
        "#             self.mlp_head = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         seq_patches = rearrange(x,\n",
        "#                                 'b c (patch s) -> b (s) (patch c)',\n",
        "#                                 patch=self.patch_dim)\n",
        "\n",
        "#         batch_size, tokens, _ = seq_patches.shape\n",
        "\n",
        "#         project = self.projection(seq_patches)\n",
        "#         token = repeat(self.cls_token, 'b ... -> (b batch_size) ...',\n",
        "#                        batch_size=batch_size)\n",
        "\n",
        "#         patches = torch.cat([token, project], dim=1)\n",
        "#         patches += self.embedding[:tokens + 1, :]\n",
        "\n",
        "#         x = self.dropout(patches)\n",
        "#         x = self.transformer(x)\n",
        "#         x = self.mlp_head(x[:, 0, :]) if self.classification else x[:, 1:, :]\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "\n",
        "# class EncoderBottleneck(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, stride=1, base_width=64):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.downsample = nn.Sequential(\n",
        "#             nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "#             nn.BatchNorm1d(out_channels)\n",
        "#         )\n",
        "\n",
        "#         width = int(out_channels * (base_width / 64))\n",
        "\n",
        "#         self.conv1 = nn.Conv1d(in_channels, width, kernel_size=1, stride=1, bias=False)\n",
        "#         self.norm1 = nn.BatchNorm1d(width)\n",
        "\n",
        "#         self.conv2 = nn.Conv1d(width, width, kernel_size=3, stride=2, groups=1, padding=1, dilation=1, bias=False)\n",
        "#         self.norm2 = nn.BatchNorm1d(width)\n",
        "\n",
        "#         self.conv3 = nn.Conv1d(width, out_channels, kernel_size=1, stride=1, bias=False)\n",
        "#         self.norm3 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x_down = self.downsample(x)\n",
        "\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.norm1(x)\n",
        "#         x = self.relu(x)\n",
        "\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.norm2(x)\n",
        "#         x = self.relu(x)\n",
        "\n",
        "#         x = self.conv3(x)\n",
        "#         x = self.norm3(x)\n",
        "#         x = x + x_down\n",
        "#         x = self.relu(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class DecoderBottleneck(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, scale_factor=2):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.upsample = nn.Upsample(scale_factor=scale_factor, mode='linear', align_corners=True)\n",
        "#         self.layer = nn.Sequential(\n",
        "#             nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm1d(out_channels),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm1d(out_channels),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x, x_concat=None):\n",
        "#         x = self.upsample(x)\n",
        "\n",
        "#         if x_concat is not None:\n",
        "#             print(f\"x_concat shape: {x_concat.shape}\")\n",
        "#             print(f\"x shape: {x.shape}\")\n",
        "#             x = torch.cat([x_concat, x], dim=1)\n",
        "\n",
        "#         x = self.layer(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, seq_len, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "#         self.norm1 = nn.BatchNorm1d(out_channels)\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "#         self.encoder1 = EncoderBottleneck(out_channels, out_channels * 2, stride=2)\n",
        "#         self.encoder2 = EncoderBottleneck(out_channels * 2, out_channels * 4, stride=2)\n",
        "#         self.encoder3 = EncoderBottleneck(out_channels * 4, out_channels * 8, stride=2)\n",
        "\n",
        "#         self.vit_seq_len = seq_len // patch_dim\n",
        "#         self.vit = ViT(self.vit_seq_len, out_channels * 8, out_channels * 8,\n",
        "#                        head_num, mlp_dim, block_num, patch_dim=1, classification=False)\n",
        "\n",
        "#         self.conv2 = nn.Conv1d(out_channels * 8, 512, kernel_size=3, stride=1, padding=1)\n",
        "#         self.norm2 = nn.BatchNorm1d(512)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.norm1(x)\n",
        "#         x1 = self.relu(x)\n",
        "\n",
        "#         x2 = self.encoder1(x1)\n",
        "#         x3 = self.encoder2(x2)\n",
        "#         x = self.encoder3(x3)\n",
        "\n",
        "#         x = self.vit(x)\n",
        "#         x = rearrange(x, \"b c l -> b l c\")\n",
        "\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.norm2(x)\n",
        "#         x = self.relu(x)\n",
        "\n",
        "#         return x, x1, x2, x3\n",
        "\n",
        "\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, out_channels, class_num):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.decoder1 = DecoderBottleneck(out_channels * 8, out_channels * 2)\n",
        "#         self.decoder2 = DecoderBottleneck(out_channels * 4, out_channels)\n",
        "#         self.decoder3 = DecoderBottleneck(out_channels * 2, int(out_channels * 1 / 2))\n",
        "#         self.decoder4 = DecoderBottleneck(int(out_channels * 1 / 2), int(out_channels * 1 / 8))\n",
        "\n",
        "#         self.conv1 = nn.Conv1d(int(out_channels * 1 / 8), class_num, kernel_size=1)\n",
        "\n",
        "#     def forward(self, x, x1, x2, x3):\n",
        "#         print(f\"Initial x1 shape: {x1.shape}, x2 shape: {x2.shape}\")\n",
        "#         print(f\"Initial x shape: {x.shape}, x3 shape: {x3.shape}\")\n",
        "#         x = self.decoder1(x, x3)\n",
        "#         print(f\"After decoder 1 x shape: {x.shape}, x2 shape: {x2.shape}\")\n",
        "#         x = self.decoder2(x, x2)\n",
        "#         print(f\"After decoder 2 x shape: {x.shape}, x1 shape: {x1.shape}\")\n",
        "#         x = self.decoder3(x, x1)\n",
        "#         print(f\"After decoder 3 x shape: {x.shape}\")\n",
        "#         x = self.decoder4(x)\n",
        "#         print(f\"Final decoder output shape before conv1: {x.shape}\")\n",
        "#         x = self.conv1(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class TransUNet(nn.Module):\n",
        "#     def __init__(self, img_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim, class_num):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.encoder = Encoder(img_dim, in_channels, out_channels,\n",
        "#                                head_num, mlp_dim, block_num, patch_dim)\n",
        "\n",
        "#         self.decoder = Decoder(out_channels, class_num)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x, x1, x2, x3 = self.encoder(x)\n",
        "#         x = self.decoder(x, x1, x2, x3)\n",
        "\n",
        "#         return x"
      ],
      "metadata": {
        "id": "XW9JYATbltWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from einops import rearrange, repeat\n",
        "\n",
        "# # Multi-Head Attention for 1D\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "#     def __init__(self, embedding_dim, head_num):\n",
        "#         super().__init__()\n",
        "#         self.head_num = head_num\n",
        "#         self.dk = (embedding_dim // head_num) ** (1 / 2)\n",
        "#         self.qkv_layer = nn.Linear(embedding_dim, embedding_dim * 3, bias=False)\n",
        "#         self.out_attention = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "\n",
        "#     def forward(self, x, mask=None):\n",
        "#         qkv = self.qkv_layer(x)\n",
        "#         query, key, value = tuple(rearrange(qkv, 'b t (d k h) -> k b h t d', k=3, h=self.head_num))\n",
        "#         energy = torch.einsum(\"... i d , ... j d -> ... i j\", query, key) * self.dk\n",
        "\n",
        "#         if mask is not None:\n",
        "#             energy = energy.masked_fill(mask, float('-inf'))\n",
        "\n",
        "#         attention = torch.softmax(energy, dim=-1)\n",
        "#         x = torch.einsum(\"... i j , ... j d -> ... i d\", attention, value)\n",
        "#         x = rearrange(x, \"b h t d -> b t (h d)\")\n",
        "#         x = self.out_attention(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# # MLP for Feedforward\n",
        "# class MLP(nn.Module):\n",
        "#     def __init__(self, embedding_dim, mlp_dim):\n",
        "#         super().__init__()\n",
        "#         self.mlp_layers = nn.Sequential(\n",
        "#             nn.Linear(embedding_dim, mlp_dim),\n",
        "#             nn.GELU(),\n",
        "#             nn.Dropout(0.1),\n",
        "#             nn.Linear(mlp_dim, embedding_dim),\n",
        "#             nn.Dropout(0.1)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.mlp_layers(x)\n",
        "\n",
        "# # Transformer Encoder Block\n",
        "# class TransformerEncoderBlock(nn.Module):\n",
        "#     def __init__(self, embedding_dim, head_num, mlp_dim):\n",
        "#         super().__init__()\n",
        "#         self.multi_head_attention = MultiHeadAttention(embedding_dim, head_num)\n",
        "#         self.mlp = MLP(embedding_dim, mlp_dim)\n",
        "#         self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
        "#         self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
        "#         self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         _x = self.multi_head_attention(x)\n",
        "#         _x = self.dropout(_x)\n",
        "#         x = x + _x\n",
        "#         x = self.layer_norm1(x)\n",
        "#         _x = self.mlp(x)\n",
        "#         x = x + _x\n",
        "#         x = self.layer_norm2(x)\n",
        "#         return x\n",
        "\n",
        "# # Transformer Encoder\n",
        "# class TransformerEncoder(nn.Module):\n",
        "#     def __init__(self, embedding_dim, head_num, mlp_dim, block_num=12):\n",
        "#         super().__init__()\n",
        "#         self.layer_blocks = nn.ModuleList(\n",
        "#             [TransformerEncoderBlock(embedding_dim, head_num, mlp_dim) for _ in range(block_num)]\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         for layer_block in self.layer_blocks:\n",
        "#             x = layer_block(x)\n",
        "#         return x\n",
        "\n",
        "# # Vision Transformer\n",
        "# class ViT(nn.Module):\n",
        "#     def __init__(self, seq_len, in_channels, embedding_dim, head_num, mlp_dim, block_num, patch_dim):\n",
        "#         super().__init__()\n",
        "#         self.patch_dim = patch_dim\n",
        "#         self.num_tokens = (seq_len // patch_dim)\n",
        "#         self.token_dim = in_channels * (patch_dim)\n",
        "\n",
        "#         self.projection = nn.Linear(self.token_dim, embedding_dim)\n",
        "#         self.embedding = nn.Parameter(torch.rand(self.num_tokens + 1, embedding_dim))\n",
        "#         self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
        "#         self.dropout = nn.Dropout(0.1)\n",
        "#         self.transformer = TransformerEncoder(embedding_dim, head_num, mlp_dim, block_num)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         seq_patches = rearrange(x, 'b c (patch s) -> b (s) (patch c)', patch=self.patch_dim)\n",
        "#         batch_size, tokens, _ = seq_patches.shape\n",
        "#         project = self.projection(seq_patches)\n",
        "#         token = repeat(self.cls_token, 'b ... -> (b batch_size) ...', batch_size=batch_size)\n",
        "\n",
        "#         patches = torch.cat([token, project], dim=1)\n",
        "#         patches += self.embedding[:tokens + 1, :]\n",
        "#         x = self.dropout(patches)\n",
        "#         x = self.transformer(x)\n",
        "#         return x[:, 1:, :]  # Return the patch tokens only\n",
        "\n",
        "# # Encoder Bottleneck\n",
        "# class EncoderBottleneck(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, stride=1):\n",
        "#         super().__init__()\n",
        "#         self.downsample = nn.Sequential(\n",
        "#             nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "#             nn.BatchNorm1d(out_channels)\n",
        "#         )\n",
        "\n",
        "#         self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "#         self.norm1 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "#         self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "#         self.norm2 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x_down = self.downsample(x)\n",
        "\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.norm1(x)\n",
        "#         x = self.relu(x)\n",
        "\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.norm2(x)\n",
        "#         x = x + x_down\n",
        "#         x = self.relu(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# # Encoder\n",
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, spectrum_len, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim):\n",
        "#         super().__init__()\n",
        "#         self.bottleneck1 = EncoderBottleneck(in_channels, out_channels)        # Output shape will be [batch_size, out_channels, spectrum_len / 2]\n",
        "#         self.bottleneck2 = EncoderBottleneck(out_channels, out_channels * 2)  # Output shape will be [batch_size, out_channels * 2, spectrum_len / 4]\n",
        "#         self.bottleneck3 = EncoderBottleneck(out_channels * 2, out_channels * 4)  # Output shape will be [batch_size, out_channels * 4, spectrum_len / 8]\n",
        "\n",
        "#         self.transformer = ViT(spectrum_len // 8, out_channels * 4, out_channels * 4, head_num, mlp_dim, block_num, patch_dim)  # Adjusting input dimensions for ViT\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         print(f\"Input shape: {x.shape}\")\n",
        "\n",
        "#         x1 = self.bottleneck1(x)\n",
        "#         print(f\"After bottleneck1 shape: {x1.shape}\")\n",
        "\n",
        "#         x2 = self.bottleneck2(x1)\n",
        "#         print(f\"After bottleneck2 shape: {x2.shape}\")\n",
        "\n",
        "#         x3 = self.bottleneck3(x2)\n",
        "#         print(f\"After bottleneck3 shape: {x3.shape}\")\n",
        "\n",
        "#         # Preparing for ViT\n",
        "#         x3_reshaped = rearrange(x3, 'b c t -> b t c')  # Reshape for ViT input\n",
        "#         print(f\"Reshaped x3 for ViT: {x3_reshaped.shape}\")\n",
        "\n",
        "#         x3_transformed = self.transformer(x3_reshaped)  # Pass through the ViT\n",
        "#         print(f\"Output shape from ViT: {x3_transformed.shape}\")\n",
        "\n",
        "#         return x3_transformed, x1, x2, x3\n",
        "\n",
        "# # Decoder Bottleneck\n",
        "# class DecoderBottleneck(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, scale_factor=2):\n",
        "#         super().__init__()\n",
        "#         self.upsample = nn.Upsample(scale_factor=scale_factor, mode='linear', align_corners=True)\n",
        "#         self.layer = nn.Sequential(\n",
        "#             nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm1d(out_channels),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm1d(out_channels),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x, x_concat=None):\n",
        "#         x = self.upsample(x)\n",
        "#         if x_concat is not None:\n",
        "#             # Ensure sizes match before concatenation\n",
        "#             if x_concat.size(2) != x.size(2):\n",
        "#                 diff = x_concat.size(2) - x.size(2)\n",
        "#                 x_concat = F.pad(x_concat, (0, -diff, 0, 0))  # Pad x_concat\n",
        "#             x = torch.cat((x, x_concat), dim=1)\n",
        "#         return self.layer(x)\n",
        "\n",
        "# # Decoder\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, out_channels, num_classes):\n",
        "#         super().__init__()\n",
        "#         self.decoder3 = DecoderBottleneck(out_channels * 4, out_channels * 2)\n",
        "#         self.decoder2 = DecoderBottleneck(out_channels * 2 * 2, out_channels)  # *2 for concatenation\n",
        "#         self.decoder1 = DecoderBottleneck(out_channels * 2, out_channels)  # *2 for concatenation\n",
        "#         self.final_conv = nn.Conv1d(out_channels, num_classes, kernel_size=1)\n",
        "\n",
        "#     def forward(self, x, x1, x2, x3):\n",
        "#         print(f\"Input to decoder: {x.shape}\")\n",
        "#         x = self.decoder3(x, x3)\n",
        "#         print(f\"After decoder3 shape: {x.shape}\")\n",
        "#         x = self.decoder2(x, x2)\n",
        "#         print(f\"After decoder2 shape: {x.shape}\")\n",
        "#         x = self.decoder1(x, x1)\n",
        "#         print(f\"After decoder1 shape: {x.shape}\")\n",
        "#         return self.final_conv(x)\n",
        "\n",
        "# # Full TransUNet Model\n",
        "# class TransUNet(nn.Module):\n",
        "#     def __init__(self, spectrum_len, in_channels, out_channels, head_num, mlp_dim, block_num, num_classes, patch_dim):\n",
        "#         super().__init__()\n",
        "#         self.encoder = Encoder(spectrum_len, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim)\n",
        "#         self.decoder = Decoder(out_channels, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x, x1, x2, x3 = self.encoder(x)\n",
        "#         return self.decoder(x, x1, x2, x3)"
      ],
      "metadata": {
        "id": "BzqncE9qVnPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class MultiHeadSelfAttention(nn.Module):\n",
        "#     def __init__(self, embed_dim, num_heads):\n",
        "#         super(MultiHeadSelfAttention, self).__init__()\n",
        "#         self.num_heads = num_heads\n",
        "#         self.head_dim = embed_dim // num_heads\n",
        "\n",
        "#         assert (\n",
        "#             self.head_dim * num_heads == embed_dim\n",
        "#         ), \"Embedding dimension must be divisible by number of heads\"\n",
        "\n",
        "#         self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "#         self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "#         self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "#         self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         N, seq_length, embed_dim = x.shape\n",
        "#         q = self.q_linear(x).view(N, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # (N, num_heads, seq_length, head_dim)\n",
        "#         k = self.k_linear(x).view(N, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # (N, num_heads, seq_length, head_dim)\n",
        "#         v = self.v_linear(x).view(N, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # (N, num_heads, seq_length, head_dim)\n",
        "\n",
        "#         # Calculate attention scores\n",
        "#         energy = torch.einsum(\"nqhd,nkhd->nqkhd\", [q, k])  # (N, num_heads, seq_length, seq_length)\n",
        "#         attention = F.softmax(energy / (self.head_dim ** 0.5), dim=3)\n",
        "\n",
        "#         out = torch.einsum(\"nqkhd,nkhd->nqhd\", [attention, v]).reshape(N, seq_length, embed_dim)\n",
        "#         return self.fc_out(out)\n",
        "\n",
        "# class EncoderBlock(nn.Module):\n",
        "#     def __init__(self, in_channels, embed_dim):\n",
        "#         super(EncoderBlock, self).__init__()\n",
        "#         self.conv = nn.Conv1d(in_channels, embed_dim, kernel_size=3, padding=1)\n",
        "#         self.attn = MultiHeadSelfAttention(embed_dim, num_heads=4)\n",
        "#         self.pool = nn.MaxPool1d(2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.conv(x))  # Convolution layer\n",
        "#         x = x.permute(0, 2, 1)  # Change to (B, L, C) for attention\n",
        "#         x = self.attn(x)  # Apply Multi-Head Self-Attention\n",
        "#         x = x.permute(0, 2, 1)  # Change back to (B, C, L)\n",
        "#         skip = x  # Skip connection\n",
        "#         x = self.pool(x)\n",
        "#         return x, skip\n",
        "\n",
        "# class DecoderBlock(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels):\n",
        "#         super(DecoderBlock, self).__init__()\n",
        "#         self.upconv = nn.ConvTranspose1d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "#         self.conv = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "#     def forward(self, x, skip):\n",
        "#         x = self.upconv(x)\n",
        "\n",
        "#         # Adjust the size of skip if necessary\n",
        "#         if x.size(1) != skip.size(1):  # Check number of channels\n",
        "#             # Create a Conv1d layer with the same dtype as skip\n",
        "#             conv_adjust = nn.Conv1d(skip.size(1), x.size(1), kernel_size=1).to(skip.dtype)\n",
        "#             skip = conv_adjust(skip)  # Adjust channels with 1x1 convolution\n",
        "\n",
        "#         if x.size(2) != skip.size(2):  # Check length\n",
        "#             skip = skip[:, :, :x.size(2)]  # Trim the skip connection if necessary\n",
        "\n",
        "#         x = x + skip  # Skip connection\n",
        "#         x = F.relu(self.conv(x))\n",
        "#         return x\n",
        "\n",
        "# class TransUNetBasic(nn.Module):\n",
        "#     def __init__(self, in_channels=1, out_channels=1):\n",
        "#         super(TransUNetBasic, self).__init__()\n",
        "#         self.encoder1 = EncoderBlock(in_channels, 64)\n",
        "#         self.encoder2 = EncoderBlock(64, 128)\n",
        "#         self.encoder3 = EncoderBlock(128, 256)\n",
        "\n",
        "#         self.decoder3 = DecoderBlock(256, 128)\n",
        "#         self.decoder2 = DecoderBlock(128, 64)\n",
        "#         self.decoder1 = DecoderBlock(64, out_channels)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         enc1, skip1 = self.encoder1(x)\n",
        "#         enc2, skip2 = self.encoder2(enc1)\n",
        "#         enc3, skip3 = self.encoder3(enc2)\n",
        "\n",
        "#         dec3 = self.decoder3(enc3, skip3)\n",
        "#         dec2 = self.decoder2(dec3, skip2)\n",
        "#         dec1 = self.decoder1(dec2, skip1)\n",
        "\n",
        "#         return dec1"
      ],
      "metadata": {
        "id": "5QaKHb68aI8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# class PatchEmbedding1D(nn.Module):\n",
        "#     def __init__(self, in_channels, patch_size, embed_dim):\n",
        "#         super(PatchEmbedding1D, self).__init__()\n",
        "#         self.patch_size = patch_size\n",
        "#         self.conv = nn.Conv1d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x: (batch_size, in_channels, signal_length)\n",
        "#         x = self.conv(x)  # (batch_size, embed_dim, signal_length // patch_size)\n",
        "#         x = x.permute(0, 2, 1)  # (batch_size, num_patches, embed_dim)\n",
        "#         return x\n",
        "\n",
        "# class TransformerEncoderLayer1D(nn.Module):\n",
        "#     def __init__(self, embed_dim, num_heads, dropout=0.1, dim_feedforward=2048):\n",
        "#         super(TransformerEncoderLayer1D, self).__init__()\n",
        "#         self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "#         self.linear1 = nn.Linear(embed_dim, dim_feedforward)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.linear2 = nn.Linear(dim_feedforward, embed_dim)\n",
        "#         self.norm1 = nn.LayerNorm(embed_dim)\n",
        "#         self.norm2 = nn.LayerNorm(embed_dim)\n",
        "#         self.dropout1 = nn.Dropout(dropout)\n",
        "#         self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, src):\n",
        "#         # src: (num_patches, batch_size, embed_dim)\n",
        "#         src2 = self.self_attn(src, src, src)[0]\n",
        "#         src = src + self.dropout1(src2)\n",
        "#         src = self.norm1(src)\n",
        "#         src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
        "#         src = src + self.dropout2(src2)\n",
        "#         src = self.norm2(src)\n",
        "#         return src\n",
        "\n",
        "# class TransformerEncoder1D(nn.Module):\n",
        "#     def __init__(self, embed_dim, num_heads, depth, dropout=0.1):\n",
        "#         super(TransformerEncoder1D, self).__init__()\n",
        "#         self.layers = nn.ModuleList([\n",
        "#             TransformerEncoderLayer1D(embed_dim, num_heads, dropout) for _ in range(depth)\n",
        "#         ])\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x: (batch_size, num_patches, embed_dim)\n",
        "#         x = x.permute(1, 0, 2)  # (num_patches, batch_size, embed_dim)\n",
        "#         for layer in self.layers:\n",
        "#             x = layer(x)\n",
        "#         x = x.permute(1, 0, 2)  # (batch_size, num_patches, embed_dim)\n",
        "#         return x\n",
        "\n",
        "# class UpSampleBlock1D(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels):\n",
        "#         super(UpSampleBlock1D, self).__init__()\n",
        "#         self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "#         self.up = nn.ConvTranspose1d(out_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.conv(x))\n",
        "#         x = self.up(x)\n",
        "#         return x\n",
        "\n",
        "# class TransUNet1D(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, embed_dim, num_heads, depth, patch_size):\n",
        "#         super(TransUNet1D, self).__init__()\n",
        "#         self.patch_embed = PatchEmbedding1D(in_channels, patch_size, embed_dim)\n",
        "#         self.transformer_encoder = TransformerEncoder1D(embed_dim, num_heads, depth)\n",
        "#         self.decoder1 = UpSampleBlock1D(embed_dim, embed_dim // 2)\n",
        "#         self.decoder2 = UpSampleBlock1D(embed_dim // 2, embed_dim // 4)\n",
        "#         self.output_conv = nn.Conv1d(embed_dim // 4, out_channels, kernel_size=1)\n",
        "#         self.up_final = nn.ConvTranspose1d(16, 16, kernel_size=2, stride=2, padding=150)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         print(f\"Input size: {x.shape}\")\n",
        "#         # x: (batch_size, in_channels, signal_length)\n",
        "#         x = self.patch_embed(x)  # (batch_size, num_patches, embed_dim)\n",
        "#         print(f\"after embedding, expect (batch_size, num_patches, embed_dim) size: {x.shape}\")\n",
        "#         x = self.transformer_encoder(x)  # (batch_size, num_patches, embed_dim)\n",
        "#         print(f\"after transformer_encoder, expect (batch_size, num_patches, embed_dim) size: {x.shape}\")\n",
        "\n",
        "#         # Reshape back to 1D format for the decoder\n",
        "#         batch_size, num_patches, embed_dim = x.shape\n",
        "#         x = x.permute(0, 2, 1)  # (batch_size, embed_dim, num_patches)\n",
        "#         print(f\"after permute, expect (batch_size, embed_dim, num_patches) size: {x.shape}\")\n",
        "#         x = F.interpolate(x, scale_factor=2, mode='linear', align_corners=False)  # Upsample\n",
        "#         print(f\"after interpolate size: {x.shape}\")\n",
        "\n",
        "#         # Decoder: Upsample and reconstruct the signal\n",
        "#         x = self.decoder1(x)  # (batch_size, embed_dim // 2, upsampled_length)\n",
        "#         print(f\"after decoder1 expect (batch_size, embed_dim // 2, upsampled_length) size: {x.shape}\")\n",
        "#         x = self.decoder2(x)  # (batch_size, embed_dim // 4, further_upsampled_length)\n",
        "#         print(f\"after decoder2 expect (batch_size, embed_dim // 4, further_upsampled_length) size: {x.shape}\")\n",
        "#         x = self.up_final(x)\n",
        "#         print(f\"after up_final expect (batch_size, 16, original_length) size: {x.shape}\")\n",
        "#         x = self.output_conv(x)  # (batch_size, out_channels, original_length)\n",
        "#         print(f\"after output_conv expect (batch_size, out_channels, original_length) size: {x.shape}\")\n",
        "\n",
        "#         return x"
      ],
      "metadata": {
        "id": "mAPfA0BJtsp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding1D(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size, embed_dim):\n",
        "        super(PatchEmbedding1D, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.conv = nn.Conv1d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, in_channels, signal_length)\n",
        "        x = self.conv(x)  # (batch_size, embed_dim, signal_length // patch_size)\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, num_patches, embed_dim)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderLayer1D(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1, dim_feedforward=2048):\n",
        "        super(TransformerEncoderLayer1D, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(embed_dim, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, embed_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: (num_patches, batch_size, embed_dim)\n",
        "        src2 = self.self_attn(src, src, src)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "class TransformerEncoder1D(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, depth, dropout=0.1):\n",
        "        super(TransformerEncoder1D, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer1D(embed_dim, num_heads, dropout) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, num_patches, embed_dim)\n",
        "        x = x.permute(1, 0, 2)  # (num_patches, batch_size, embed_dim)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = x.permute(1, 0, 2)  # (batch_size, num_patches, embed_dim)\n",
        "        return x\n",
        "\n",
        "class UpSampleBlock1D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UpSampleBlock1D, self).__init__()\n",
        "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.up = nn.ConvTranspose1d(out_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv(x))\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "class TransUNet1D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, embed_dim, num_heads, depth, patch_size):\n",
        "        super(TransUNet1D, self).__init__()\n",
        "        self.patch_embed = PatchEmbedding1D(in_channels, patch_size, embed_dim)\n",
        "        self.transformer_encoder = TransformerEncoder1D(embed_dim, num_heads, depth)\n",
        "        self.decoder1 = UpSampleBlock1D(embed_dim, embed_dim // 2)\n",
        "        self.decoder2 = UpSampleBlock1D(embed_dim // 2, embed_dim // 4)\n",
        "        self.output_conv = nn.Conv1d(embed_dim // 4, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(f\"Input size: {x.shape}\")\n",
        "        # x: (batch_size, in_channels, signal_length)\n",
        "        x = self.patch_embed(x)  # (batch_size, num_patches, embed_dim)\n",
        "        # print(f\"after embedding, expect (batch_size, num_patches, embed_dim) size: {x.shape}\")\n",
        "        x = self.transformer_encoder(x)  # (batch_size, num_patches, embed_dim)\n",
        "        # print(f\"after transformer_encoder, expect (batch_size, num_patches, embed_dim) size: {x.shape}\")\n",
        "\n",
        "        # Reshape back to 1D format for the decoder\n",
        "        batch_size, num_patches, embed_dim = x.shape\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, embed_dim, num_patches)\n",
        "        # print(f\"after permute, expect (batch_size, embed_dim, num_patches) size: {x.shape}\")\n",
        "        x = F.interpolate(x, scale_factor=2, mode='linear', align_corners=False)  # Upsample\n",
        "        # print(f\"after interpolate size: {x.shape}\")\n",
        "\n",
        "        # Decoder: Upsample and reconstruct the signal\n",
        "        x = self.decoder1(x)  # (batch_size, embed_dim // 2, upsampled_length)\n",
        "        # print(f\"after decoder1 expect (batch_size, embed_dim // 2, upsampled_length) size: {x.shape}\")\n",
        "        x = self.decoder2(x)  # (batch_size, embed_dim // 4, further_upsampled_length)\n",
        "        # print(f\"after decoder2 expect (batch_size, embed_dim // 4, further_upsampled_length) size: {x.shape}\")\n",
        "        x = F.interpolate(x, size=500, mode='linear', align_corners=True)\n",
        "        # print(f\"after up_final expect (batch_size, 16, original_length) size: {x.shape}\")\n",
        "        x = self.output_conv(x)  # (batch_size, out_channels, original_length)\n",
        "        # print(f\"after output_conv expect (batch_size, out_channels, original_length) size: {x.shape}\")\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "W1Xw1Sji0sMD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data set\n",
        "\n",
        "class RamanDataset(Dataset):\n",
        "    def __init__(self, inputs, outputs, batch_size=64,spectrum_len=500, spectrum_shift=0.,\n",
        "                 spectrum_window=False, horizontal_flip=False, mixup=False):\n",
        "        self.inputs = inputs\n",
        "        self.outputs = outputs\n",
        "        self.batch_size = batch_size\n",
        "        self.spectrum_len = spectrum_len\n",
        "        self.spectrum_shift = spectrum_shift\n",
        "        self.spectrum_window = spectrum_window\n",
        "        self.horizontal_flip = horizontal_flip\n",
        "        self.mixup = mixup\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def pad_spectrum(self, input_spectrum, spectrum_length):\n",
        "        if len(input_spectrum) == spectrum_length:\n",
        "            padded_spectrum = input_spectrum\n",
        "        elif len(input_spectrum) > spectrum_length:\n",
        "            padded_spectrum = input_spectrum[0:spectrum_length]\n",
        "        else:\n",
        "            padded_spectrum = np.pad(input_spectrum, ((0,spectrum_length - len(input_spectrum)),(0,0)), 'reflect')\n",
        "\n",
        "        return padded_spectrum\n",
        "\n",
        "    def window_spectrum(self, input_spectrum, start_idx, window_length):\n",
        "        if len(input_spectrum) <= window_length:\n",
        "            output_spectrum = input_spectrum\n",
        "        else:\n",
        "            end_idx = start_idx + window_length\n",
        "            output_spectrum = input_spectrum[start_idx:end_idx]\n",
        "\n",
        "        return output_spectrum\n",
        "\n",
        "    def flip_axis(self, x, axis):\n",
        "        if np.random.random() < 0.5:\n",
        "            x = np.asarray(x).swapaxes(axis, 0)\n",
        "            x = x[::-1, ...]\n",
        "            x = x.swapaxes(0, axis)\n",
        "        return x\n",
        "\n",
        "    def shift_spectrum(self, x, shift_range):\n",
        "        x = np.expand_dims(x,axis=-1)\n",
        "        shifted_spectrum = x\n",
        "        spectrum_shift_range = int(np.round(shift_range*len(x)))\n",
        "        if spectrum_shift_range > 0:\n",
        "            shifted_spectrum = np.pad(x[spectrum_shift_range:,:], ((0,abs(spectrum_shift_range)), (0,0)), 'reflect')\n",
        "        elif spectrum_shift_range < 0:\n",
        "            shifted_spectrum = np.pad(x[:spectrum_shift_range,:], ((abs(spectrum_shift_range), 0), (0,0)), 'reflect')\n",
        "        return shifted_spectrum\n",
        "\n",
        "    def mixup_spectrum(self, input_spectrum1, input_spectrum2, output_spectrum1, output_spectrum2, alpha):\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        input_spectrum = (lam * input_spectrum1) + ((1 - lam) * input_spectrum2)\n",
        "        output_spectrum = (lam * output_spectrum1) + ((1 - lam) * output_spectrum2)\n",
        "        return input_spectrum, output_spectrum\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_spectrum = self.inputs[index]\n",
        "        output_spectrum = self.outputs[index]\n",
        "\n",
        "        mixup_on = False\n",
        "        if self.mixup:\n",
        "            if np.random.random() < 0.5:\n",
        "                spectrum_idx = int(np.round(np.random.random() * (len(self.inputs)-1)))\n",
        "                input_spectrum2 = self.inputs[spectrum_idx]\n",
        "                output_spectrum2 = self.outputs[spectrum_idx]\n",
        "                mixup_on = True\n",
        "\n",
        "        if self.spectrum_window:\n",
        "            start_idx = int(np.floor(np.random.random() * (len(input_spectrum)-self.spectrum_len)))\n",
        "            input_spectrum = self.window_spectrum(input_spectrum, start_idx, self.spectrum_len)\n",
        "            output_spectrum = self.window_spectrum(output_spectrum, start_idx, self.spectrum_len)\n",
        "            if mixup_on:\n",
        "                input_spectrum2 = self.window_spectrum(input_spectrum2, start_idx, self.spectrum_len)\n",
        "                output_spectrum2 = self.window_spectrum(output_spectrum2, start_idx, self.spectrum_len)\n",
        "\n",
        "        input_spectrum = self.pad_spectrum(input_spectrum, self.spectrum_len)\n",
        "        output_spectrum = self.pad_spectrum(output_spectrum, self.spectrum_len)\n",
        "        if mixup_on:\n",
        "            input_spectrum2 = self.pad_spectrum(input_spectrum2, self.spectrum_len)\n",
        "            output_spectrum2 = self.pad_spectrum(output_spectrum2, self.spectrum_len)\n",
        "\n",
        "        if self.spectrum_shift != 0.0:\n",
        "            shift_range = np.random.uniform(-self.spectrum_shift, self.spectrum_shift)\n",
        "            input_spectrum = self.shift_spectrum(input_spectrum, shift_range)\n",
        "            output_spectrum = self.shift_spectrum(output_spectrum, shift_range)\n",
        "            if mixup_on:\n",
        "                input_spectrum2 = self.shift_spectrum(input_spectrum2, shift_range)\n",
        "                output_spectrum2 = self.shift_spectrum(output_spectrum2, shift_range)\n",
        "        else:\n",
        "            input_spectrum = np.expand_dims(input_spectrum, axis=-1)\n",
        "            output_spectrum = np.expand_dims(output_spectrum, axis=-1)\n",
        "            if mixup_on:\n",
        "                input_spectrum2 = np.expand_dims(input_spectrum2, axis=-1)\n",
        "                output_spectrum2 = np.expand_dims(output_spectrum2, axis=-1)\n",
        "\n",
        "        if self.horizontal_flip:\n",
        "            if np.random.random() < 0.5:\n",
        "                input_spectrum = self.flip_axis(input_spectrum, 0)\n",
        "                output_spectrum = self.flip_axis(output_spectrum, 0)\n",
        "                if mixup_on:\n",
        "                    input_spectrum2 = self.flip_axis(input_spectrum2, 0)\n",
        "                    output_spectrum2 = self.flip_axis(output_spectrum2, 0)\n",
        "\n",
        "        if mixup_on:\n",
        "            input_spectrum, output_spectrum = self.mixup_spectrum(input_spectrum, input_spectrum2, output_spectrum, output_spectrum2, 0.2)\n",
        "\n",
        "        input_spectrum = input_spectrum/np.amax(input_spectrum)\n",
        "        output_spectrum = output_spectrum/np.amax(output_spectrum)\n",
        "\n",
        "        input_spectrum = np.moveaxis(input_spectrum, -1, 0)\n",
        "        output_spectrum = np.moveaxis(output_spectrum, -1, 0)\n",
        "\n",
        "        sample = {'input_spectrum': input_spectrum, 'output_spectrum': output_spectrum}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)"
      ],
      "metadata": {
        "id": "w6PAklAu0hNJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utilities\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)"
      ],
      "metadata": {
        "id": "3NAulroG0i29"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, net, optimizer, scheduler, criterion, criterion_MSE, epoch, args):\n",
        "\n",
        "    # For measuring time and losses\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss MSE', ':.4e')\n",
        "    progress = ProgressMeter(len(dataloader), [batch_time, losses], prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    # Initialize GradScaler for automatic mixed precision\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    # Iterate through the batches\n",
        "    for i, data in enumerate(dataloader):\n",
        "        # Efficient data transfer to GPU\n",
        "        inputs = data['input_spectrum'].float().cuda(args.gpu, non_blocking=True)\n",
        "        target = data['output_spectrum'].float().cuda(args.gpu, non_blocking=True)\n",
        "\n",
        "        # Forward pass with mixed precision (autocast)\n",
        "        with amp.autocast():\n",
        "            output = net(inputs)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass with scaled gradients for stability in AMP\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Step the optimizer using the scaled loss\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update learning rate scheduler, if applicable\n",
        "        if args.scheduler in [\"cyclic-lr\", \"one-cycle-lr\"]:\n",
        "            scheduler.step()\n",
        "\n",
        "        # MSE loss calculation outside of AMP (usually in FP32 for stability)\n",
        "        with torch.no_grad():\n",
        "            loss_MSE = criterion_MSE(output, target)\n",
        "            losses.update(loss_MSE.item(), inputs.size(0))\n",
        "\n",
        "        # Measure batch processing time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # Log progress every 400 iterations\n",
        "        if i % 400 == 0:\n",
        "            progress.display(i)\n",
        "\n",
        "    return losses.avg\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "smrIJIsQDp3c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(dataloader, net, criterion_MSE, args):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    progress = ProgressMeter(len(dataloader), [batch_time, losses], prefix='Validation: ')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, data in enumerate(dataloader):\n",
        "            inputs = data['input_spectrum']\n",
        "            inputs = inputs.float()\n",
        "            inputs = inputs.cuda(args.gpu)\n",
        "            target = data['output_spectrum']\n",
        "            target = target.float()\n",
        "            target = target.cuda(args.gpu)\n",
        "\n",
        "            output = net(inputs)\n",
        "\n",
        "            loss_MSE = criterion_MSE(output, target)\n",
        "            losses.update(loss_MSE.item(), inputs.size(0))\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % 400 == 0:\n",
        "                progress.display(i)\n",
        "\n",
        "    return losses.avg"
      ],
      "metadata": {
        "id": "ZvHvb_vkD2Wm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_noKmeans(args):\n",
        "\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "\n",
        "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "    ngpus_per_node = torch.cuda.device_count()\n",
        "\n",
        "    gpu = args.gpu\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create model(s) and send to device(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "\n",
        "    net = TransUNet1D(in_channels=1, out_channels=1, embed_dim=64, num_heads=4, depth=4, patch_size=10).float()\n",
        "\n",
        "\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.gpu is not None:\n",
        "            torch.cuda.set_device(args.gpu)\n",
        "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.gpu])\n",
        "        else:\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "    elif args.gpu is not None:\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "        net.cuda(args.gpu)\n",
        "    else:\n",
        "        net.cuda(args.gpu)\n",
        "        net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define dataset path and data splits\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Input_Data = scipy.io.loadmat(\"Dataset/Train_Inputs.mat\")\n",
        "    Output_Data = scipy.io.loadmat(\"Dataset/Train_Outputs.mat\")\n",
        "\n",
        "    Input = Input_Data['Train_Inputs']\n",
        "    Output = Output_Data['Train_Outputs']\n",
        "\n",
        "    spectra_num = len(Input)\n",
        "\n",
        "    train_split = round(0.9 * spectra_num)\n",
        "    val_split = round(0.1 * spectra_num)\n",
        "\n",
        "    input_train = Input[:train_split]\n",
        "    input_val = Input[train_split:train_split+val_split]\n",
        "\n",
        "    output_train = Output[:train_split]\n",
        "    output_val = Output[train_split:train_split+val_split]\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create datasets (with augmentation) and dataloaders\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Raman_Dataset_Train = RamanDataset(input_train, output_train, batch_size = args.batch_size, spectrum_len = args.spectrum_len,\n",
        "                                   spectrum_shift=0.1, spectrum_window = False, horizontal_flip = False, mixup = True)\n",
        "\n",
        "    Raman_Dataset_Val = RamanDataset(input_val, output_val, batch_size = args.batch_size, spectrum_len = args.spectrum_len)\n",
        "\n",
        "# From here down per fold\n",
        "    train_loader = DataLoader(Raman_Dataset_Train, batch_size = args.batch_size, shuffle = False, num_workers = 0, pin_memory = True)\n",
        "    val_loader = DataLoader(Raman_Dataset_Val, batch_size = args.batch_size, shuffle = False, num_workers = 0, pin_memory = True)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define criterion(s), optimizer(s), and scheduler(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    criterion = nn.L1Loss().cuda(args.gpu)\n",
        "    criterion_MSE = nn.MSELoss().cuda(args.gpu)\n",
        "    if args.optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(net.parameters(), lr = args.lr)\n",
        "    elif args.optimizer == \"adamW\":\n",
        "        optimizer = optim.AdamW(net.parameters(), lr = args.lr)\n",
        "    else: # Adam\n",
        "        optimizer = optim.Adam(net.parameters(), lr = args.lr)\n",
        "\n",
        "    if args.scheduler == \"decay-lr\":\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.2)\n",
        "    elif args.scheduler == \"multiplicative-lr\":\n",
        "        lmbda = lambda epoch: 0.985\n",
        "        scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
        "    elif args.scheduler == \"cyclic-lr\":\n",
        "        scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr = args.base_lr, max_lr = args.lr, mode = 'triangular2', cycle_momentum = False)\n",
        "    elif args.scheduler == \"one-cycle-lr\":\n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr = args.lr, steps_per_epoch=len(train_loader), epochs=args.epochs, cycle_momentum = False)\n",
        "    else: # constant-lr\n",
        "        scheduler = None\n",
        "\n",
        "    print('Started Training')\n",
        "    print('Training Details:')\n",
        "    print('Network:         {}'.format(args.network))\n",
        "    print('Epochs:          {}'.format(args.epochs))\n",
        "    print('Batch Size:      {}'.format(args.batch_size))\n",
        "    print('Optimizer:       {}'.format(args.optimizer))\n",
        "    print('Scheduler:       {}'.format(args.scheduler))\n",
        "    print('Learning Rate:   {}'.format(args.lr))\n",
        "    print('Spectrum Length: {}'.format(args.spectrum_len))\n",
        "\n",
        "    DATE = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
        "\n",
        "    formatted_lr = '{:_.6f}'.format(float(args.lr)).rstrip('0').rstrip('.')\n",
        "    losses_dir = \"losses/{}_{}_{}_{}_{}.csv\".format(DATE, args.optimizer, args.scheduler, formatted_lr, args.network)\n",
        "    models_dir = \"{}_{}_{}_{}_{}.pt\".format(DATE, args.optimizer, args.scheduler, formatted_lr, args.network)\n",
        "\n",
        "    df = pd.DataFrame(columns=['epoch', 'train_loss', 'val_loss'])\n",
        "\n",
        "    # Early stopping\n",
        "    patience = args.patience if hasattr(args, 'patience') else 10  # Default patience of 10 epochs\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        train_loss = train(train_loader, net, optimizer, scheduler, criterion, criterion_MSE, epoch, args)\n",
        "        val_loss = validate(val_loader, net, criterion_MSE, args)\n",
        "        if args.scheduler == \"decay-lr\" or args.scheduler == \"multiplicative-lr\":\n",
        "            scheduler.step()\n",
        "\n",
        "        print(\"Epoch: \", epoch)\n",
        "        print(\"Train Loss: \", train_loss)\n",
        "        print(\"Val Loss: \", val_loss)\n",
        "        new_row = pd.DataFrame({'epoch': [epoch], 'train_loss': [train_loss], 'val_loss': [val_loss]})\n",
        "\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "\n",
        "        # Early Stopping Logic\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping triggered. No improvement in validation loss for {patience} epochs. Finished at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    torch.save(net.state_dict(), models_dir)\n",
        "    df.to_csv(losses_dir, index=False)\n",
        "    print('Finished Training')"
      ],
      "metadata": {
        "id": "pPPCH3WyxvzD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "qtYI_0qkGa3n",
        "outputId": "f7253e2b-45a6-4d4e-dbe9-b6fe93ec4fc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "O1sjKibTHrPN",
        "outputId": "faf2dfc5-58e6-4e07-a679-b7d45b36b069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset.py  model.py  ResUNet.pt  utilities.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J1ZBnz0lm1w",
        "outputId": "73d9cd4b-dfb5-462a-a61c-02db644a1f8a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/My\\ Drive/Colab\\ Notebooks/DeepeR-master/Raman Spectral Denoising"
      ],
      "metadata": {
        "id": "RAHNfyHBGcGD",
        "outputId": "1fc9c645-cfa0-436b-b9a6-ef3d83657159",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/DeepeR-master/Raman Spectral Denoising\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Default args from original code\n",
        "#Namespace(workers=0, epochs=2, start_epoch=0, batch_size=256, network='ResUNet', optimizer='adam', lr=0.0005, base_lr=5e-06, scheduler='one-cycle-lr', batch_norm=True, spectrum_len=500, seed=None, gpu=0, world_size=-1, rank=-1, dist_url='tcp://224.66.41.62:23456', dist_backend='nccl', multiprocessing_distributed=False)\n",
        "\n",
        "class Arguments:\n",
        "    pass\n",
        "\n",
        "args = Arguments()\n",
        "args.workers = 0\n",
        "args.epochs = 500\n",
        "args.start_epoch = 0\n",
        "args.batch_size = 256\n",
        "args.network = \"TransUNet1D\"\n",
        "args.optimizer = \"adamW\"\n",
        "args.lr = 1e-4\n",
        "args.base_lr = 5e-6\n",
        "args.scheduler = \"one-cycle-lr\"\n",
        "args.batch_norm = True\n",
        "args.spectrum_len = 500\n",
        "args.seed = None\n",
        "args.gpu = 0\n",
        "args.world_size = -1\n",
        "args.rank = -1\n",
        "args.dist_url = \"tcp://224.66.41.62:23456\"\n",
        "args.dist_backend = \"nccl\"\n",
        "args.multiprocessing_distributed = False\n",
        "args.patience = 10\n",
        "\n",
        "\n",
        "args.epochs=500\n",
        "train_noKmeans(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqEw59PrCzKG",
        "outputId": "ab878bba-9f7f-4377-dee8-c67e5cb374d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: 0 for training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "0GfpXoHek8ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_psnr(output, target):\n",
        "    psnr = 0.\n",
        "    mse = nn.MSELoss()(output, target)\n",
        "    psnr = 10 * math.log10(torch.max(output)/mse)\n",
        "    return psnr\n",
        "\n",
        "def calc_ssim(output, target):\n",
        "    ssim = 0.\n",
        "    output = output.cpu().detach().numpy()\n",
        "    target = target.cpu().detach().numpy()\n",
        "\n",
        "    if output.ndim == 4:\n",
        "        for i in range(output.shape[0]):\n",
        "            output_i = np.squeeze(output[i,:,:,:])\n",
        "            output_i = np.moveaxis(output_i, 0, -1)\n",
        "            target_i = np.squeeze(target[i,:,:,:])\n",
        "            target_i = np.moveaxis(target_i, 0, -1)\n",
        "            batch_size = output.shape[0]\n",
        "            ssim += sk_ssim(output_i, target_i, data_range = output_i.max() - target_i.max(), multichannel=True)\n",
        "    else:\n",
        "        output_i = np.squeeze(output)\n",
        "        output_i = np.moveaxis(output_i, 0, -1)\n",
        "        target_i = np.squeeze(target)\n",
        "        target_i = np.moveaxis(target_i, 0, -1)\n",
        "        batch_size = 1\n",
        "        ssim += sk_ssim(output_i, target_i, data_range = output_i.max() - target_i.max(), multichannel=True)\n",
        "\n",
        "    ssim = ssim / batch_size\n",
        "    return ssim"
      ],
      "metadata": {
        "id": "6sQ6cib9zsW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "def evaluate(dataloader, net, args):\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    psnr = AverageMeter('PSNR', ':.4f')\n",
        "    ssim = AverageMeter('SSIM', ':.4f')\n",
        "    SG_loss = AverageMeter('Savitzky-Golay Loss', ':.4e')\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    MSE_SG = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(dataloader):\n",
        "            x = data['input_spectrum']\n",
        "            inputs = x.float()\n",
        "            inputs = inputs.cuda(args.gpu)\n",
        "            y = data['output_spectrum']\n",
        "            target = y.float()\n",
        "            target = target.cuda(args.gpu)\n",
        "\n",
        "            x = np.squeeze(x.numpy())\n",
        "            y = np.squeeze(y.numpy())\n",
        "\n",
        "            output = net(inputs)\n",
        "            loss = nn.MSELoss()(output, target)\n",
        "\n",
        "            x_out = output.cpu().detach().numpy()\n",
        "            x_out = np.squeeze(x_out)\n",
        "\n",
        "            SGF_1_9 = scipy.signal.savgol_filter(x,9,1)\n",
        "            MSE_SGF_1_9 = np.mean(np.mean(np.square(np.absolute(y - (SGF_1_9 - np.reshape(np.amin(SGF_1_9, axis = 1), (len(SGF_1_9),1)))))))\n",
        "            MSE_SG.append(MSE_SGF_1_9)\n",
        "\n",
        "            psnr_batch = calc_psnr(output, target)\n",
        "            psnr.update(psnr_batch, inputs.size(0))\n",
        "            ssim_batch = calc_ssim(output, target)\n",
        "            ssim.update(ssim_batch, inputs.size(0))\n",
        "\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "\n",
        "        print(\"Neural Network MSE: {}\".format(losses.avg))\n",
        "        print(\"Neural Network PSNR: {}\".format(psnr.avg))\n",
        "        print(\"Neural Network SSIM: {}\".format(ssim.avg))\n",
        "        print(\"Savitzky-Golay MSE: {}\".format(np.mean(np.asarray(MSE_SG))))\n",
        "        print(\"Neural Network performed {0:.2f}x better than Savitzky-Golay\".format(np.mean(np.asarray(MSE_SG))/losses.avg))\n",
        "\n",
        "    return losses.avg, psnr.avg, ssim.avg, MSE_SG"
      ],
      "metadata": {
        "id": "8ujeP58VmTxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_test(args):\n",
        "    gpu = args.gpu\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "\n",
        "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "    ngpus_per_node = torch.cuda.device_count()\n",
        "\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for testing\".format(args.gpu))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create model(s) and send to device(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    if args.network == \"UNetPlusPlus1D\":\n",
        "      net = UNetPlusPlus1D().float()\n",
        "    elif args.network == \"UNet1D\":\n",
        "      net = UNet1D().float()\n",
        "    elif args.network == \"AttentionUNet\":\n",
        "      net = AttentionUNet().float()\n",
        "    net.load_state_dict(torch.load(args.model))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.gpu is not None:\n",
        "            torch.cuda.set_device(args.gpu)\n",
        "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.gpu])\n",
        "        else:\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "    elif args.gpu is not None:\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "        net.cuda(args.gpu)\n",
        "    else:\n",
        "        net.cuda(args.gpu)\n",
        "        net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define dataset path and data splits\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Input_Data = scipy.io.loadmat(\"Dataset/Test_Inputs.mat\")\n",
        "    Output_Data = scipy.io.loadmat(\"Dataset/Test_Outputs.mat\")\n",
        "\n",
        "    Input = Input_Data['Test_Inputs']\n",
        "    Output = Output_Data['Test_Outputs']\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create datasets (with augmentation) and dataloaders\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Raman_Dataset_Test = RamanDataset(Input, Output, batch_size = args.batch_size, spectrum_len = args.spectrum_len)\n",
        "\n",
        "    test_loader = DataLoader(Raman_Dataset_Test, batch_size = args.batch_size, shuffle = False, num_workers = 0, pin_memory = True)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Evaluate\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    MSE_NN, PSNR_NN, SSIM_NN, MSE_SG = evaluate(test_loader, net, args)"
      ],
      "metadata": {
        "id": "pGV-EuKysDBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Arguments:\n",
        "    pass\n",
        "\n",
        "args = Arguments()\n",
        "args.workers = 0\n",
        "args.batch_size = 256\n",
        "args.spectrum_len = 500\n",
        "args.seed = None\n",
        "args.gpu = 0\n",
        "args.world_size = -1\n",
        "args.rank = -1\n",
        "args.dist_url = \"tcp://224.66.41.62:23456\"\n",
        "args.dist_backend = \"nccl\"\n",
        "args.multiprocessing_distributed = False\n",
        "args.batch_norm = True\n",
        "args.network = \"UNet1D\"\n",
        "args.model = \"2024_10_09_adamW_one-cycle-lr_0.001_UNet1D.pt\"\n",
        "\n",
        "\n",
        "main_test(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I97mxHSasvCS",
        "outputId": "8d06b4cf-4410-417e-81b7-ecf00c63fb32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: 0 for testing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-cf748de30bb6>:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(torch.load(args.model))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network MSE: 0.0023067386205582094\n",
            "Neural Network PSNR: 26.385451591812636\n",
            "Neural Network SSIM: 0.2742282462580637\n",
            "Savitzky-Golay MSE: 0.027660622850368643\n",
            "Neural Network performed 11.99x better than Savitzky-Golay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 'losses/training_logs.csv'\n",
        "\n",
        "df = pd.read_csv(file_name)\n",
        "\n",
        "# Plotting train_loss and val_loss\n",
        "plt.plot(df['epoch'], df['train_loss'], label='Train Loss', marker='o')\n",
        "plt.plot(df['epoch'], df['val_loss'], label='Validation Loss', marker='o')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Validation Loss vs Epoch')\n",
        "\n",
        "# Adding a legend to distinguish the two lines\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M6n2Q8wWpEm_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
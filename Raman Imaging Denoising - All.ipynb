{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1GHdCjqh84Xexa4GTzb8qntXQ08wgcIgd",
      "authorship_tag": "ABX9TyNo9AhtKXXvjmswEdjcyoIO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiverBotham/Raman/blob/main/Raman%20Imaging%20Denoising%20-%20All.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import datetime\n",
        "import time\n",
        "import shutil\n",
        "import argparse\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import scipy.signal\n",
        "import math\n",
        "from skimage.metrics import structural_similarity as sk_ssim\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.distributed as dist\n",
        "import torch.utils.data.distributed\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms, utils\n",
        "import torch.cuda.amp as amp\n",
        "from einops import rearrange, repeat"
      ],
      "metadata": {
        "id": "KWeXO7nYDF8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original Baseline model - ResUNet\n",
        "\n",
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, channels_in, channels_out, batch_norm):\n",
        "        super(BasicConv, self).__init__()\n",
        "        basic_conv = [nn.Conv1d(channels_in, channels_out, kernel_size = 3, stride = 1, padding = 1, bias = True)]\n",
        "        basic_conv.append(nn.PReLU())\n",
        "        if batch_norm:\n",
        "            basic_conv.append(nn.BatchNorm1d(channels_out))\n",
        "\n",
        "        self.body = nn.Sequential(*basic_conv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n",
        "\n",
        "class ResUNetConv(nn.Module):\n",
        "    def __init__(self, num_convs, channels, batch_norm):\n",
        "        super(ResUNetConv, self).__init__()\n",
        "        unet_conv = []\n",
        "        for _ in range(num_convs):\n",
        "            unet_conv.append(nn.Conv1d(channels, channels, kernel_size = 3, stride = 1, padding = 1, bias = True))\n",
        "            unet_conv.append(nn.PReLU())\n",
        "            if batch_norm:\n",
        "                unet_conv.append(nn.BatchNorm1d(channels))\n",
        "\n",
        "        self.body = nn.Sequential(*unet_conv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.body(x)\n",
        "        res += x\n",
        "        return res\n",
        "\n",
        "class UNetLinear(nn.Module):\n",
        "    def __init__(self, repeats, channels_in, channels_out):\n",
        "        super().__init__()\n",
        "        modules = []\n",
        "        for i in range(repeats):\n",
        "            modules.append(nn.Linear(channels_in, channels_out))\n",
        "            modules.append(nn.PReLU())\n",
        "\n",
        "        self.body = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.body(x)\n",
        "        return x\n",
        "\n",
        "class ResUNet(nn.Module):\n",
        "    def __init__(self, num_convs, batch_norm):\n",
        "        super(ResUNet, self).__init__()\n",
        "        res_conv1 = [BasicConv(1, 64, batch_norm)]\n",
        "        res_conv1.append(ResUNetConv(num_convs, 64, batch_norm))\n",
        "        self.conv1 = nn.Sequential(*res_conv1)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        res_conv2 = [BasicConv(64, 128, batch_norm)]\n",
        "        res_conv2.append(ResUNetConv(num_convs, 128, batch_norm))\n",
        "        self.conv2 = nn.Sequential(*res_conv2)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "\n",
        "        res_conv3 = [BasicConv(128, 256, batch_norm)]\n",
        "        res_conv3.append(ResUNetConv(num_convs, 256, batch_norm))\n",
        "        res_conv3.append(BasicConv(256, 128, batch_norm))\n",
        "        self.conv3 = nn.Sequential(*res_conv3)\n",
        "        self.up3 = nn.Upsample(scale_factor = 2)\n",
        "\n",
        "        res_conv4 = [BasicConv(256, 128, batch_norm)]\n",
        "        res_conv4.append(ResUNetConv(num_convs, 128, batch_norm))\n",
        "        res_conv4.append(BasicConv(128, 64, batch_norm))\n",
        "        self.conv4 = nn.Sequential(*res_conv4)\n",
        "        self.up4 = nn.Upsample(scale_factor = 2)\n",
        "\n",
        "        res_conv5 = [BasicConv(128, 64, batch_norm)]\n",
        "        res_conv5.append(ResUNetConv(num_convs,64, batch_norm))\n",
        "        self.conv5 = nn.Sequential(*res_conv5)\n",
        "        res_conv6 = [BasicConv(64, 1, batch_norm)]\n",
        "        self.conv6 = nn.Sequential(*res_conv6)\n",
        "\n",
        "        self.linear7 = UNetLinear(3, 500, 500)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x1 = self.pool1(x)\n",
        "\n",
        "        x2 = self.conv2(x1)\n",
        "        x3 = self.pool1(x2)\n",
        "\n",
        "        x3 = self.conv3(x3)\n",
        "        x3 = self.up3(x3)\n",
        "\n",
        "        x4 = torch.cat((x2, x3), dim = 1)\n",
        "        x4 = self.conv4(x4)\n",
        "        x5 = self.up4(x4)\n",
        "\n",
        "        x6 = torch.cat((x, x5), dim = 1)\n",
        "        x6 = self.conv5(x6)\n",
        "        x7 = self.conv6(x6)\n",
        "\n",
        "        out = self.linear7(x7)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "Mei5k_ff0fci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1D U-Net architecture for signal denoising\n",
        "\n",
        "class UNet1D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet1D, self).__init__()\n",
        "\n",
        "        self.encoder1 = self.conv_block(1, 64)\n",
        "        self.encoder2 = self.conv_block(64, 128)\n",
        "        self.encoder3 = self.conv_block(128, 256)\n",
        "        self.encoder4 = self.conv_block(256, 512)\n",
        "\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "\n",
        "        self.upconv4 = self.upconv(1024, 512)\n",
        "        self.decoder4 = self.conv_block(1024, 512)\n",
        "\n",
        "        self.upconv3 = self.upconv(512, 256)\n",
        "        self.decoder3 = self.conv_block(512, 256)\n",
        "\n",
        "        self.upconv2 = self.upconv(256, 128)\n",
        "        self.decoder2 = self.conv_block(256, 128)\n",
        "\n",
        "        self.upconv1 = self.upconv(128, 64)\n",
        "        self.decoder1 = self.conv_block(128, 64)\n",
        "\n",
        "        self.conv_final = nn.Conv1d(64, 1, kernel_size=1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def upconv(self, in_channels, out_channels):\n",
        "        return nn.ConvTranspose1d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(F.max_pool1d(enc1, 2))\n",
        "        enc3 = self.encoder3(F.max_pool1d(enc2, 2))\n",
        "        enc4 = self.encoder4(F.max_pool1d(enc3, 2))\n",
        "\n",
        "        bottleneck = self.bottleneck(F.max_pool1d(enc4, 2))\n",
        "\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "\n",
        "        if dec4.size(2) != enc4.size(2):\n",
        "            dec4 = F.pad(dec4, (0, enc4.size(2) - dec4.size(2)))\n",
        "\n",
        "        dec4 = torch.cat((enc4, dec4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        if dec3.size(2) != enc3.size(2):\n",
        "            dec3 = F.pad(dec3, (0, enc3.size(2) - dec3.size(2)))\n",
        "\n",
        "        dec3 = torch.cat((enc3, dec3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        if dec2.size(2) != enc2.size(2):\n",
        "            dec2 = F.pad(dec2, (0, enc2.size(2) - dec2.size(2)))\n",
        "\n",
        "        dec2 = torch.cat((enc2, dec2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        if dec1.size(2) != enc1.size(2):\n",
        "            dec1 = F.pad(dec1, (0, enc1.size(2) - dec1.size(2)))\n",
        "\n",
        "        dec1 = torch.cat((enc1, dec1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "\n",
        "        return self.conv_final(dec1)"
      ],
      "metadata": {
        "id": "0NXmUmgf701y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNet++ in 1D\n",
        "class VGGBlock(nn.Module):\n",
        "    def __init__(self, in_channels, middle_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv1d(in_channels, middle_channels, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(middle_channels)\n",
        "        self.conv2 = nn.Conv1d(middle_channels, out_channels, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class UNetPlusPlus1D(nn.Module):\n",
        "    def __init__(self, input_channels=1, deep_supervision=False, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        nb_filter = [32, 64, 128, 256, 512]\n",
        "\n",
        "        self.deep_supervision = deep_supervision\n",
        "\n",
        "        self.pool = nn.MaxPool1d(2, 2)\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='linear', align_corners=True)\n",
        "\n",
        "        self.conv0_0 = VGGBlock(input_channels, nb_filter[0], nb_filter[0])\n",
        "        self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])\n",
        "        self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])\n",
        "        self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])\n",
        "        self.conv4_0 = VGGBlock(nb_filter[3], nb_filter[4], nb_filter[4])\n",
        "\n",
        "        self.conv0_1 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])\n",
        "        self.conv1_1 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])\n",
        "        self.conv2_1 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])\n",
        "        self.conv3_1 = VGGBlock(nb_filter[3]+nb_filter[4], nb_filter[3], nb_filter[3])\n",
        "\n",
        "        self.conv0_2 = VGGBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0], nb_filter[0])\n",
        "        self.conv1_2 = VGGBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1], nb_filter[1])\n",
        "        self.conv2_2 = VGGBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2], nb_filter[2])\n",
        "\n",
        "        self.conv0_3 = VGGBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0], nb_filter[0])\n",
        "        self.conv1_3 = VGGBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1], nb_filter[1])\n",
        "\n",
        "        self.conv0_4 = VGGBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0], nb_filter[0])\n",
        "\n",
        "        if self.deep_supervision:\n",
        "            self.final1 = nn.Conv1d(nb_filter[0], 1, kernel_size=1)\n",
        "            self.final2 = nn.Conv1d(nb_filter[0], 1, kernel_size=1)\n",
        "            self.final3 = nn.Conv1d(nb_filter[0], 1, kernel_size=1)\n",
        "            self.final4 = nn.Conv1d(nb_filter[0], 1, kernel_size=1)\n",
        "        else:\n",
        "            self.final = nn.Conv1d(nb_filter[0], 1, kernel_size=1)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        x0_0 = self.conv0_0(input)\n",
        "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n",
        "\n",
        "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n",
        "\n",
        "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
        "\n",
        "        upsampled_x3_0 = self.up(x3_0)\n",
        "        padded_upsampled_x3_0 = F.pad(upsampled_x3_0, (0, x2_0.size(2) - upsampled_x3_0.size(2)))\n",
        "\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, padded_upsampled_x3_0], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n",
        "\n",
        "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n",
        "\n",
        "        padded_x2_1 = F.pad(x2_1, (0, x2_0.size(2) - x2_1.size(2)))\n",
        "        upsampled_x3_1 = self.up(x3_1)\n",
        "        padded_upsampled_x3_1 = F.pad(upsampled_x3_1, (0, x2_0.size(2) - upsampled_x3_1.size(2)))\n",
        "\n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0, padded_x2_1, padded_upsampled_x3_1], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n",
        "\n",
        "        if self.deep_supervision:\n",
        "            output1 = self.final1(x0_1)\n",
        "            output2 = self.final2(x0_2)\n",
        "            output3 = self.final3(x0_3)\n",
        "            output4 = self.final4(x0_4)\n",
        "            return [output1, output2, output3, output4]\n",
        "\n",
        "        else:\n",
        "            output = self.final(x0_4)\n",
        "            return output"
      ],
      "metadata": {
        "id": "i6Ba06G572GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention U-Net in 1D\n",
        "class ConvBlockA(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "                                  nn.Conv1d(ch_in, ch_out,\n",
        "                                            kernel_size=3, stride=1,\n",
        "                                            padding=1, bias=True),\n",
        "                                  nn.BatchNorm1d(ch_out),\n",
        "                                  nn.ReLU(inplace=True),\n",
        "                                  nn.Conv1d(ch_out, ch_out,\n",
        "                                            kernel_size=3, stride=1,\n",
        "                                            padding=1, bias=True),\n",
        "                                  nn.BatchNorm1d(ch_out),\n",
        "                                  nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class UpConvBlock(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super().__init__()\n",
        "        self.up = nn.Sequential(\n",
        "                                nn.Upsample(scale_factor=2),\n",
        "                                nn.Conv1d(ch_in, ch_out,\n",
        "                                         kernel_size=3,stride=1,\n",
        "                                         padding=1, bias=True),\n",
        "                                nn.BatchNorm1d(ch_out),\n",
        "                                nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x = self.up(x)\n",
        "        return x\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, f_g, f_l, f_int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.w_g = nn.Sequential(\n",
        "                                nn.Conv1d(f_g, f_int,\n",
        "                                         kernel_size=1, stride=1,\n",
        "                                         padding=0, bias=True),\n",
        "                                nn.BatchNorm1d(f_int)\n",
        "        )\n",
        "\n",
        "        self.w_x = nn.Sequential(\n",
        "                                nn.Conv1d(f_l, f_int,\n",
        "                                         kernel_size=1, stride=1,\n",
        "                                         padding=0, bias=True),\n",
        "                                nn.BatchNorm1d(f_int)\n",
        "        )\n",
        "\n",
        "        self.psi = nn.Sequential(\n",
        "                                nn.Conv1d(f_int, 1,\n",
        "                                         kernel_size=1, stride=1,\n",
        "                                         padding=0,  bias=True),\n",
        "                                nn.BatchNorm1d(1),\n",
        "                                nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.w_g(g)\n",
        "        x1 = self.w_x(x)\n",
        "        psi = self.relu(g1+x1)\n",
        "        psi = self.psi(psi)\n",
        "\n",
        "        return psi*x\n",
        "\n",
        "class AttentionUNet(nn.Module):\n",
        "    def __init__(self, n_classes=1, in_channel=1, out_channel=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv1 = ConvBlockA(ch_in=in_channel, ch_out=64)\n",
        "        self.conv2 = ConvBlockA(ch_in=64, ch_out=128)\n",
        "        self.conv3 = ConvBlockA(ch_in=128, ch_out=256)\n",
        "        self.conv4 = ConvBlockA(ch_in=256, ch_out=512)\n",
        "        self.conv5 = ConvBlockA(ch_in=512, ch_out=1024)\n",
        "\n",
        "        self.up5 = UpConvBlock(ch_in=1024, ch_out=512)\n",
        "        self.att5 = AttentionBlock(f_g=512, f_l=512, f_int=256)\n",
        "        self.upconv5 = ConvBlockA(ch_in=1024, ch_out=512)\n",
        "\n",
        "        self.up4 = UpConvBlock(ch_in=512, ch_out=256)\n",
        "        self.att4 = AttentionBlock(f_g=256, f_l=256, f_int=128)\n",
        "        self.upconv4 = ConvBlockA(ch_in=512, ch_out=256)\n",
        "\n",
        "        self.up3 = UpConvBlock(ch_in=256, ch_out=128)\n",
        "        self.att3 = AttentionBlock(f_g=128, f_l=128, f_int=64)\n",
        "        self.upconv3 = ConvBlockA(ch_in=256, ch_out=128)\n",
        "\n",
        "        self.up2 = UpConvBlock(ch_in=128, ch_out=64)\n",
        "        self.att2 = AttentionBlock(f_g=64, f_l=64, f_int=32)\n",
        "        self.upconv2 = ConvBlockA(ch_in=128, ch_out=64)\n",
        "\n",
        "        self.conv_1x1 = nn.Conv1d(64, out_channel,\n",
        "                                  kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "\n",
        "        x2 = self.maxpool(x1)\n",
        "        x2 = self.conv2(x2)\n",
        "\n",
        "        x3 = self.maxpool(x2)\n",
        "        x3 = self.conv3(x3)\n",
        "\n",
        "        x4 = self.maxpool(x3)\n",
        "        x4 = self.conv4(x4)\n",
        "\n",
        "        x5 = self.maxpool(x4)\n",
        "        x5 = self.conv5(x5)\n",
        "\n",
        "        d5 = self.up5(x5)\n",
        "        x4 = self.att5(g=d5, x=x4)\n",
        "        d5 = torch.concat((x4, d5), dim=1)\n",
        "        d5 = self.upconv5(d5)\n",
        "\n",
        "        d4 = self.up4(d5)\n",
        "        d4 = F.pad(d4, (0, x3.size(2) - d4.size(2)))\n",
        "        x3 = self.att4(g=d4, x=x3)\n",
        "        d4 = torch.concat((x3, d4), dim=1)\n",
        "        d4 = self.upconv4(d4)\n",
        "\n",
        "        d3 = self.up3(d4)\n",
        "        x2 = self.att3(g=d3, x=x2)\n",
        "        d3 = torch.concat((x2, d3), dim=1)\n",
        "        d3 = self.upconv3(d3)\n",
        "\n",
        "        d2 = self.up2(d3)\n",
        "        x1 = self.att2(g=d2, x=x1)\n",
        "        d2 = torch.concat((x1, d2), dim=1)\n",
        "        d2 = self.upconv2(d2)\n",
        "\n",
        "        d1 = self.conv_1x1(d2)\n",
        "\n",
        "        return d1"
      ],
      "metadata": {
        "id": "UvfLfo2h79_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TransUNet\n",
        "class PatchEmbedding1D(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size, embed_dim):\n",
        "        super(PatchEmbedding1D, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.conv = nn.Conv1d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding1D(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=500):\n",
        "        super(PositionalEncoding1D, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        num_patches = x.size(1)\n",
        "        x = x + self.pe[:, :num_patches, :].to(x.device)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderLayer1D(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1, dim_feedforward=2048):\n",
        "        super(TransformerEncoderLayer1D, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(embed_dim, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, embed_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src2 = self.self_attn(src, src, src)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "class TransformerEncoder1D(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, depth, dropout=0.1):\n",
        "        super(TransformerEncoder1D, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer1D(embed_dim, num_heads, dropout) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(1, 0, 2)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        return x\n",
        "\n",
        "class UpSampleBlock1D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UpSampleBlock1D, self).__init__()\n",
        "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.up = nn.ConvTranspose1d(out_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv(x))\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "class EnhancedCNNBackbone1D(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels):\n",
        "        super(EnhancedCNNBackbone1D, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, mid_channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(mid_channels, mid_channels, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(mid_channels, mid_channels * 2, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv1d(mid_channels * 2, mid_channels * 2, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = F.relu(self.conv1(x))\n",
        "        x2 = F.relu(self.conv2(x1))\n",
        "        x3 = F.relu(self.conv3(x2))\n",
        "        x4 = F.relu(self.conv4(x3))\n",
        "        return x2, x4\n",
        "\n",
        "class TransUNet1D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, embed_dim, num_heads, depth, patch_size, mid_channels):\n",
        "        super(TransUNet1D, self).__init__()\n",
        "        self.backbone = EnhancedCNNBackbone1D(in_channels, mid_channels)\n",
        "        self.patch_embed = PatchEmbedding1D(mid_channels * 2, patch_size, embed_dim)\n",
        "        self.positional_encoding = PositionalEncoding1D(embed_dim)\n",
        "        self.transformer_encoder = TransformerEncoder1D(embed_dim, num_heads, depth)\n",
        "        self.decoder1 = UpSampleBlock1D(embed_dim + mid_channels * 2, embed_dim // 2)\n",
        "        self.decoder2 = UpSampleBlock1D(embed_dim // 2 + mid_channels, embed_dim // 4)\n",
        "        self.output_conv = nn.Conv1d(embed_dim // 4, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip1, skip2 = self.backbone(x)\n",
        "\n",
        "        x = self.patch_embed(skip2)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        batch_size, num_patches, embed_dim = x.shape\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = F.interpolate(x, size=skip2.size(-1), mode='linear', align_corners=False)\n",
        "        x = torch.cat([x, skip2], dim=1)\n",
        "\n",
        "        x = self.decoder1(x)\n",
        "        x = F.interpolate(x, size=skip1.size(-1), mode='linear', align_corners=False)\n",
        "        x = torch.cat([x, skip1], dim=1)\n",
        "        x = self.decoder2(x)\n",
        "        x = F.interpolate(x, size=500, mode='linear', align_corners=False)\n",
        "        x = self.output_conv(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "6G5X5c7c8X8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data set\n",
        "\n",
        "class RamanDataset(Dataset):\n",
        "    def __init__(self, inputs, outputs, batch_size=64,spectrum_len=500, spectrum_shift=0.,\n",
        "                 spectrum_window=False, horizontal_flip=False, mixup=False):\n",
        "        self.inputs = inputs\n",
        "        self.outputs = outputs\n",
        "        self.batch_size = batch_size\n",
        "        self.spectrum_len = spectrum_len\n",
        "        self.spectrum_shift = spectrum_shift\n",
        "        self.spectrum_window = spectrum_window\n",
        "        self.horizontal_flip = horizontal_flip\n",
        "        self.mixup = mixup\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def pad_spectrum(self, input_spectrum, spectrum_length):\n",
        "        if len(input_spectrum) == spectrum_length:\n",
        "            padded_spectrum = input_spectrum\n",
        "        elif len(input_spectrum) > spectrum_length:\n",
        "            padded_spectrum = input_spectrum[0:spectrum_length]\n",
        "        else:\n",
        "            padded_spectrum = np.pad(input_spectrum, ((0,spectrum_length - len(input_spectrum)),(0,0)), 'reflect')\n",
        "\n",
        "        return padded_spectrum\n",
        "\n",
        "    def window_spectrum(self, input_spectrum, start_idx, window_length):\n",
        "        if len(input_spectrum) <= window_length:\n",
        "            output_spectrum = input_spectrum\n",
        "        else:\n",
        "            end_idx = start_idx + window_length\n",
        "            output_spectrum = input_spectrum[start_idx:end_idx]\n",
        "\n",
        "        return output_spectrum\n",
        "\n",
        "    def flip_axis(self, x, axis):\n",
        "        if np.random.random() < 0.5:\n",
        "            x = np.asarray(x).swapaxes(axis, 0)\n",
        "            x = x[::-1, ...]\n",
        "            x = x.swapaxes(0, axis)\n",
        "        return x\n",
        "\n",
        "    def shift_spectrum(self, x, shift_range):\n",
        "        x = np.expand_dims(x,axis=-1)\n",
        "        shifted_spectrum = x\n",
        "        spectrum_shift_range = int(np.round(shift_range*len(x)))\n",
        "        if spectrum_shift_range > 0:\n",
        "            shifted_spectrum = np.pad(x[spectrum_shift_range:,:], ((0,abs(spectrum_shift_range)), (0,0)), 'reflect')\n",
        "        elif spectrum_shift_range < 0:\n",
        "            shifted_spectrum = np.pad(x[:spectrum_shift_range,:], ((abs(spectrum_shift_range), 0), (0,0)), 'reflect')\n",
        "        return shifted_spectrum\n",
        "\n",
        "    def mixup_spectrum(self, input_spectrum1, input_spectrum2, output_spectrum1, output_spectrum2, alpha):\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        input_spectrum = (lam * input_spectrum1) + ((1 - lam) * input_spectrum2)\n",
        "        output_spectrum = (lam * output_spectrum1) + ((1 - lam) * output_spectrum2)\n",
        "        return input_spectrum, output_spectrum\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_spectrum = self.inputs[index]\n",
        "        output_spectrum = self.outputs[index]\n",
        "\n",
        "        mixup_on = False\n",
        "        if self.mixup:\n",
        "            if np.random.random() < 0.5:\n",
        "                spectrum_idx = int(np.round(np.random.random() * (len(self.inputs)-1)))\n",
        "                input_spectrum2 = self.inputs[spectrum_idx]\n",
        "                output_spectrum2 = self.outputs[spectrum_idx]\n",
        "                mixup_on = True\n",
        "\n",
        "        if self.spectrum_window:\n",
        "            start_idx = int(np.floor(np.random.random() * (len(input_spectrum)-self.spectrum_len)))\n",
        "            input_spectrum = self.window_spectrum(input_spectrum, start_idx, self.spectrum_len)\n",
        "            output_spectrum = self.window_spectrum(output_spectrum, start_idx, self.spectrum_len)\n",
        "            if mixup_on:\n",
        "                input_spectrum2 = self.window_spectrum(input_spectrum2, start_idx, self.spectrum_len)\n",
        "                output_spectrum2 = self.window_spectrum(output_spectrum2, start_idx, self.spectrum_len)\n",
        "\n",
        "        input_spectrum = self.pad_spectrum(input_spectrum, self.spectrum_len)\n",
        "        output_spectrum = self.pad_spectrum(output_spectrum, self.spectrum_len)\n",
        "        if mixup_on:\n",
        "            input_spectrum2 = self.pad_spectrum(input_spectrum2, self.spectrum_len)\n",
        "            output_spectrum2 = self.pad_spectrum(output_spectrum2, self.spectrum_len)\n",
        "\n",
        "        if self.spectrum_shift != 0.0:\n",
        "            shift_range = np.random.uniform(-self.spectrum_shift, self.spectrum_shift)\n",
        "            input_spectrum = self.shift_spectrum(input_spectrum, shift_range)\n",
        "            output_spectrum = self.shift_spectrum(output_spectrum, shift_range)\n",
        "            if mixup_on:\n",
        "                input_spectrum2 = self.shift_spectrum(input_spectrum2, shift_range)\n",
        "                output_spectrum2 = self.shift_spectrum(output_spectrum2, shift_range)\n",
        "        else:\n",
        "            input_spectrum = np.expand_dims(input_spectrum, axis=-1)\n",
        "            output_spectrum = np.expand_dims(output_spectrum, axis=-1)\n",
        "            if mixup_on:\n",
        "                input_spectrum2 = np.expand_dims(input_spectrum2, axis=-1)\n",
        "                output_spectrum2 = np.expand_dims(output_spectrum2, axis=-1)\n",
        "\n",
        "        if self.horizontal_flip:\n",
        "            if np.random.random() < 0.5:\n",
        "                input_spectrum = self.flip_axis(input_spectrum, 0)\n",
        "                output_spectrum = self.flip_axis(output_spectrum, 0)\n",
        "                if mixup_on:\n",
        "                    input_spectrum2 = self.flip_axis(input_spectrum2, 0)\n",
        "                    output_spectrum2 = self.flip_axis(output_spectrum2, 0)\n",
        "\n",
        "        if mixup_on:\n",
        "            input_spectrum, output_spectrum = self.mixup_spectrum(input_spectrum, input_spectrum2, output_spectrum, output_spectrum2, 0.2)\n",
        "\n",
        "        input_spectrum = input_spectrum/np.amax(input_spectrum)\n",
        "        output_spectrum = output_spectrum/np.amax(output_spectrum)\n",
        "\n",
        "        input_spectrum = np.moveaxis(input_spectrum, -1, 0)\n",
        "        output_spectrum = np.moveaxis(output_spectrum, -1, 0)\n",
        "\n",
        "        sample = {'input_spectrum': input_spectrum, 'output_spectrum': output_spectrum}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)"
      ],
      "metadata": {
        "id": "w6PAklAu0hNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utilities\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)"
      ],
      "metadata": {
        "id": "3NAulroG0i29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, net, optimizer, scheduler, criterion, criterion_MSE, epoch, args):\n",
        "\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss MSE', ':.4e')\n",
        "    progress = ProgressMeter(len(dataloader), [batch_time, losses], prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    end = time.time()\n",
        "    for i, data in enumerate(dataloader):\n",
        "        inputs = data['input_spectrum']\n",
        "        inputs = inputs.float()\n",
        "        inputs = inputs.cuda(args.gpu)\n",
        "        target = data['output_spectrum']\n",
        "        target = target.float()\n",
        "        target = target.cuda(args.gpu)\n",
        "\n",
        "        output = net(inputs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if args.scheduler == \"cyclic-lr\" or args.scheduler == \"one-cycle-lr\":\n",
        "            scheduler.step()\n",
        "\n",
        "        loss_MSE = criterion_MSE(output, target)\n",
        "        losses.update(loss_MSE.item(), inputs.size(0))\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 400 == 0:\n",
        "            progress.display(i)\n",
        "    return losses.avg\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "smrIJIsQDp3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(dataloader, net, criterion_MSE, args):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    progress = ProgressMeter(len(dataloader), [batch_time, losses], prefix='Validation: ')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, data in enumerate(dataloader):\n",
        "            inputs = data['input_spectrum']\n",
        "            inputs = inputs.float()\n",
        "            inputs = inputs.cuda(args.gpu)\n",
        "            target = data['output_spectrum']\n",
        "            target = target.float()\n",
        "            target = target.cuda(args.gpu)\n",
        "\n",
        "            output = net(inputs)\n",
        "\n",
        "            loss_MSE = criterion_MSE(output, target)\n",
        "            losses.update(loss_MSE.item(), inputs.size(0))\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % 400 == 0:\n",
        "                progress.display(i)\n",
        "\n",
        "    return losses.avg"
      ],
      "metadata": {
        "id": "ZvHvb_vkD2Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_noKmeans(args):\n",
        "\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "\n",
        "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "    ngpus_per_node = torch.cuda.device_count()\n",
        "\n",
        "    gpu = args.gpu\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create model(s) and send to device(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "\n",
        "    if args.network == \"ResUNet\":\n",
        "        net = ResUNet(3, args.batch_norm).float()\n",
        "    if args.network == \"UNetPlusPlus1D\":\n",
        "        net = UNetPlusPlus1D().float()\n",
        "    elif args.network == \"UNet1D\":\n",
        "        net = UNet1D().float()\n",
        "    elif args.network == \"AttentionUNet\":\n",
        "        net = AttentionUNet().float()\n",
        "    elif args.network == \"TransUNet\"\n",
        "        net = TransUNet1D(in_channels=1, out_channels=1, embed_dim=128, num_heads=4, depth=4, patch_size=25, mid_channels=64).float()\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.gpu is not None:\n",
        "            torch.cuda.set_device(args.gpu)\n",
        "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.gpu])\n",
        "        else:\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "    elif args.gpu is not None:\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "        net.cuda(args.gpu)\n",
        "    else:\n",
        "        net.cuda(args.gpu)\n",
        "        net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define dataset path and data splits\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Input_Data = scipy.io.loadmat(\"Dataset/Train_Inputs.mat\")\n",
        "    Output_Data = scipy.io.loadmat(\"Dataset/Train_Outputs.mat\")\n",
        "\n",
        "    Input = Input_Data['Train_Inputs']\n",
        "    Output = Output_Data['Train_Outputs']\n",
        "\n",
        "    spectra_num = len(Input)\n",
        "\n",
        "    train_split = round(0.9 * spectra_num)\n",
        "    val_split = round(0.1 * spectra_num)\n",
        "\n",
        "    input_train = Input[:train_split]\n",
        "    input_val = Input[train_split:train_split+val_split]\n",
        "\n",
        "    output_train = Output[:train_split]\n",
        "    output_val = Output[train_split:train_split+val_split]\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create datasets (with augmentation) and dataloaders\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Raman_Dataset_Train = RamanDataset(input_train, output_train, batch_size = args.batch_size, spectrum_len = args.spectrum_len,\n",
        "                                   spectrum_shift=0.1, spectrum_window = False, horizontal_flip = False, mixup = True)\n",
        "\n",
        "    Raman_Dataset_Val = RamanDataset(input_val, output_val, batch_size = args.batch_size, spectrum_len = args.spectrum_len)\n",
        "\n",
        "# From here down per fold\n",
        "    train_loader = DataLoader(Raman_Dataset_Train, batch_size = args.batch_size, shuffle = False, num_workers = 0, pin_memory = True)\n",
        "    val_loader = DataLoader(Raman_Dataset_Val, batch_size = args.batch_size, shuffle = False, num_workers = 0, pin_memory = True)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define criterion(s), optimizer(s), and scheduler(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    criterion = nn.L1Loss().cuda(args.gpu)\n",
        "    criterion_MSE = nn.MSELoss().cuda(args.gpu)\n",
        "    if args.optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(net.parameters(), lr = args.lr)\n",
        "    elif args.optimizer == \"adamW\":\n",
        "        optimizer = optim.AdamW(net.parameters(), lr = args.lr)\n",
        "    else: # Adam\n",
        "        optimizer = optim.Adam(net.parameters(), lr = args.lr)\n",
        "\n",
        "    if args.scheduler == \"decay-lr\":\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.2)\n",
        "    elif args.scheduler == \"multiplicative-lr\":\n",
        "        lmbda = lambda epoch: 0.985\n",
        "        scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
        "    elif args.scheduler == \"cyclic-lr\":\n",
        "        scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr = args.base_lr, max_lr = args.lr, mode = 'triangular2', cycle_momentum = False)\n",
        "    elif args.scheduler == \"one-cycle-lr\":\n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr = args.lr, steps_per_epoch=len(train_loader), epochs=args.epochs, cycle_momentum = False)\n",
        "    else: # constant-lr\n",
        "        scheduler = None\n",
        "\n",
        "    print('Started Training')\n",
        "    print('Training Details:')\n",
        "    print('Network:         {}'.format(args.network))\n",
        "    print('Epochs:          {}'.format(args.epochs))\n",
        "    print('Batch Size:      {}'.format(args.batch_size))\n",
        "    print('Optimizer:       {}'.format(args.optimizer))\n",
        "    print('Scheduler:       {}'.format(args.scheduler))\n",
        "    print('Learning Rate:   {}'.format(args.lr))\n",
        "    print('Spectrum Length: {}'.format(args.spectrum_len))\n",
        "\n",
        "    DATE = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
        "\n",
        "    formatted_lr = '{:_.6f}'.format(float(args.lr)).rstrip('0').rstrip('.')\n",
        "    losses_dir = \"losses/{}_{}_{}_{}_{}.csv\".format(DATE, args.optimizer, args.scheduler, formatted_lr, args.network)\n",
        "    models_dir = \"{}_{}_{}_{}_{}.pt\".format(DATE, args.optimizer, args.scheduler, formatted_lr, args.network)\n",
        "\n",
        "    df = pd.DataFrame(columns=['epoch', 'train_loss', 'val_loss'])\n",
        "\n",
        "    # Early stopping\n",
        "    patience = args.patience if hasattr(args, 'patience') else 10  # Default patience of 10 epochs\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        train_loss = train(train_loader, net, optimizer, scheduler, criterion, criterion_MSE, epoch, args)\n",
        "        val_loss = validate(val_loader, net, criterion_MSE, args)\n",
        "        if args.scheduler == \"decay-lr\" or args.scheduler == \"multiplicative-lr\":\n",
        "            scheduler.step()\n",
        "\n",
        "        print(\"Epoch: \", epoch)\n",
        "        print(\"Train Loss: \", train_loss)\n",
        "        print(\"Val Loss: \", val_loss)\n",
        "        new_row = pd.DataFrame({'epoch': [epoch], 'train_loss': [train_loss], 'val_loss': [val_loss]})\n",
        "\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "\n",
        "        # Early Stopping Logic\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(net.state_dict(), models_dir)\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping triggered. No improvement in validation loss for {patience} epochs. Finished at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    df.to_csv(losses_dir, index=False)\n",
        "    print('Finished Training')"
      ],
      "metadata": {
        "id": "SNYVMuYwCfwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensuring code is running in same location as dataset**"
      ],
      "metadata": {
        "id": "CV3utPmJ86bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data set stored in google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "qtYI_0qkGa3n",
        "outputId": "43c109aa-a0b3-40a6-e087-33c844f8b597",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J1ZBnz0lm1w",
        "outputId": "62494dbd-ddfb-4899-c1e5-a7a525623c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/My\\ Drive/Colab\\ Notebooks/DeepeR-master/Raman Spectral Denoising"
      ],
      "metadata": {
        "id": "RAHNfyHBGcGD",
        "outputId": "9c244c0a-d007-4317-b69b-c73fb8d7798a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/DeepeR-master/Raman Spectral Denoising\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "kIeKqW8i82_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run training with the desired args\n",
        "\n",
        "class Arguments:\n",
        "    pass\n",
        "\n",
        "args = Arguments()\n",
        "args.workers = 0\n",
        "args.epochs = 500\n",
        "args.start_epoch = 0\n",
        "args.batch_size = 250\n",
        "args.network = \"ResUNet\"\n",
        "args.optimizer = \"adam\"\n",
        "args.lr = 5e-4\n",
        "args.base_lr = 5e-6\n",
        "args.scheduler = \"constant-lr\"\n",
        "args.batch_norm = True\n",
        "args.spectrum_len = 500\n",
        "args.seed = None\n",
        "args.gpu = 0\n",
        "args.world_size = -1\n",
        "args.rank = -1\n",
        "args.dist_url = \"tcp://224.66.41.62:23456\"\n",
        "args.dist_backend = \"nccl\"\n",
        "args.multiprocessing_distributed = False\n",
        "\n",
        "\n",
        "train_noKmeans(args)"
      ],
      "metadata": {
        "id": "HqEw59PrCzKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "iJpX5pZg8x9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "def evaluate(dataloader, net, args):\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    SG_loss = AverageMeter('Savitzky-Golay Loss', ':.4e')\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    MSE_SG = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(dataloader):\n",
        "            x = data['input_spectrum']\n",
        "            inputs = x.float()\n",
        "            inputs = inputs.cuda(args.gpu)\n",
        "            y = data['output_spectrum']\n",
        "            target = y.float()\n",
        "            target = target.cuda(args.gpu)\n",
        "\n",
        "            x = np.squeeze(x.numpy())\n",
        "            y = np.squeeze(y.numpy())\n",
        "\n",
        "            output = net(inputs)\n",
        "            loss = nn.MSELoss()(output, target)\n",
        "\n",
        "            x_out = output.cpu().detach().numpy()\n",
        "            x_out = np.squeeze(x_out)\n",
        "\n",
        "            SGF_1_9 = scipy.signal.savgol_filter(x,9,1)\n",
        "            MSE_SGF_1_9 = np.mean(np.mean(np.square(np.absolute(y - (SGF_1_9 - np.reshape(np.amin(SGF_1_9, axis = 1), (len(SGF_1_9),1)))))))\n",
        "            MSE_SG.append(MSE_SGF_1_9)\n",
        "\n",
        "\n",
        "\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "\n",
        "        print(\"Neural Network MSE: {}\".format(losses.avg))\n",
        "        print(\"Savitzky-Golay MSE: {}\".format(np.mean(np.asarray(MSE_SG))))\n",
        "        print(\"Neural Network performed {0:.2f}x better than Savitzky-Golay\".format(np.mean(np.asarray(MSE_SG))/losses.avg))\n",
        "\n",
        "    return losses.avg, MSE_SG"
      ],
      "metadata": {
        "id": "8ujeP58VmTxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_test(args):\n",
        "    gpu = args.gpu\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "\n",
        "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "    ngpus_per_node = torch.cuda.device_count()\n",
        "\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for testing\".format(args.gpu))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create model(s) and send to device(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "   if args.network == \"ResUNet\":\n",
        "        net = ResUNet(3, args.batch_norm).float()\n",
        "    if args.network == \"UNetPlusPlus1D\":\n",
        "        net = UNetPlusPlus1D().float()\n",
        "    elif args.network == \"UNet1D\":\n",
        "        net = UNet1D().float()\n",
        "    elif args.network == \"AttentionUNet\":\n",
        "        net = AttentionUNet().float()\n",
        "    elif args.network == \"AttentionUNet2\":\n",
        "        net = AttentionUNet2().float()\n",
        "    net.load_state_dict(torch.load(args.model))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.gpu is not None:\n",
        "            torch.cuda.set_device(args.gpu)\n",
        "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.gpu])\n",
        "        else:\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "    elif args.gpu is not None:\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "        net.cuda(args.gpu)\n",
        "    else:\n",
        "        net.cuda(args.gpu)\n",
        "        net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define dataset path and data splits\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Input_Data = scipy.io.loadmat(\"Dataset/Test_Inputs.mat\")\n",
        "    Output_Data = scipy.io.loadmat(\"Dataset/Test_Outputs.mat\")\n",
        "\n",
        "    Input = Input_Data['Test_Inputs']\n",
        "    Output = Output_Data['Test_Outputs']\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create datasets (with augmentation) and dataloaders\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Raman_Dataset_Test = RamanDataset(Input, Output, batch_size = args.batch_size, spectrum_len = args.spectrum_len)\n",
        "\n",
        "    test_loader = DataLoader(Raman_Dataset_Test, batch_size = args.batch_size, shuffle = False, num_workers = 0, pin_memory = True)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Evaluate\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    MSE_NN, MSE_SG = evaluate(test_loader, net, args)"
      ],
      "metadata": {
        "id": "pGV-EuKysDBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run testing with provided args\n",
        "class Arguments:\n",
        "    pass\n",
        "\n",
        "args = Arguments()\n",
        "args.workers = 0\n",
        "args.batch_size = 256\n",
        "args.spectrum_len = 500\n",
        "args.seed = None\n",
        "args.gpu = 0\n",
        "args.world_size = -1\n",
        "args.rank = -1\n",
        "args.dist_url = \"tcp://224.66.41.62:23456\"\n",
        "args.dist_backend = \"nccl\"\n",
        "args.multiprocessing_distributed = False\n",
        "args.batch_norm = True\n",
        "args.network = \"ResUNet\"\n",
        "args.model = \"__.pt\"\n",
        "\n",
        "\n",
        "main_test(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I97mxHSasvCS",
        "outputId": "51579610-7f34-4ab3-eb83-d6f77d1a3f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: 0 for testing\n",
            "Neural Network MSE: 0.00260821853749535\n",
            "Neural Network PSNR: 26.159036795137737\n",
            "Neural Network SSIM: 0.2734942460924173\n",
            "Savitzky-Golay MSE: 0.027660622850368643\n",
            "Neural Network performed 10.61x better than Savitzky-Golay\n"
          ]
        }
      ]
    }
  ]
}
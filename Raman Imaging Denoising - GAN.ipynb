{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1GHdCjqh84Xexa4GTzb8qntXQ08wgcIgd",
      "authorship_tag": "ABX9TyMSd9Fvuo+V/At0/tMF5/vT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiverBotham/Raman/blob/main/Raman%20Imaging%20Denoising%20-%20GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Denoising - GAN\n",
        "\n",
        "\n",
        "*   Generator -> can this be existing network?\n",
        "*   Descriminator: todo\n",
        "*   Update training\n",
        "\n"
      ],
      "metadata": {
        "id": "D077tOtY-b6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To save forst clone the repo\n",
        "!git config --global user.name \"RiverBotham\"\n",
        "!git config --global user.email \"river.botham@gmail.com\"\n",
        "!git config --global user.password \"MY_PASSWORD\"\n",
        "\n",
        "token = 'MY_TOKEN'\n",
        "username = 'RiverBotham'\n",
        "repo = 'Raman'\n",
        "\n",
        "!git clone https://{token}@github.com/{username}/{repo}"
      ],
      "metadata": {
        "id": "q-lUBSc7BMNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ad8b5d-6660-4e73-cba8-1ebc6ec0f3fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Raman'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 71 (delta 37), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (71/71), 7.44 MiB | 7.11 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move into the cloned repo, then File -> Save copy in GitHub\n",
        "%cd {repo}/Denoising"
      ],
      "metadata": {
        "id": "zzLakAJlDBnY",
        "outputId": "c7c9f072-27ec-41ee-e839-6d3fbf44b28b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Raman/Denoising\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import datetime\n",
        "import time\n",
        "import shutil\n",
        "import argparse\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import scipy.signal\n",
        "import math\n",
        "from skimage.metrics import structural_similarity as sk_ssim\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.distributed as dist\n",
        "import torch.utils.data.distributed\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms, utils\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# import model, dataset, utilities"
      ],
      "metadata": {
        "id": "KWeXO7nYDF8j"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "\n",
        "\n",
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, channels_in, channels_out, batch_norm):\n",
        "        super(BasicConv, self).__init__()\n",
        "        basic_conv = [nn.Conv1d(channels_in, channels_out, kernel_size = 3, stride = 1, padding = 1, bias = True)]\n",
        "        basic_conv.append(nn.PReLU())\n",
        "        if batch_norm:\n",
        "            basic_conv.append(nn.BatchNorm1d(channels_out))\n",
        "\n",
        "        self.body = nn.Sequential(*basic_conv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n",
        "\n",
        "class ResUNetConv(nn.Module):\n",
        "    def __init__(self, num_convs, channels, batch_norm):\n",
        "        super(ResUNetConv, self).__init__()\n",
        "        unet_conv = []\n",
        "        for _ in range(num_convs):\n",
        "            unet_conv.append(nn.Conv1d(channels, channels, kernel_size = 3, stride = 1, padding = 1, bias = True))\n",
        "            unet_conv.append(nn.PReLU())\n",
        "            if batch_norm:\n",
        "                unet_conv.append(nn.BatchNorm1d(channels))\n",
        "\n",
        "        self.body = nn.Sequential(*unet_conv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.body(x)\n",
        "        res += x\n",
        "        return res\n",
        "\n",
        "class UNetLinear(nn.Module):\n",
        "    def __init__(self, repeats, channels_in, channels_out):\n",
        "        super().__init__()\n",
        "        modules = []\n",
        "        for i in range(repeats):\n",
        "            modules.append(nn.Linear(channels_in, channels_out))\n",
        "            modules.append(nn.PReLU())\n",
        "\n",
        "        self.body = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.body(x)\n",
        "        return x\n",
        "\n",
        "class ResUNet(nn.Module):\n",
        "    def __init__(self, num_convs, batch_norm):\n",
        "        super(ResUNet, self).__init__()\n",
        "        res_conv1 = [BasicConv(1, 64, batch_norm)]\n",
        "        res_conv1.append(ResUNetConv(num_convs, 64, batch_norm))\n",
        "        self.conv1 = nn.Sequential(*res_conv1)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        res_conv2 = [BasicConv(64, 128, batch_norm)]\n",
        "        res_conv2.append(ResUNetConv(num_convs, 128, batch_norm))\n",
        "        self.conv2 = nn.Sequential(*res_conv2)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "\n",
        "        res_conv3 = [BasicConv(128, 256, batch_norm)]\n",
        "        res_conv3.append(ResUNetConv(num_convs, 256, batch_norm))\n",
        "        res_conv3.append(BasicConv(256, 128, batch_norm))\n",
        "        self.conv3 = nn.Sequential(*res_conv3)\n",
        "        self.up3 = nn.Upsample(scale_factor = 2)\n",
        "\n",
        "        res_conv4 = [BasicConv(256, 128, batch_norm)]\n",
        "        res_conv4.append(ResUNetConv(num_convs, 128, batch_norm))\n",
        "        res_conv4.append(BasicConv(128, 64, batch_norm))\n",
        "        self.conv4 = nn.Sequential(*res_conv4)\n",
        "        self.up4 = nn.Upsample(scale_factor = 2)\n",
        "\n",
        "        res_conv5 = [BasicConv(128, 64, batch_norm)]\n",
        "        res_conv5.append(ResUNetConv(num_convs,64, batch_norm))\n",
        "        self.conv5 = nn.Sequential(*res_conv5)\n",
        "        res_conv6 = [BasicConv(64, 1, batch_norm)]\n",
        "        self.conv6 = nn.Sequential(*res_conv6)\n",
        "\n",
        "        self.linear7 = UNetLinear(3, 500, 500)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x1 = self.pool1(x)\n",
        "\n",
        "        x2 = self.conv2(x1)\n",
        "        x3 = self.pool1(x2)\n",
        "\n",
        "        x3 = self.conv3(x3)\n",
        "        x3 = self.up3(x3)\n",
        "\n",
        "        x4 = torch.cat((x2, x3), dim = 1)\n",
        "        x4 = self.conv4(x4)\n",
        "        x5 = self.up4(x4)\n",
        "\n",
        "        x6 = torch.cat((x, x5), dim = 1)\n",
        "        x6 = self.conv5(x6)\n",
        "        x7 = self.conv6(x6)\n",
        "\n",
        "        out = self.linear7(x7)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "Mei5k_ff0fci"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Discriminator\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # Input: [batch_size, 1, signal_length]\n",
        "            nn.Conv1d(1, 64, kernel_size=7, stride=1, padding=3, bias=True),  # Wider kernel for initial low-level feature extraction\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2, bias=False),  # Reducing signal length by factor of 2\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv1d(128, 256, kernel_size=5, stride=2, padding=2, bias=False),  # Further reducing signal length\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv1d(256, 512, kernel_size=5, stride=2, padding=2, bias=False),  # High-level feature extraction\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Adaptive pooling to compress features, depending on the input size (signal length)\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "\n",
        "            nn.Conv1d(512, 1, kernel_size=1, stride=1, bias=True),  # Reduce to a single output\n",
        "            # nn.Sigmoid()  # Sigmoid for binary classification -> don't need as using BCEWithLogitsLoss\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)  # Forward pass through the network\n",
        "        return output.squeeze(-1)"
      ],
      "metadata": {
        "id": "m3Odch31fXau"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data set\n",
        "\n",
        "class RamanDataset(Dataset):\n",
        "    def __init__(self, inputs, outputs, batch_size=64,spectrum_len=500, spectrum_shift=0.,\n",
        "                 spectrum_window=False, horizontal_flip=False, mixup=False):\n",
        "        self.inputs = inputs\n",
        "        self.outputs = outputs\n",
        "        self.batch_size = batch_size\n",
        "        self.spectrum_len = spectrum_len\n",
        "        self.spectrum_shift = spectrum_shift\n",
        "        self.spectrum_window = spectrum_window\n",
        "        self.horizontal_flip = horizontal_flip\n",
        "        self.mixup = mixup\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def pad_spectrum(self, input_spectrum, spectrum_length):\n",
        "        if len(input_spectrum) == spectrum_length:\n",
        "            padded_spectrum = input_spectrum\n",
        "        elif len(input_spectrum) > spectrum_length:\n",
        "            padded_spectrum = input_spectrum[0:spectrum_length]\n",
        "        else:\n",
        "            padded_spectrum = np.pad(input_spectrum, ((0,spectrum_length - len(input_spectrum)),(0,0)), 'reflect')\n",
        "\n",
        "        return padded_spectrum\n",
        "\n",
        "    def window_spectrum(self, input_spectrum, start_idx, window_length):\n",
        "        if len(input_spectrum) <= window_length:\n",
        "            output_spectrum = input_spectrum\n",
        "        else:\n",
        "            end_idx = start_idx + window_length\n",
        "            output_spectrum = input_spectrum[start_idx:end_idx]\n",
        "\n",
        "        return output_spectrum\n",
        "\n",
        "    def flip_axis(self, x, axis):\n",
        "        if np.random.random() < 0.5:\n",
        "            x = np.asarray(x).swapaxes(axis, 0)\n",
        "            x = x[::-1, ...]\n",
        "            x = x.swapaxes(0, axis)\n",
        "        return x\n",
        "\n",
        "    def shift_spectrum(self, x, shift_range):\n",
        "        x = np.expand_dims(x,axis=-1)\n",
        "        shifted_spectrum = x\n",
        "        spectrum_shift_range = int(np.round(shift_range*len(x)))\n",
        "        if spectrum_shift_range > 0:\n",
        "            shifted_spectrum = np.pad(x[spectrum_shift_range:,:], ((0,abs(spectrum_shift_range)), (0,0)), 'reflect')\n",
        "        elif spectrum_shift_range < 0:\n",
        "            shifted_spectrum = np.pad(x[:spectrum_shift_range,:], ((abs(spectrum_shift_range), 0), (0,0)), 'reflect')\n",
        "        return shifted_spectrum\n",
        "\n",
        "    def mixup_spectrum(self, input_spectrum1, input_spectrum2, output_spectrum1, output_spectrum2, alpha):\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        input_spectrum = (lam * input_spectrum1) + ((1 - lam) * input_spectrum2)\n",
        "        output_spectrum = (lam * output_spectrum1) + ((1 - lam) * output_spectrum2)\n",
        "        return input_spectrum, output_spectrum\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_spectrum = self.inputs[index]\n",
        "        output_spectrum = self.outputs[index]\n",
        "\n",
        "        mixup_on = False\n",
        "        if self.mixup:\n",
        "            if np.random.random() < 0.5:\n",
        "                spectrum_idx = int(np.round(np.random.random() * (len(self.inputs)-1)))\n",
        "                input_spectrum2 = self.inputs[spectrum_idx]\n",
        "                output_spectrum2 = self.outputs[spectrum_idx]\n",
        "                mixup_on = True\n",
        "\n",
        "        if self.spectrum_window:\n",
        "            start_idx = int(np.floor(np.random.random() * (len(input_spectrum)-self.spectrum_len)))\n",
        "            input_spectrum = self.window_spectrum(input_spectrum, start_idx, self.spectrum_len)\n",
        "            output_spectrum = self.window_spectrum(output_spectrum, start_idx, self.spectrum_len)\n",
        "            if mixup_on:\n",
        "                input_spectrum2 = self.window_spectrum(input_spectrum2, start_idx, self.spectrum_len)\n",
        "                output_spectrum2 = self.window_spectrum(output_spectrum2, start_idx, self.spectrum_len)\n",
        "\n",
        "        input_spectrum = self.pad_spectrum(input_spectrum, self.spectrum_len)\n",
        "        output_spectrum = self.pad_spectrum(output_spectrum, self.spectrum_len)\n",
        "        if mixup_on:\n",
        "            input_spectrum2 = self.pad_spectrum(input_spectrum2, self.spectrum_len)\n",
        "            output_spectrum2 = self.pad_spectrum(output_spectrum2, self.spectrum_len)\n",
        "\n",
        "        if self.spectrum_shift != 0.0:\n",
        "            shift_range = np.random.uniform(-self.spectrum_shift, self.spectrum_shift)\n",
        "            input_spectrum = self.shift_spectrum(input_spectrum, shift_range)\n",
        "            output_spectrum = self.shift_spectrum(output_spectrum, shift_range)\n",
        "            if mixup_on:\n",
        "                input_spectrum2 = self.shift_spectrum(input_spectrum2, shift_range)\n",
        "                output_spectrum2 = self.shift_spectrum(output_spectrum2, shift_range)\n",
        "        else:\n",
        "            input_spectrum = np.expand_dims(input_spectrum, axis=-1)\n",
        "            output_spectrum = np.expand_dims(output_spectrum, axis=-1)\n",
        "            if mixup_on:\n",
        "                input_spectrum2 = np.expand_dims(input_spectrum2, axis=-1)\n",
        "                output_spectrum2 = np.expand_dims(output_spectrum2, axis=-1)\n",
        "\n",
        "        if self.horizontal_flip:\n",
        "            if np.random.random() < 0.5:\n",
        "                input_spectrum = self.flip_axis(input_spectrum, 0)\n",
        "                output_spectrum = self.flip_axis(output_spectrum, 0)\n",
        "                if mixup_on:\n",
        "                    input_spectrum2 = self.flip_axis(input_spectrum2, 0)\n",
        "                    output_spectrum2 = self.flip_axis(output_spectrum2, 0)\n",
        "\n",
        "        if mixup_on:\n",
        "            input_spectrum, output_spectrum = self.mixup_spectrum(input_spectrum, input_spectrum2, output_spectrum, output_spectrum2, 0.2)\n",
        "\n",
        "        input_spectrum = input_spectrum/np.amax(input_spectrum)\n",
        "        output_spectrum = output_spectrum/np.amax(output_spectrum)\n",
        "\n",
        "        input_spectrum = np.moveaxis(input_spectrum, -1, 0)\n",
        "        output_spectrum = np.moveaxis(output_spectrum, -1, 0)\n",
        "\n",
        "        sample = {'input_spectrum': input_spectrum, 'output_spectrum': output_spectrum}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)"
      ],
      "metadata": {
        "id": "w6PAklAu0hNJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utilities\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)"
      ],
      "metadata": {
        "id": "3NAulroG0i29"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, generator, generator_optimizer, discriminator, discriminator_optimizer, generator_scheduler, discriminator_scheduler, criterion, criterion_MSE, epoch, args):\n",
        "\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses_gen_mse = AverageMeter('Loss GEN MSE', ':.4e')\n",
        "    losses_gen = AverageMeter('Loss GEN', ':.4e')\n",
        "    losses_dis = AverageMeter('Loss DIS', ':.4e')\n",
        "    progress = ProgressMeter(len(dataloader), [batch_time, losses_gen_mse, losses_gen, losses_dis], prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    end = time.time()\n",
        "    scaler = GradScaler()  # Initialize the GradScaler\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "\n",
        "        inputs = data['input_spectrum'].float().cuda(args.gpu)\n",
        "        target = data['output_spectrum'].float().cuda(args.gpu)\n",
        "\n",
        "        # Update Discriminator\n",
        "        discriminator_optimizer.zero_grad()\n",
        "\n",
        "        with autocast():  # Use autocast for discriminator operations\n",
        "            label = torch.ones(inputs.size(0), 1, device=inputs.device)\n",
        "            real_outputs = discriminator(target)\n",
        "            real_loss = criterion(real_outputs, label)\n",
        "\n",
        "            fake_images = generator(inputs)\n",
        "            fake_labels = torch.zeros(inputs.size(0), 1, device=inputs.device)\n",
        "            fake_outputs = discriminator(fake_images.detach())\n",
        "            fake_loss = criterion(fake_outputs, fake_labels)\n",
        "\n",
        "        # Backward pass and optimization step for Discriminator\n",
        "        scaler.scale(real_loss + fake_loss).backward()\n",
        "        scaler.step(discriminator_optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update Generator\n",
        "        generator_optimizer.zero_grad()\n",
        "\n",
        "        with autocast():  # Use autocast for generator operations\n",
        "            fake_labels = torch.ones(inputs.size(0), 1, device=inputs.device)\n",
        "            fake_outputs = discriminator(fake_images)\n",
        "            gen_loss = criterion(fake_outputs, fake_labels)\n",
        "\n",
        "        # Backward pass and optimization step for Generator\n",
        "        scaler.scale(gen_loss).backward()\n",
        "        scaler.step(generator_optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if args.scheduler == \"cyclic-lr\" or args.scheduler == \"one-cycle-lr\":\n",
        "            generator_scheduler.step()\n",
        "            discriminator_scheduler.step()\n",
        "\n",
        "        # Calculate and log losses\n",
        "        with torch.no_grad():  # No need to track gradients for loss calculation\n",
        "            loss_GEN_MSE = criterion_MSE(fake_images, target)\n",
        "            losses_gen_mse.update(loss_GEN_MSE.item(), inputs.size(0))\n",
        "            losses_gen.update(gen_loss.item(), inputs.size(0))\n",
        "            losses_dis.update(fake_loss.item(), inputs.size(0))\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 400 == 0:\n",
        "            progress.display(i)\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return losses_gen_mse.avg\n",
        "\n"
      ],
      "metadata": {
        "id": "smrIJIsQDp3c"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(dataloader, net, criterion_MSE, args):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    progress = ProgressMeter(len(dataloader), [batch_time, losses], prefix='Validation: ')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, data in enumerate(dataloader):\n",
        "            inputs = data['input_spectrum']\n",
        "            inputs = inputs.float()\n",
        "            inputs = inputs.cuda(args.gpu)\n",
        "            target = data['output_spectrum']\n",
        "            target = target.float()\n",
        "            target = target.cuda(args.gpu)\n",
        "\n",
        "            output = net(inputs)\n",
        "\n",
        "            loss_MSE = criterion_MSE(output, target)\n",
        "            losses.update(loss_MSE.item(), inputs.size(0))\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % 400 == 0:\n",
        "                progress.display(i)\n",
        "\n",
        "    return losses.avg"
      ],
      "metadata": {
        "id": "ZvHvb_vkD2Wm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_noKmeans(args):\n",
        "\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "\n",
        "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "    ngpus_per_node = torch.cuda.device_count()\n",
        "\n",
        "    gpu = args.gpu\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create model(s) and send to device(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    generator = ResUNet(3, args.batch_norm).float()\n",
        "    discriminator = Discriminator(1).float()\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.gpu is not None:\n",
        "            torch.cuda.set_device(args.gpu)\n",
        "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "\n",
        "            generator.cuda(args.gpu)\n",
        "            generator = torch.nn.parallel.DistributedDataParallel(generator, device_ids=[args.gpu])\n",
        "            discriminator.cuda(args.gpu)\n",
        "            discriminator = torch.nn.parallel.DistributedDataParallel(discriminator, device_ids=[args.gpu])\n",
        "        else:\n",
        "            generator.cuda(args.gpu)\n",
        "            generator = torch.nn.parallel.DistributedDataParallel(generator)\n",
        "            discriminator.cuda(args.gpu)\n",
        "            discriminator = torch.nn.parallel.DistributedDataParallel(discriminator)\n",
        "    elif args.gpu is not None:\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "        generator.cuda(args.gpu)\n",
        "        discriminator.cuda(args.gpu)\n",
        "    else:\n",
        "        generator.cuda(args.gpu)\n",
        "        generator = torch.nn.parallel.DistributedDataParallel(generator)\n",
        "        discriminator.cuda(args.gpu)\n",
        "        discriminator = torch.nn.parallel.DistributedDataParallel(discriminator)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define dataset path and data splits\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Input_Data = scipy.io.loadmat(\"Dataset/Train_Inputs.mat\")\n",
        "    Output_Data = scipy.io.loadmat(\"Dataset/Train_Outputs.mat\")\n",
        "\n",
        "    Input = Input_Data['Train_Inputs']\n",
        "    Output = Output_Data['Train_Outputs']\n",
        "\n",
        "    spectra_num = len(Input)\n",
        "\n",
        "    train_split = round(0.9 * spectra_num)\n",
        "    val_split = round(0.1 * spectra_num)\n",
        "\n",
        "    input_train = Input[:train_split]\n",
        "    input_val = Input[train_split:train_split+val_split]\n",
        "\n",
        "    output_train = Output[:train_split]\n",
        "    output_val = Output[train_split:train_split+val_split]\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create datasets (with augmentation) and dataloaders\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Raman_Dataset_Train = RamanDataset(input_train, output_train, batch_size = args.batch_size, spectrum_len = args.spectrum_len,\n",
        "                                   spectrum_shift=0.1, spectrum_window = False, horizontal_flip = False, mixup = True)\n",
        "\n",
        "    Raman_Dataset_Val = RamanDataset(input_val, output_val, batch_size = args.batch_size, spectrum_len = args.spectrum_len)\n",
        "\n",
        "# From here down per fold\n",
        "    train_loader = DataLoader(Raman_Dataset_Train, batch_size = args.batch_size, shuffle = False, num_workers = 4, pin_memory = True)\n",
        "    val_loader = DataLoader(Raman_Dataset_Val, batch_size = args.batch_size, shuffle = False, num_workers = 4, pin_memory = True)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define criterion(s), optimizer(s), and scheduler(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # criterion = nn.BCEWithLogitsLoss().cuda(args.gpu)\n",
        "    criterion = nn.MSELoss().cuda(args.gpu) # this makes it lsGAN\n",
        "    criterion_MSE = nn.MSELoss().cuda(args.gpu)\n",
        "    if args.optimizer == \"sgd\":\n",
        "        generator_optimizer = optim.SGD(generator.parameters(), lr = args.lr)\n",
        "        discriminator_optimizer = optim.SGD(discriminator.parameters(), lr = args.lr)\n",
        "    elif args.optimizer == \"adamW\":\n",
        "        generator_optimizer = optim.AdamW(generator.parameters(), lr = args.lr)\n",
        "        discriminator_optimizer = optim.AdamW(discriminator.parameters(), lr = args.lr)\n",
        "    else: # Adam\n",
        "        generator_optimizer = optim.Adam(generator.parameters(), lr = args.lr)\n",
        "        discriminator_optimizer = optim.Adam(discriminator.parameters(), lr = args.lr)\n",
        "\n",
        "    if args.scheduler == \"decay-lr\":\n",
        "        generator_scheduler = optim.lr_scheduler.StepLR(generator_optimizer, step_size=50, gamma=0.2)\n",
        "        discriminator_scheduler = optim.lr_scheduler.StepLR(discriminator_optimizer, step_size=50, gamma=0.2)\n",
        "    elif args.scheduler == \"multiplicative-lr\":\n",
        "        lmbda = lambda epoch: 0.985\n",
        "        generator_scheduler = optim.lr_scheduler.MultiplicativeLR(generator_optimizer, lr_lambda=lmbda)\n",
        "        discriminator_scheduler = optim.lr_scheduler.MultiplicativeLR(discriminator_optimizer, lr_lambda=lmbda)\n",
        "    elif args.scheduler == \"cyclic-lr\":\n",
        "        generator_scheduler = optim.lr_scheduler.CyclicLR(generator_optimizer, base_lr = args.base_lr, max_lr = args.lr, mode = 'triangular2', cycle_momentum = False)\n",
        "        discriminator_scheduler = optim.lr_scheduler.CyclicLR(discriminator_optimizer, base_lr = args.base_lr, max_lr = args.lr, mode = 'triangular2', cycle_momentum = False)\n",
        "    elif args.scheduler == \"one-cycle-lr\":\n",
        "        generator_scheduler = optim.lr_scheduler.OneCycleLR(generator_optimizer, max_lr = args.lr, steps_per_epoch=len(train_loader), epochs=args.epochs, cycle_momentum = False)\n",
        "        discriminator_scheduler = optim.lr_scheduler.OneCycleLR(discriminator_optimizer, max_lr = args.lr, steps_per_epoch=len(train_loader), epochs=args.epochs, cycle_momentum = False)\n",
        "    else: # constant-lr\n",
        "        generator_scheduler = None\n",
        "        discriminator_scheduler = None\n",
        "\n",
        "    print('Started Training')\n",
        "    print('Training Details:')\n",
        "    print('Network:         {}'.format(args.network))\n",
        "    print('Epochs:          {}'.format(args.epochs))\n",
        "    print('Batch Size:      {}'.format(args.batch_size))\n",
        "    print('Optimizer:       {}'.format(args.optimizer))\n",
        "    print('Scheduler:       {}'.format(args.scheduler))\n",
        "    print('Learning Rate:   {}'.format(args.lr))\n",
        "    print('Spectrum Length: {}'.format(args.spectrum_len))\n",
        "\n",
        "    DATE = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
        "\n",
        "    log_dir = \"runs/{}_{}_{}_{}\".format(DATE, args.optimizer, args.scheduler, args.network)\n",
        "    models_dir = \"{}_{}_{}_{}.pt\".format(DATE, args.optimizer, args.scheduler, args.network)\n",
        "\n",
        "    writer = SummaryWriter(log_dir = log_dir)\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        train_loss = train(train_loader, generator, generator_optimizer, discriminator, discriminator_optimizer, generator_scheduler, discriminator_scheduler, criterion, criterion_MSE, epoch, args)\n",
        "        val_loss = validate(val_loader, generator, criterion_MSE, args)\n",
        "        if args.scheduler == \"decay-lr\" or args.scheduler == \"multiplicative-lr\":\n",
        "            generator_scheduler.step()\n",
        "            discriminator_scheduler.step()\n",
        "\n",
        "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    torch.save(generator.state_dict(), models_dir)\n",
        "    print('Finished Training')"
      ],
      "metadata": {
        "id": "SNYVMuYwCfwP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_kmeans(args, k_folds = 5):\n",
        "\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "\n",
        "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "    ngpus_per_node = torch.cuda.device_count()\n",
        "\n",
        "    gpu = args.gpu\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define dataset path and data splits\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Input_Data = scipy.io.loadmat(\"Dataset/Train_Inputs.mat\")\n",
        "    Output_Data = scipy.io.loadmat(\"Dataset/Train_Outputs.mat\")\n",
        "\n",
        "    Input = Input_Data['Train_Inputs']\n",
        "    Output = Output_Data['Train_Outputs']\n",
        "\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create datasets (with augmentation) and dataloaders\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Raman_Dataset_Train = RamanDataset(Input, Output, batch_size = args.batch_size, spectrum_len = args.spectrum_len,\n",
        "                                   spectrum_shift=0.1, spectrum_window = False, horizontal_flip = False, mixup = True)\n",
        "\n",
        "    # Raman_Dataset_Val = RamanDataset(input_val, output_val, batch_size = args.batch_size, spectrum_len = args.spectrum_len)\n",
        "\n",
        "# From here down per fold\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(Raman_Dataset_Train)):\n",
        "      print(f\"Fold {fold + 1}\")\n",
        "      print(\"-------\")\n",
        "\n",
        "      train_loader = DataLoader(Raman_Dataset_Train, batch_size = args.batch_size, shuffle = False, num_workers = 4, pin_memory = True, sampler=torch.utils.data.SubsetRandomSampler(train_idx))\n",
        "      val_loader = DataLoader(Raman_Dataset_Train, batch_size = args.batch_size, shuffle = False, num_workers = 4, pin_memory = True, sampler=torch.utils.data.SubsetRandomSampler(test_idx))\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create model(s) and send to device(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "      generator = ResUNet(3, args.batch_norm).float()\n",
        "      discriminator = Discriminator(1).float()\n",
        "\n",
        "      if args.distributed:\n",
        "          if args.gpu is not None:\n",
        "              torch.cuda.set_device(args.gpu)\n",
        "              args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "              args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "\n",
        "              generator.cuda(args.gpu)\n",
        "              generator = torch.nn.parallel.DistributedDataParallel(generator, device_ids=[args.gpu])\n",
        "              discriminator.cuda(args.gpu)\n",
        "              discriminator = torch.nn.parallel.DistributedDataParallel(discriminator, device_ids=[args.gpu])\n",
        "          else:\n",
        "              generator.cuda(args.gpu)\n",
        "              generator = torch.nn.parallel.DistributedDataParallel(generator)\n",
        "              discriminator.cuda(args.gpu)\n",
        "              discriminator = torch.nn.parallel.DistributedDataParallel(discriminator)\n",
        "      elif args.gpu is not None:\n",
        "          torch.cuda.set_device(args.gpu)\n",
        "          generator.cuda(args.gpu)\n",
        "          discriminator.cuda(args.gpu)\n",
        "      else:\n",
        "          generator.cuda(args.gpu)\n",
        "          generator = torch.nn.parallel.DistributedDataParallel(generator)\n",
        "          discriminator.cuda(args.gpu)\n",
        "          discriminator = torch.nn.parallel.DistributedDataParallel(discriminator)\n",
        "\n",
        "\n",
        "\n",
        "      # ----------------------------------------------------------------------------------------\n",
        "      # Define criterion(s), optimizer(s), and scheduler(s)\n",
        "      # ----------------------------------------------------------------------------------------\n",
        "      # criterion = nn.BCEWithLogitsLoss().cuda(args.gpu)\n",
        "      criterion = nn.MSELoss().cuda(args.gpu) # this makes it lsGAN\n",
        "      criterion_MSE = nn.MSELoss().cuda(args.gpu)\n",
        "      if args.optimizer == \"sgd\":\n",
        "          generator_optimizer = optim.SGD(generator.parameters(), lr = args.lr)\n",
        "          discriminator_optimizer = optim.SGD(discriminator.parameters(), lr = args.lr)\n",
        "      elif args.optimizer == \"adamW\":\n",
        "          generator_optimizer = optim.AdamW(generator.parameters(), lr = args.lr)\n",
        "          discriminator_optimizer = optim.AdamW(discriminator.parameters(), lr = args.lr)\n",
        "      else: # Adam\n",
        "          generator_optimizer = optim.Adam(generator.parameters(), lr = args.lr)\n",
        "          discriminator_optimizer = optim.Adam(discriminator.parameters(), lr = args.lr)\n",
        "\n",
        "      if args.scheduler == \"decay-lr\":\n",
        "          generator_scheduler = optim.lr_scheduler.StepLR(generator_optimizer, step_size=50, gamma=0.2)\n",
        "          discriminator_scheduler = optim.lr_scheduler.StepLR(discriminator_optimizer, step_size=50, gamma=0.2)\n",
        "      elif args.scheduler == \"multiplicative-lr\":\n",
        "          lmbda = lambda epoch: 0.985\n",
        "          generator_scheduler = optim.lr_scheduler.MultiplicativeLR(generator_optimizer, lr_lambda=lmbda)\n",
        "          discriminator_scheduler = optim.lr_scheduler.MultiplicativeLR(discriminator_optimizer, lr_lambda=lmbda)\n",
        "      elif args.scheduler == \"cyclic-lr\":\n",
        "          generator_scheduler = optim.lr_scheduler.CyclicLR(generator_optimizer, base_lr = args.base_lr, max_lr = args.lr, mode = 'triangular2', cycle_momentum = False)\n",
        "          discriminator_scheduler = optim.lr_scheduler.CyclicLR(discriminator_optimizer, base_lr = args.base_lr, max_lr = args.lr, mode = 'triangular2', cycle_momentum = False)\n",
        "      elif args.scheduler == \"one-cycle-lr\":\n",
        "          generator_scheduler = optim.lr_scheduler.OneCycleLR(generator_optimizer, max_lr = args.lr, steps_per_epoch=len(train_loader), epochs=args.epochs, cycle_momentum = False)\n",
        "          discriminator_scheduler = optim.lr_scheduler.OneCycleLR(discriminator_optimizer, max_lr = args.lr, steps_per_epoch=len(train_loader), epochs=args.epochs, cycle_momentum = False)\n",
        "      else: # constant-lr\n",
        "          generator_scheduler = None\n",
        "          discriminator_scheduler = None\n",
        "\n",
        "      print('Started Training')\n",
        "      print('Training Details:')\n",
        "      print('Network:         {}'.format(args.network))\n",
        "      print('Epochs:          {}'.format(args.epochs))\n",
        "      print('Batch Size:      {}'.format(args.batch_size))\n",
        "      print('Optimizer:       {}'.format(args.optimizer))\n",
        "      print('Scheduler:       {}'.format(args.scheduler))\n",
        "      print('Learning Rate:   {}'.format(args.lr))\n",
        "      print('Spectrum Length: {}'.format(args.spectrum_len))\n",
        "\n",
        "      DATE = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
        "\n",
        "      log_dir = \"runs/{}_{}_{}_{}_{}\".format(DATE, args.optimizer, args.scheduler, args.network, fold + 1)\n",
        "      models_dir = \"{}_{}_{}_{}_.pt\".format(DATE, args.optimizer, args.scheduler, args.network, fold + 1)\n",
        "\n",
        "      writer = SummaryWriter(log_dir = log_dir)\n",
        "\n",
        "      for epoch in range(args.epochs):\n",
        "          train_loss = train(train_loader, generator, generator_optimizer, discriminator, discriminator_optimizer, generator_scheduler, discriminator_scheduler, criterion, criterion_MSE, epoch, args)\n",
        "          val_loss = validate(val_loader, generator, criterion_MSE, args)\n",
        "          if args.scheduler == \"decay-lr\" or args.scheduler == \"multiplicative-lr\":\n",
        "              generator_scheduler.step()\n",
        "              discriminator_scheduler.step()\n",
        "\n",
        "          # TODO: log out the train and validation for the epoch\n",
        "          writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "          writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "      torch.save(generator.state_dict(), models_dir)\n",
        "      print('Finished Training')"
      ],
      "metadata": {
        "id": "HDDCxx3lkV3F"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "qtYI_0qkGa3n",
        "outputId": "c7f76625-f0c6-4d28-a288-35657f2be88e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "O1sjKibTHrPN",
        "outputId": "fc0602ba-0774-4dfa-f605-2f9cdd99ed1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset.py  model.py  ResUNet.pt  utilities.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J1ZBnz0lm1w",
        "outputId": "3751c0d8-8779-480e-d972-f152a7558c4e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/My\\ Drive/Colab\\ Notebooks/DeepeR-master/Raman Spectral Denoising"
      ],
      "metadata": {
        "id": "RAHNfyHBGcGD",
        "outputId": "b46edd09-d4b7-4da2-88cc-7f2518b1cd46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/DeepeR-master/Raman Spectral Denoising\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Default args from original code\n",
        "#Namespace(workers=0, epochs=2, start_epoch=0, batch_size=256, network='ResUNet', optimizer='adam', lr=0.0005, base_lr=5e-06, scheduler='one-cycle-lr', batch_norm=True, spectrum_len=500, seed=None, gpu=0, world_size=-1, rank=-1, dist_url='tcp://224.66.41.62:23456', dist_backend='nccl', multiprocessing_distributed=False)\n",
        "\n",
        "class Arguments:\n",
        "    pass\n",
        "\n",
        "args = Arguments()\n",
        "args.workers = 0\n",
        "args.epochs = 500\n",
        "args.start_epoch = 0\n",
        "args.batch_size = 250\n",
        "args.network = \"ResUNet\"\n",
        "args.optimizer = \"adam\"\n",
        "args.lr = 5e-4\n",
        "args.base_lr = 5e-6\n",
        "args.scheduler = \"constant-lr\"\n",
        "args.batch_norm = True\n",
        "args.spectrum_len = 500\n",
        "args.seed = None\n",
        "args.gpu = 0\n",
        "args.world_size = -1\n",
        "args.rank = -1\n",
        "args.dist_url = \"tcp://224.66.41.62:23456\"\n",
        "args.dist_backend = \"nccl\"\n",
        "args.multiprocessing_distributed = False\n",
        "\n",
        "\n",
        "# args.epochs=10\n",
        "train_noKmeans(args)\n",
        "# train_kmeans(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqEw59PrCzKG",
        "outputId": "387bcd34-e750-48c9-8f4c-7155cdad53f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: 0 for training\n",
            "Started Training\n",
            "Training Details:\n",
            "Network:         ResUNet\n",
            "Epochs:          500\n",
            "Batch Size:      250\n",
            "Optimizer:       adam\n",
            "Scheduler:       constant-lr\n",
            "Learning Rate:   0.0005\n",
            "Spectrum Length: 500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-668f3e0b1e9a>:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # Initialize the GradScaler\n",
            "<ipython-input-22-668f3e0b1e9a>:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Use autocast for discriminator operations\n",
            "<ipython-input-22-668f3e0b1e9a>:38: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Use autocast for generator operations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][  0/575]\tTime  0.515 ( 0.515)\tLoss GEN MSE 1.5133e-01 (1.5133e-01)\tLoss GEN 1.5087e+00 (1.5087e+00)\tLoss DIS 5.3305e-02 (5.3305e-02)\n",
            "Epoch: [0][400/575]\tTime  0.060 ( 0.061)\tLoss GEN MSE 8.3099e-02 (8.0065e-01)\tLoss GEN 3.9190e-01 (5.6136e-01)\tLoss DIS 1.2685e-01 (1.3816e-01)\n",
            "Validation: [ 0/64]\tTime  0.368 ( 0.368)\tLoss 8.0327e-02 (8.0327e-02)\n",
            "Epoch: [1][  0/575]\tTime  0.516 ( 0.516)\tLoss GEN MSE 9.6523e-02 (9.6523e-02)\tLoss GEN 3.4648e-01 (3.4648e-01)\tLoss DIS 1.8838e-01 (1.8838e-01)\n",
            "Epoch: [1][400/575]\tTime  0.059 ( 0.060)\tLoss GEN MSE 8.0487e-02 (8.2860e-02)\tLoss GEN 3.3705e-01 (4.1120e-01)\tLoss DIS 2.5651e-01 (1.7804e-01)\n",
            "Validation: [ 0/64]\tTime  0.352 ( 0.352)\tLoss 1.1657e-01 (1.1657e-01)\n",
            "Epoch: [2][  0/575]\tTime  0.484 ( 0.484)\tLoss GEN MSE 1.0947e-01 (1.0947e-01)\tLoss GEN 4.9282e-01 (4.9282e-01)\tLoss DIS 1.6679e-01 (1.6679e-01)\n",
            "Epoch: [2][400/575]\tTime  0.058 ( 0.059)\tLoss GEN MSE 1.1345e-01 (1.0289e-01)\tLoss GEN 2.6488e-01 (3.8763e-01)\tLoss DIS 2.5411e-01 (1.9393e-01)\n",
            "Validation: [ 0/64]\tTime  0.375 ( 0.375)\tLoss 7.7165e-02 (7.7165e-02)\n",
            "Epoch: [3][  0/575]\tTime  0.482 ( 0.482)\tLoss GEN MSE 8.6454e-02 (8.6454e-02)\tLoss GEN 2.3195e-01 (2.3195e-01)\tLoss DIS 2.7665e-01 (2.7665e-01)\n",
            "Epoch: [3][400/575]\tTime  0.058 ( 0.059)\tLoss GEN MSE 1.2833e-01 (1.0158e-01)\tLoss GEN 3.6111e-01 (3.4521e-01)\tLoss DIS 2.1367e-01 (2.1070e-01)\n",
            "Validation: [ 0/64]\tTime  0.360 ( 0.360)\tLoss 7.4521e-02 (7.4521e-02)\n",
            "Epoch: [4][  0/575]\tTime  0.482 ( 0.482)\tLoss GEN MSE 8.8175e-02 (8.8175e-02)\tLoss GEN 4.7149e-01 (4.7149e-01)\tLoss DIS 1.5385e-01 (1.5385e-01)\n",
            "Epoch: [4][400/575]\tTime  0.058 ( 0.060)\tLoss GEN MSE 1.6877e-01 (1.1947e-01)\tLoss GEN 5.3076e-01 (5.2288e-01)\tLoss DIS 1.0516e-01 (1.5960e-01)\n",
            "Validation: [ 0/64]\tTime  0.362 ( 0.362)\tLoss 1.0594e-01 (1.0594e-01)\n",
            "Epoch: [5][  0/575]\tTime  0.502 ( 0.502)\tLoss GEN MSE 1.3173e-01 (1.3173e-01)\tLoss GEN 5.7341e-01 (5.7341e-01)\tLoss DIS 1.0170e-01 (1.0170e-01)\n",
            "Epoch: [5][400/575]\tTime  0.056 ( 0.060)\tLoss GEN MSE 1.7311e-01 (1.5041e-01)\tLoss GEN 5.6881e-01 (6.6069e-01)\tLoss DIS 1.3654e-01 (1.0637e-01)\n",
            "Validation: [ 0/64]\tTime  0.351 ( 0.351)\tLoss 1.5823e-01 (1.5823e-01)\n",
            "Epoch: [6][  0/575]\tTime  0.477 ( 0.477)\tLoss GEN MSE 1.4769e-01 (1.4769e-01)\tLoss GEN 4.3941e-01 (4.3941e-01)\tLoss DIS 1.3992e-01 (1.3992e-01)\n",
            "Epoch: [6][400/575]\tTime  0.065 ( 0.060)\tLoss GEN MSE 1.0767e-01 (1.2240e-01)\tLoss GEN 3.7999e-01 (4.5512e-01)\tLoss DIS 1.8687e-01 (1.8452e-01)\n",
            "Validation: [ 0/64]\tTime  0.363 ( 0.363)\tLoss 1.1223e-01 (1.1223e-01)\n",
            "Epoch: [7][  0/575]\tTime  0.502 ( 0.502)\tLoss GEN MSE 1.0220e-01 (1.0220e-01)\tLoss GEN 3.3502e-01 (3.3502e-01)\tLoss DIS 1.9814e-01 (1.9814e-01)\n",
            "Epoch: [7][400/575]\tTime  0.058 ( 0.059)\tLoss GEN MSE 1.3527e-01 (1.1456e-01)\tLoss GEN 2.6808e-01 (2.9995e-01)\tLoss DIS 2.4341e-01 (2.2601e-01)\n",
            "Validation: [ 0/64]\tTime  0.366 ( 0.366)\tLoss 1.3350e-01 (1.3350e-01)\n",
            "Epoch: [8][  0/575]\tTime  0.486 ( 0.486)\tLoss GEN MSE 1.1513e-01 (1.1513e-01)\tLoss GEN 4.9301e-01 (4.9301e-01)\tLoss DIS 1.3545e-01 (1.3545e-01)\n",
            "Epoch: [8][400/575]\tTime  0.057 ( 0.059)\tLoss GEN MSE 1.2233e-01 (1.2418e-01)\tLoss GEN 5.4355e-01 (4.4973e-01)\tLoss DIS 1.0683e-01 (1.7381e-01)\n",
            "Validation: [ 0/64]\tTime  0.357 ( 0.357)\tLoss 1.5273e-01 (1.5273e-01)\n",
            "Epoch: [9][  0/575]\tTime  0.480 ( 0.480)\tLoss GEN MSE 1.5674e-01 (1.5674e-01)\tLoss GEN 4.2110e-01 (4.2110e-01)\tLoss DIS 2.3771e-01 (2.3771e-01)\n",
            "Epoch: [9][400/575]\tTime  0.058 ( 0.060)\tLoss GEN MSE 1.0794e-01 (1.4341e-01)\tLoss GEN 4.9712e-01 (4.7250e-01)\tLoss DIS 1.1693e-01 (1.5220e-01)\n",
            "Validation: [ 0/64]\tTime  0.365 ( 0.365)\tLoss 1.1063e-01 (1.1063e-01)\n",
            "Epoch: [10][  0/575]\tTime  0.508 ( 0.508)\tLoss GEN MSE 1.2718e-01 (1.2718e-01)\tLoss GEN 5.1046e-01 (5.1046e-01)\tLoss DIS 8.6097e-02 (8.6097e-02)\n",
            "Epoch: [10][400/575]\tTime  0.057 ( 0.060)\tLoss GEN MSE 1.7094e-01 (1.3289e-01)\tLoss GEN 3.4639e-01 (4.3549e-01)\tLoss DIS 2.2770e-01 (1.8239e-01)\n",
            "Validation: [ 0/64]\tTime  0.378 ( 0.378)\tLoss 1.0419e-01 (1.0419e-01)\n",
            "Epoch: [11][  0/575]\tTime  0.501 ( 0.501)\tLoss GEN MSE 1.0518e-01 (1.0518e-01)\tLoss GEN 6.3143e-01 (6.3143e-01)\tLoss DIS 1.2671e-01 (1.2671e-01)\n",
            "Epoch: [11][400/575]\tTime  0.058 ( 0.060)\tLoss GEN MSE 1.1952e-01 (1.2297e-01)\tLoss GEN 4.2772e-01 (3.8922e-01)\tLoss DIS 1.8769e-01 (1.9415e-01)\n",
            "Validation: [ 0/64]\tTime  0.356 ( 0.356)\tLoss 1.0841e-01 (1.0841e-01)\n",
            "Epoch: [12][  0/575]\tTime  0.492 ( 0.492)\tLoss GEN MSE 1.2457e-01 (1.2457e-01)\tLoss GEN 2.5133e-01 (2.5133e-01)\tLoss DIS 2.5297e-01 (2.5297e-01)\n",
            "Epoch: [12][400/575]\tTime  0.065 ( 0.060)\tLoss GEN MSE 1.3455e-01 (1.2225e-01)\tLoss GEN 4.5591e-01 (4.0744e-01)\tLoss DIS 1.4272e-01 (1.8086e-01)\n",
            "Validation: [ 0/64]\tTime  0.370 ( 0.370)\tLoss 1.2418e-01 (1.2418e-01)\n",
            "Epoch: [13][  0/575]\tTime  0.497 ( 0.497)\tLoss GEN MSE 1.3348e-01 (1.3348e-01)\tLoss GEN 4.5164e-01 (4.5164e-01)\tLoss DIS 1.7138e-01 (1.7138e-01)\n",
            "Epoch: [13][400/575]\tTime  0.057 ( 0.060)\tLoss GEN MSE 1.0807e-01 (1.1638e-01)\tLoss GEN 2.2801e-01 (3.6516e-01)\tLoss DIS 2.8653e-01 (2.0931e-01)\n",
            "Validation: [ 0/64]\tTime  0.355 ( 0.355)\tLoss 1.2107e-01 (1.2107e-01)\n",
            "Epoch: [14][  0/575]\tTime  0.478 ( 0.478)\tLoss GEN MSE 1.1044e-01 (1.1044e-01)\tLoss GEN 2.5027e-01 (2.5027e-01)\tLoss DIS 2.5324e-01 (2.5324e-01)\n",
            "Epoch: [14][400/575]\tTime  0.058 ( 0.060)\tLoss GEN MSE 8.0138e-02 (1.0377e-01)\tLoss GEN 6.5552e-01 (4.6364e-01)\tLoss DIS 1.5389e-01 (1.5291e-01)\n",
            "Validation: [ 0/64]\tTime  0.357 ( 0.357)\tLoss 9.2790e-02 (9.2790e-02)\n",
            "Epoch: [15][  0/575]\tTime  0.497 ( 0.497)\tLoss GEN MSE 9.2930e-02 (9.2930e-02)\tLoss GEN 4.4182e-01 (4.4182e-01)\tLoss DIS 1.5125e-01 (1.5125e-01)\n",
            "Epoch: [15][400/575]\tTime  0.053 ( 0.059)\tLoss GEN MSE 1.9565e-01 (1.1207e-01)\tLoss GEN 1.3019e+00 (5.4447e-01)\tLoss DIS 1.7849e-02 (1.3704e-01)\n",
            "Validation: [ 0/64]\tTime  0.366 ( 0.366)\tLoss 1.6792e-01 (1.6792e-01)\n",
            "Epoch: [16][  0/575]\tTime  0.498 ( 0.498)\tLoss GEN MSE 1.6702e-01 (1.6702e-01)\tLoss GEN 4.0346e-01 (4.0346e-01)\tLoss DIS 1.4220e-01 (1.4220e-01)\n",
            "Epoch: [16][400/575]\tTime  0.057 ( 0.060)\tLoss GEN MSE 1.7116e-01 (1.6775e-01)\tLoss GEN 3.6843e-01 (4.3703e-01)\tLoss DIS 2.2551e-01 (1.6803e-01)\n",
            "Validation: [ 0/64]\tTime  0.363 ( 0.363)\tLoss 1.8715e-01 (1.8715e-01)\n",
            "Epoch: [17][  0/575]\tTime  0.481 ( 0.481)\tLoss GEN MSE 1.7548e-01 (1.7548e-01)\tLoss GEN 2.3059e-01 (2.3059e-01)\tLoss DIS 2.8038e-01 (2.8038e-01)\n",
            "Epoch: [17][400/575]\tTime  0.057 ( 0.060)\tLoss GEN MSE 1.6978e-01 (1.6886e-01)\tLoss GEN 5.7258e-01 (4.8638e-01)\tLoss DIS 8.0127e-02 (1.3164e-01)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_psnr(output, target):\n",
        "    psnr = 0.\n",
        "    mse = nn.MSELoss()(output, target)\n",
        "    psnr = 10 * math.log10(torch.max(output)/mse)\n",
        "    return psnr\n",
        "\n",
        "def calc_ssim(output, target):\n",
        "    ssim = 0.\n",
        "    output = output.cpu().detach().numpy()\n",
        "    target = target.cpu().detach().numpy()\n",
        "\n",
        "    if output.ndim == 4:\n",
        "        for i in range(output.shape[0]):\n",
        "            output_i = np.squeeze(output[i,:,:,:])\n",
        "            output_i = np.moveaxis(output_i, 0, -1)\n",
        "            target_i = np.squeeze(target[i,:,:,:])\n",
        "            target_i = np.moveaxis(target_i, 0, -1)\n",
        "            batch_size = output.shape[0]\n",
        "            ssim += sk_ssim(output_i, target_i, data_range = output_i.max() - target_i.max(), multichannel=True)\n",
        "    else:\n",
        "        output_i = np.squeeze(output)\n",
        "        output_i = np.moveaxis(output_i, 0, -1)\n",
        "        target_i = np.squeeze(target)\n",
        "        target_i = np.moveaxis(target_i, 0, -1)\n",
        "        batch_size = 1\n",
        "        ssim += sk_ssim(output_i, target_i, data_range = output_i.max() - target_i.max(), multichannel=True)\n",
        "\n",
        "    ssim = ssim / batch_size\n",
        "    return ssim"
      ],
      "metadata": {
        "id": "6sQ6cib9zsW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "def evaluate(dataloader, net, args):\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    psnr = AverageMeter('PSNR', ':.4f')\n",
        "    ssim = AverageMeter('SSIM', ':.4f')\n",
        "    SG_loss = AverageMeter('Savitzky-Golay Loss', ':.4e')\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    MSE_SG = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(dataloader):\n",
        "            x = data['input_spectrum']\n",
        "            inputs = x.float()\n",
        "            inputs = inputs.cuda(args.gpu)\n",
        "            y = data['output_spectrum']\n",
        "            target = y.float()\n",
        "            target = target.cuda(args.gpu)\n",
        "\n",
        "            x = np.squeeze(x.numpy())\n",
        "            y = np.squeeze(y.numpy())\n",
        "\n",
        "            output = net(inputs)\n",
        "            loss = nn.MSELoss()(output, target)\n",
        "\n",
        "            x_out = output.cpu().detach().numpy()\n",
        "            x_out = np.squeeze(x_out)\n",
        "\n",
        "            SGF_1_9 = scipy.signal.savgol_filter(x,9,1)\n",
        "            MSE_SGF_1_9 = np.mean(np.mean(np.square(np.absolute(y - (SGF_1_9 - np.reshape(np.amin(SGF_1_9, axis = 1), (len(SGF_1_9),1)))))))\n",
        "            MSE_SG.append(MSE_SGF_1_9)\n",
        "\n",
        "            psnr_batch = calc_psnr(output, target)\n",
        "            psnr.update(psnr_batch, inputs.size(0))\n",
        "            ssim_batch = calc_ssim(output, target)\n",
        "            ssim.update(ssim_batch, inputs.size(0))\n",
        "\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "\n",
        "        print(\"Neural Network MSE: {}\".format(losses.avg))\n",
        "        print(\"Neural Network PSNR: {}\".format(psnr.avg))\n",
        "        print(\"Neural Network SSIM: {}\".format(ssim.avg))\n",
        "        print(\"Savitzky-Golay MSE: {}\".format(np.mean(np.asarray(MSE_SG))))\n",
        "        print(\"Neural Network performed {0:.2f}x better than Savitzky-Golay\".format(np.mean(np.asarray(MSE_SG))/losses.avg))\n",
        "\n",
        "    return losses.avg, psnr.avg, ssim.avg, MSE_SG"
      ],
      "metadata": {
        "id": "8ujeP58VmTxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_test(args):\n",
        "    gpu = args.gpu\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "\n",
        "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "    ngpus_per_node = torch.cuda.device_count()\n",
        "\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for testing\".format(args.gpu))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create model(s) and send to device(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    net = ResUNet(3, args.batch_norm).float()\n",
        "    net.load_state_dict(torch.load(args.model))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.gpu is not None:\n",
        "            torch.cuda.set_device(args.gpu)\n",
        "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.gpu])\n",
        "        else:\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "    elif args.gpu is not None:\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "        net.cuda(args.gpu)\n",
        "    else:\n",
        "        net.cuda(args.gpu)\n",
        "        net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define dataset path and data splits\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Input_Data = scipy.io.loadmat(\"Dataset/Test_Inputs.mat\")\n",
        "    Output_Data = scipy.io.loadmat(\"Dataset/Test_Outputs.mat\")\n",
        "\n",
        "    Input = Input_Data['Test_Inputs']\n",
        "    Output = Output_Data['Test_Outputs']\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create datasets (with augmentation) and dataloaders\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Raman_Dataset_Test = RamanDataset(Input, Output, batch_size = args.batch_size, spectrum_len = args.spectrum_len)\n",
        "\n",
        "    test_loader = DataLoader(Raman_Dataset_Test, batch_size = args.batch_size, shuffle = False, num_workers = 0, pin_memory = True)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Evaluate\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    MSE_NN, PSNR_NN, SSIM_NN, MSE_SG = evaluate(test_loader, net, args)"
      ],
      "metadata": {
        "id": "pGV-EuKysDBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Arguments:\n",
        "    pass\n",
        "\n",
        "args = Arguments()\n",
        "args.workers = 0\n",
        "args.batch_size = 256\n",
        "args.spectrum_len = 500\n",
        "args.seed = None\n",
        "args.gpu = 0\n",
        "args.world_size = -1\n",
        "args.rank = -1\n",
        "args.dist_url = \"tcp://224.66.41.62:23456\"\n",
        "args.dist_backend = \"nccl\"\n",
        "args.multiprocessing_distributed = False\n",
        "# args.batch_norm = True\n",
        "args.model = \"ResUNet.pt\"\n",
        "\n",
        "args.batch_norm = False\n",
        "main_test(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I97mxHSasvCS",
        "outputId": "2ce02201-84d1-4296-cb0b-8bd83cd8b625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: 0 for testing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-e742af0f8998>:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(torch.load(args.model))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network MSE: 0.001954648160132516\n",
            "Neural Network PSNR: 27.27959634164852\n",
            "Neural Network SSIM: 0.3171306463866458\n",
            "Savitzky-Golay MSE: 0.027660622850368643\n",
            "Neural Network performed 14.15x better than Savitzky-Golay\n"
          ]
        }
      ]
    }
  ]
}
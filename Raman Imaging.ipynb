{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1GHdCjqh84Xexa4GTzb8qntXQ08wgcIgd",
      "authorship_tag": "ABX9TyPkuPxywkXZ/Onwf39gg5Wu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TODO:\n",
        "\n",
        "\n",
        "*   Add utilities to github\n",
        "*   Update this notebook to clone repo\n",
        "*   Add updates to this notbook to run a train & test for de-noising using images from google drive but utilities from github\n",
        "*   Add in k-means & testing framework\n",
        "*   Repeat with second notebook for hyper-spectral super sesolution\n",
        "\n"
      ],
      "metadata": {
        "id": "D077tOtY-b6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"RiverBotham\"\n",
        "!git config --global user.email \"river.botham@gmail.com\"\n",
        "!git config --global user.password \"MY_PASSWORD\"\n",
        "\n",
        "token = 'GIT_TOKEN'\n",
        "username = 'RiverBotham'\n",
        "repo = 'Raman'\n",
        "\n",
        "!git clone https://{token}@github.com/{username}/{repo}"
      ],
      "metadata": {
        "id": "q-lUBSc7BMNb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {repo}/Denoising"
      ],
      "metadata": {
        "id": "zzLakAJlDBnY",
        "outputId": "bfc6f8f9-930c-4081-885f-86f5ed0051d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Raman/Denoising\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import datetime\n",
        "import time\n",
        "import shutil\n",
        "import argparse\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import scipy.signal\n",
        "import math\n",
        "from skimage.metrics import structural_similarity as sk_ssim\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.distributed as dist\n",
        "import torch.utils.data.distributed\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "import model, dataset, utilities"
      ],
      "metadata": {
        "id": "KWeXO7nYDF8j"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, net, optimizer, scheduler, criterion, criterion_MSE, epoch, gpu, scheduler):\n",
        "\n",
        "    batch_time = utilities.AverageMeter('Time', ':6.3f')\n",
        "    losses = utilities.AverageMeter('Loss', ':.4e')\n",
        "    psnr = utilities.AverageMeter('PSNR', ':.4f')\n",
        "    ssim = utilities.AverageMeter('SSIM', ':.4f')\n",
        "    progress = utilities.ProgressMeter(len(dataloader), [batch_time, psnr, ssim], prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    end = time.time()\n",
        "    for i, data in enumerate(dataloader):\n",
        "        inputs = data['input_image']\n",
        "        inputs = inputs.float()\n",
        "        inputs = inputs.cuda(gpu)\n",
        "        target = data['output_image']\n",
        "        target = target.float()\n",
        "        target = target.cuda(gpu)\n",
        "\n",
        "        output = net(inputs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if scheduler == \"cyclic-lr\" or scheduler == \"one-cycle-lr\":\n",
        "            scheduler.step()\n",
        "\n",
        "        loss_MSE = criterion_MSE(output, target)\n",
        "        losses.update(loss_MSE.item(), inputs.size(0))\n",
        "\n",
        "        psnr_batch = utilities.calc_psnr(output, target)\n",
        "        psnr.update(psnr_batch, inputs.size(0))\n",
        "\n",
        "        ssim_batch = utilities.calc_ssim(output, target)\n",
        "        ssim.update(ssim_batch, inputs.size(0))\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 20 == 0:\n",
        "            progress.display(i)\n",
        "    return losses.avg, psnr.avg, ssim.avg\n"
      ],
      "metadata": {
        "id": "smrIJIsQDp3c"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(dataloader, net, criterion_MSE, gpu):\n",
        "\n",
        "    batch_time = utilities.AverageMeter('Time', ':6.3f')\n",
        "    losses = utilities.AverageMeter('Loss', ':.4e')\n",
        "    psnr = utilities.AverageMeter('PSNR', ':.4f')\n",
        "    ssim = utilities.AverageMeter('SSIM', ':.4f')\n",
        "    progress = utilities.ProgressMeter(len(dataloader), [batch_time, psnr, ssim], prefix='Validation: ')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, data in enumerate(dataloader):\n",
        "            inputs = data['input_image']\n",
        "            inputs = inputs.float()\n",
        "            inputs = inputs.cuda(gpu)\n",
        "            target = data['output_image']\n",
        "            target = target.float()\n",
        "            target = target.cuda(gpu)\n",
        "\n",
        "            output = net(inputs)\n",
        "\n",
        "            loss_MSE = criterion_MSE(output, target)\n",
        "            losses.update(loss_MSE.item(), inputs.size(0))\n",
        "\n",
        "            psnr_batch = utilities.calc_psnr(output, target)\n",
        "            psnr.update(psnr_batch, inputs.size(0))\n",
        "\n",
        "            ssim_batch = utilities.calc_ssim(output, target)\n",
        "            ssim.update(ssim_batch, inputs.size(0))\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % 20 == 0:\n",
        "                progress.display(i)\n",
        "\n",
        "    return losses.avg, psnr.avg, ssim.avg"
      ],
      "metadata": {
        "id": "ZvHvb_vkD2Wm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_worker(gpu, ngpus_per_node, args, distributed, dist_url):\n",
        "\n",
        "\n",
        "    if gpu is not None:\n",
        "        print(\"Use GPU: {} for training\".format(gpu))\n",
        "\n",
        "    if distributed:\n",
        "        if dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create model(s) and send to device(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    scale = args.hr_image_size // args.lr_image_size\n",
        "    net = model.Hyperspectral_RCAN(args.spectrum_len, scale).float()\n",
        "\n",
        "    if distributed:\n",
        "        if gpu is not None:\n",
        "            torch.cuda.set_device(gpu)\n",
        "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "\n",
        "            net.cuda(gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[gpu])\n",
        "        else:\n",
        "            net.cuda(gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "    elif gpu is not None:\n",
        "        torch.cuda.set_device(gpu)\n",
        "        net.cuda(gpu)\n",
        "    else:\n",
        "        net = nn.DataParallel(net).cuda()\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define dataset path and data splits\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    dataset_path = \"Dataset/\"\n",
        "    image_ids_csv = pd.read_csv(dataset_path + \"Image_IDs.csv\")\n",
        "\n",
        "    image_ids = image_ids_csv[\"id\"].values\n",
        "\n",
        "    train_split = round(0.85 * len(image_ids))\n",
        "    val_split = round(0.10 * len(image_ids))\n",
        "    test_split = round(0.05 * len(image_ids))\n",
        "    train_ids = image_ids[:train_split]\n",
        "    val_ids = image_ids[train_split:train_split+val_split]\n",
        "    test_ids = image_ids[train_split+val_split:]\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create datasets and dataloaders\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Raman_Dataset_Train = torch.utils.data.ConcatDataset([dataset.RamanImageDataset(train_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len, spectrum_shift = 0.1, spectrum_flip = True,\n",
        "                                                    horizontal_flip = True, vertical_flip = True, rotate = True, patch = True, mixup = True),\n",
        "                                                    dataset.RamanImageDataset(train_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len, spectrum_shift = -0.2, spectrum_flip = True,\n",
        "                                                    horizontal_flip = True, vertical_flip = True, rotate = True, patch = True, mixup = True)])\n",
        "\n",
        "    Raman_Dataset_Val = dataset.RamanImageDataset(val_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len)\n",
        "\n",
        "    train_loader = DataLoader(Raman_Dataset_Train, batch_size = args.batch_size, shuffle = False, num_workers = args.workers)\n",
        "    val_loader = DataLoader(Raman_Dataset_Val, batch_size = args.batch_size, shuffle = False, num_workers = args.workers)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define criterion(s), optimizer(s), and scheduler(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "\n",
        "    # ------------Criterion------------\n",
        "    criterion = nn.L1Loss().cuda(gpu)\n",
        "    criterion_MSE = nn.MSELoss().cuda(gpu)\n",
        "\n",
        "    # ------------Optimizer------------\n",
        "    if args.optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(net.parameters(), lr = args.lr)\n",
        "    elif args.optimizer == \"adamW\":\n",
        "        optimizer = optim.AdamW(net.parameters(), lr = args.lr)\n",
        "    else: # Adam\n",
        "        optimizer = optim.Adam(net.parameters(), lr = args.lr)\n",
        "\n",
        "    # ------------Scheduler------------\n",
        "    if args.scheduler == \"decay-lr\":\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.2)\n",
        "    elif args.scheduler == \"multiplicative-lr\":\n",
        "        lmbda = lambda epoch: 0.985\n",
        "        scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
        "    elif args.scheduler == \"cyclic-lr\":\n",
        "        scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr = args.base_lr, max_lr = args.lr, mode = 'triangular2', cycle_momentum = False)\n",
        "    elif args.scheduler == \"one-cycle-lr\":\n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr = args.lr, steps_per_epoch=len(train_loader), epochs=args.epochs, cycle_momentum = False)\n",
        "    else: # constant-lr\n",
        "        scheduler = None\n",
        "\n",
        "    print('Started Training')\n",
        "    print('Training Details:')\n",
        "    print('Network:         {}'.format(args.network))\n",
        "    print('Epochs:          {}'.format(args.epochs))\n",
        "    print('Batch Size:      {}'.format(args.batch_size))\n",
        "    print('Optimizer:       {}'.format(args.optimizer))\n",
        "    print('Scheduler:       {}'.format(args.scheduler))\n",
        "    print('Learning Rate:   {}'.format(args.lr))\n",
        "    print('Spectrum Length: {}'.format(args.spectrum_len))\n",
        "\n",
        "    date = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
        "\n",
        "    log_dir = \"runs/{}_{}_{}_{}_{}x\".format(date, args.optimizer, args.scheduler, args.network, scale)\n",
        "    models_dir = \"{}_{}_{}_{}_{}x.pt\".format(date, args.optimizer, args.scheduler, args.network, scale)\n",
        "\n",
        "    writer = SummaryWriter(log_dir = log_dir)\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        train_loss, train_psnr, train_ssim = train(train_loader, net, optimizer, scheduler, criterion, criterion_MSE, epoch, args)\n",
        "        valid_loss, valid_psnr, valid_ssim = validate(val_loader, net, criterion_MSE, args)\n",
        "        if args.scheduler != \"cyclic-lr\" and args.scheduler != \"one-cycle-lr\" and args.scheduler != \"constant-lr\":\n",
        "            scheduler.step()\n",
        "\n",
        "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "        writer.add_scalar('Loss/val', valid_loss, epoch)\n",
        "        writer.add_scalar('PSNR/train', train_psnr, epoch)\n",
        "        writer.add_scalar('PSNR/val', valid_psnr, epoch)\n",
        "        writer.add_scalar('SSIM/train', train_ssim, epoch)\n",
        "        writer.add_scalar('SSIM/val', valid_ssim, epoch)\n",
        "\n",
        "    torch.save(net.state_dict(), models_dir)\n",
        "    print('Finished Training')"
      ],
      "metadata": {
        "id": "2KgCQop-D7Gf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "mount_file_id": "1GHdCjqh84Xexa4GTzb8qntXQ08wgcIgd",
      "authorship_tag": "ABX9TyO8dEprH5KeCrQBt0dvoAMp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiverBotham/Raman/blob/main/Raman%20Imaging%20Super%20Res.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO:\n",
        "\n",
        "\n",
        "*   Add utilities to github\n",
        "*   Update this notebook to clone repo\n",
        "*   Add updates to this notbook to run a train & test for de-noising using images from google drive but utilities from github\n",
        "*   Add in k-means & testing framework\n",
        "*   Repeat with second notebook for hyper-spectral super sesolution\n",
        "\n"
      ],
      "metadata": {
        "id": "D077tOtY-b6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To save forst clone the repo\n",
        "!git config --global user.name \"RiverBotham\"\n",
        "!git config --global user.email \"river.botham@gmail.com\"\n",
        "!git config --global user.password \"MY_PASSWORD\"\n",
        "\n",
        "token = 'MY_TOKEN'\n",
        "username = 'RiverBotham'\n",
        "repo = 'Raman'\n",
        "\n",
        "!git clone https://{token}@github.com/{username}/{repo}"
      ],
      "metadata": {
        "id": "q-lUBSc7BMNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f60933b2-6bbd-4bd7-d704-663f64d883c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Raman'...\n",
            "remote: Enumerating objects: 160, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 160 (delta 53), reused 35 (delta 35), pack-reused 96 (from 1)\u001b[K\n",
            "Receiving objects: 100% (160/160), 7.55 MiB | 27.11 MiB/s, done.\n",
            "Resolving deltas: 100% (96/96), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move into the cloned repo, then File -> Save copy in GitHub\n",
        "%cd {repo}/Denoising"
      ],
      "metadata": {
        "id": "zzLakAJlDBnY",
        "outputId": "016cb4b9-7a98-40c6-a8e8-07abacd33561",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Raman/Denoising\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import datetime\n",
        "import time\n",
        "import shutil\n",
        "import argparse\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import scipy.signal\n",
        "import math\n",
        "from skimage.metrics import structural_similarity as sk_ssim\n",
        "from sklearn.model_selection import KFold\n",
        "from skimage.transform import resize\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.distributed as dist\n",
        "import torch.utils.data.distributed\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# import model, dataset, utilities"
      ],
      "metadata": {
        "id": "KWeXO7nYDF8j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "\n",
        "\n",
        "class ChannelAttentionBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttentionBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.chan_attn = nn.Sequential(\n",
        "                nn.Conv2d(channels, channels // reduction, 1, padding=0, bias=True),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channels // reduction, channels, 1, padding=0, bias=True),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.chan_attn(y)\n",
        "        return x * y\n",
        "\n",
        "class ResidualChannelAttentionBlock(nn.Module):\n",
        "    def __init__(self, channels=500, kernel_size=3, reduction=16, bias=True, act=nn.ReLU(True)):\n",
        "        super(ResidualChannelAttentionBlock, self).__init__()\n",
        "        modules_body = []\n",
        "        for i in range(2):\n",
        "            modules_body.append(nn.Conv2d(channels, channels, kernel_size, padding=(kernel_size//2), bias=bias))\n",
        "            if i == 0: modules_body.append(act)\n",
        "        modules_body.append(ChannelAttentionBlock(channels, reduction))\n",
        "\n",
        "        self.body = nn.Sequential(*modules_body)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.body(x)\n",
        "        res += x\n",
        "        return res\n",
        "\n",
        "class ResidualGroup(nn.Module):\n",
        "    def __init__(self, channels=500, kernel_size=3, reduction=16, bias=True, act=nn.ReLU(True), n_resblocks=6):\n",
        "        super(ResidualGroup, self).__init__()\n",
        "        modules_body = []\n",
        "        modules_body = [ResidualChannelAttentionBlock(channels, kernel_size, reduction, bias=bias, act=nn.ReLU(True)) for _ in range(n_resblocks)]\n",
        "        modules_body.append(nn.Conv2d(channels, channels, kernel_size, padding=(kernel_size//2), bias=bias))\n",
        "\n",
        "        self.body = nn.Sequential(*modules_body)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.body(x)\n",
        "        res += x\n",
        "        return res\n",
        "\n",
        "class Upsampler(nn.Sequential):\n",
        "    def __init__(self, scale, channels, kernel_size, bn=False, act=False, bias=True):\n",
        "\n",
        "        m = []\n",
        "        if (scale & (scale - 1)) == 0:\n",
        "            for _ in range(int(math.log(scale, 2))):\n",
        "                conv = nn.Conv2d(channels, 4*channels, kernel_size, padding=(kernel_size//2), bias=bias)\n",
        "                m.append(conv)\n",
        "                m.append(nn.PixelShuffle(2))\n",
        "                if bn: m.append(nn.BatchNorm2d(channels))\n",
        "                if act: m.append(nn.ReLU(True))\n",
        "        elif scale == 3:\n",
        "            m.append(nn.Conv2d(channels, 9*channels, kernel_size, padding=(kernel_size//2), bias=bias))\n",
        "            m.append(nn.PixelShuffle(3))\n",
        "            if bn: m.append(nn.BatchNorm2d(channels))\n",
        "            if act: m.append(nn.ReLU(True))\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        super(Upsampler, self).__init__(*m)\n",
        "\n",
        "class Hyperspectral_RCAN(nn.Module):\n",
        "    def __init__(self, spectrum_len, scale=4, kernel_size=3, reduction=16, bias=True, act=nn.ReLU(True), n_resblocks=16, n_resgroups=18):\n",
        "        super(Hyperspectral_RCAN, self).__init__()\n",
        "        modules_head1 = [Upsampler(scale, spectrum_len, kernel_size, act=False), nn.Conv2d(spectrum_len, spectrum_len, kernel_size, padding=(kernel_size//2), bias=bias)]\n",
        "        modules_head2 = [nn.Conv2d(spectrum_len, int(spectrum_len/2), kernel_size, padding=(kernel_size//2), bias=bias)]\n",
        "\n",
        "        modules_body = [ResidualGroup(int(spectrum_len/2), kernel_size, reduction, act, n_resblocks) for _ in range(n_resgroups)]\n",
        "        modules_body.append(nn.Conv2d(int(spectrum_len/2), int(spectrum_len/2), kernel_size, padding=(kernel_size//2), bias=bias))\n",
        "\n",
        "        modules_tail = [nn.Conv2d(int(spectrum_len/2), int(spectrum_len/2), kernel_size, padding=(kernel_size//2), bias=bias)]\n",
        "        modules_tail.append(nn.Conv2d(int(spectrum_len/2), spectrum_len, kernel_size, padding=(kernel_size//2), bias=bias))\n",
        "\n",
        "        self.head1 = nn.Sequential(*modules_head1)\n",
        "        self.head2 = nn.Sequential(*modules_head2)\n",
        "        self.body = nn.Sequential(*modules_body)\n",
        "        self.tail = nn.Sequential(*modules_tail)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.head1(x)\n",
        "        x1 = self.head2(x)\n",
        "\n",
        "        res1 = self.body(x1)\n",
        "        res1 += x1\n",
        "\n",
        "        res2 = self.tail(res1)\n",
        "        res2 += x\n",
        "\n",
        "        return res2"
      ],
      "metadata": {
        "id": "Mei5k_ff0fci"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data set\n",
        "\n",
        "\n",
        "class RamanImageDataset(Dataset):\n",
        "    def __init__(self, image_ids, path, batch_size=2, hr_image_size=64, lr_image_size=16, spectrum_len=500,\n",
        "                spectrum_shift = 0., spectrum_flip = False, horizontal_flip = False, vertical_flip = False,\n",
        "                 rotate = False, patch = False, mixup = False):\n",
        "        self.image_ids = image_ids\n",
        "        self.path = path\n",
        "        self.batch_size = batch_size\n",
        "        self.hr_image_size = hr_image_size\n",
        "        self.lr_image_size = lr_image_size\n",
        "        self.spectrum_len = spectrum_len\n",
        "        self.spectrum_shift = spectrum_shift\n",
        "        self.spectrum_flip = spectrum_flip\n",
        "        self.horizontal_flip = horizontal_flip\n",
        "        self.vertical_flip = vertical_flip\n",
        "        self.rotate = rotate\n",
        "        self.patch = patch\n",
        "        self.mixup = mixup\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def load_image(self, id_name):\n",
        "        input_path =self.path + id_name + \".mat\"\n",
        "\n",
        "        output_data = scipy.io.loadmat(input_path)\n",
        "        output_values = list(output_data.values())\n",
        "        output_image = output_values[3]\n",
        "        return output_image\n",
        "\n",
        "    def pad_image(self, image, size, patch):\n",
        "        if image.shape[0] == size and image.shape[1] == size:\n",
        "            padded_image = image\n",
        "        elif image.shape[0] > size and image.shape[1] > size:\n",
        "            if patch:\n",
        "                padded_image = self.get_image_patch(image, size)\n",
        "            else:\n",
        "                padded_image = self.center_crop_image(image, size)\n",
        "        else:\n",
        "            padded_image = image\n",
        "            if padded_image.shape[0] > size:\n",
        "                if patch:\n",
        "                    padded_image = self.get_image_patch(padded_image, size)\n",
        "                else:\n",
        "                    padded_image = self.center_crop_image(padded_image, size)\n",
        "            else:\n",
        "                pad_before = int(np.floor((size - padded_image.shape[0])/2))\n",
        "                pad_after = int(np.ceil((size - padded_image.shape[0])/2))\n",
        "                padded_image = np.pad(padded_image, ((pad_before, pad_after), (0,0), (0, 0)), 'reflect')\n",
        "\n",
        "            if padded_image.shape[1] > size:\n",
        "                if patch:\n",
        "                    padded_image = self.get_image_patch(padded_image, size)\n",
        "                else:\n",
        "                    padded_image = self.center_crop_image(padded_image, size)\n",
        "            else:\n",
        "                pad_before = int(np.floor((size - padded_image.shape[1])/2))\n",
        "                pad_after = int(np.ceil((size - padded_image.shape[1])/2))\n",
        "                padded_image = np.pad(padded_image, ((0,0), (pad_before, pad_after), (0, 0)), 'reflect')\n",
        "\n",
        "        return padded_image\n",
        "\n",
        "    def get_image_patch(self, image, patch_size):\n",
        "        if image.shape[0] > patch_size:\n",
        "            start_idx_x = int(np.round(np.random.random() * (image.shape[0]-patch_size)))\n",
        "            end_idx_x = start_idx_x + patch_size\n",
        "        else:\n",
        "            start_idx_x = 0\n",
        "            end_idx_x = image.shape[0]\n",
        "\n",
        "        if image.shape[1] > patch_size:\n",
        "            start_idx_y = int(np.round(np.random.random() * (image.shape[1]-patch_size)))\n",
        "            end_idx_y = start_idx_y + patch_size\n",
        "        else:\n",
        "            start_idx_y = 0\n",
        "            end_idx_y = image.shape[1]\n",
        "\n",
        "        image_patch = image[start_idx_x:end_idx_x,start_idx_y:end_idx_y,:]\n",
        "        return image_patch\n",
        "\n",
        "    def center_crop_image(self, image, image_size):\n",
        "        cropped_image = image\n",
        "        if image.shape[0] > image_size:\n",
        "            dif = int(np.floor((image.shape[0] - image_size)/2))\n",
        "            cropped_image = cropped_image[dif:image_size+dif,:,:]\n",
        "\n",
        "        if image.shape[1] > image_size:\n",
        "            dif = int(np.floor((image.shape[1] - image_size)/2))\n",
        "            cropped_image = cropped_image[:,dif:image_size+dif,:]\n",
        "        return cropped_image\n",
        "\n",
        "    def flip_axis(self, image, axis):\n",
        "        if np.random.random() < 0.5:\n",
        "            image = np.asarray(image).swapaxes(axis, 0)\n",
        "            image = image[::-1, ...]\n",
        "            image = image.swapaxes(0, axis)\n",
        "        return image\n",
        "\n",
        "    def rotate_spectral_image(self, image):\n",
        "        rotation_extent = np.random.random()\n",
        "        if rotation_extent < 0.25:\n",
        "            rotation = 1\n",
        "        elif rotation_extent < 0.5:\n",
        "            rotation = 2\n",
        "        elif rotation_extent < 0.75:\n",
        "            rotation = 3\n",
        "        else:\n",
        "            rotation = 0\n",
        "        image = np.rot90(image, rotation)\n",
        "        return image\n",
        "\n",
        "    def shift_spectrum(self, image, shift_range):\n",
        "        shifted_spectrum_image = image\n",
        "        spectrum_shift_range = int(np.round(shift_range*image.shape[2]))\n",
        "        if spectrum_shift_range > 0:\n",
        "            shifted_spectrum_image = np.pad(image[:,:,spectrum_shift_range:], ((0,0), (0,0), (0,abs(spectrum_shift_range))), 'reflect')\n",
        "        elif spectrum_shift_range < 0:\n",
        "            shifted_spectrum_image = np.pad(image[:,:,:spectrum_shift_range], ((0,0), (0,0), (abs(spectrum_shift_range), 0)), 'reflect')\n",
        "        return shifted_spectrum_image\n",
        "\n",
        "    def spectrum_padding(self, image, spectrum_length):\n",
        "        if image.shape[-1] == spectrum_length:\n",
        "            padded_spectrum_image = image\n",
        "        elif image.shape[-1] > spectrum_length:\n",
        "            padded_spectrum_image = image[:,:,0:spectrum_length]\n",
        "        else:\n",
        "            padded_spectrum_image = np.pad(image, ((0,0), (0,0), (0, spectrum_length - image.shape[-1])), 'reflect')\n",
        "        return padded_spectrum_image\n",
        "\n",
        "    def image_mixup(self, image1, image2, alpha):\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        image = (lam * image1) + ((1 - lam) * image2)\n",
        "        return image\n",
        "\n",
        "    def normalise_image(self, image):\n",
        "        image_max = np.tile(np.amax(image),image.shape)\n",
        "        normalised_image = np.divide(image,image_max)\n",
        "        return normalised_image\n",
        "\n",
        "    def downsample_image(self, image, scale = 4):\n",
        "        if scale >= 4:\n",
        "            start_idx = np.random.randint(1,scale-1)\n",
        "        else:\n",
        "            start_idx = 1\n",
        "        downsampled_image = image[start_idx::scale,start_idx::scale,:]\n",
        "        return downsampled_image\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_size_ratio = self.hr_image_size // self.lr_image_size\n",
        "\n",
        "        outputimg = self.load_image(self.image_ids[idx])\n",
        "\n",
        "        mixup_on = False\n",
        "        if self.mixup:\n",
        "            if np.random.random() < 0.5:\n",
        "                image_idx = int(np.round(np.random.random() * (len(self.image_ids)-1)))\n",
        "                image2 = self.load_image(self.image_ids[image_idx])\n",
        "                mixup_on = True\n",
        "\n",
        "        # --------------- Image Data Augmentations ---------------\n",
        "        outputimg = self.pad_image(outputimg, self.hr_image_size, self.patch)\n",
        "        if mixup_on:\n",
        "            image2 = self.pad_image(image2, self.hr_image_size, self.patch)\n",
        "\n",
        "        if self.horizontal_flip:\n",
        "            outputimg = self.flip_axis(outputimg, 1)\n",
        "            if mixup_on:\n",
        "                image2 = self.flip_axis(image2, 1)\n",
        "\n",
        "        if self.vertical_flip:\n",
        "            outputimg = self.flip_axis(outputimg, 0)\n",
        "            if mixup_on:\n",
        "                image2 = self.flip_axis(image2, 0)\n",
        "\n",
        "        if self.rotate:\n",
        "            outputimg = self.rotate_spectral_image(outputimg)\n",
        "            if mixup_on:\n",
        "                image2 = self.rotate_spectral_image(image2)\n",
        "\n",
        "        # --------------- Spectral Data Augmentations ---------------\n",
        "        if self.spectrum_shift != 0.0:\n",
        "            shift_range = np.random.uniform(-self.spectrum_shift, self.spectrum_shift)\n",
        "            outputimg = self.shift_spectrum(outputimg, shift_range)\n",
        "            if mixup_on:\n",
        "                image2 = self.shift_spectrum(image2, shift_range)\n",
        "\n",
        "        outputimg = self.spectrum_padding(outputimg, self.spectrum_len)\n",
        "        if mixup_on:\n",
        "            image2 = self.spectrum_padding(image2, self.spectrum_len)\n",
        "\n",
        "        if self.spectrum_flip:\n",
        "            if np.random.random() < 0.5:\n",
        "                outputimg = self.flip_axis(outputimg, 2)\n",
        "                if mixup_on:\n",
        "                    image2 = self.flip_axis(image2, 2)\n",
        "\n",
        "        # --------------- Mixup ---------------\n",
        "        if mixup_on:\n",
        "            outputimg = self.image_mixup(outputimg, image2, 0.2)\n",
        "\n",
        "        # --------------- Normalisation and Downsampling ---------------\n",
        "        outputimg = self.normalise_image(outputimg)\n",
        "        inputimg = self.downsample_image(outputimg, image_size_ratio)\n",
        "\n",
        "        outputimg = np.moveaxis(outputimg, -1, 0)\n",
        "        inputimg = np.moveaxis(inputimg, -1, 0)\n",
        "\n",
        "        sample = {'input_image': inputimg, 'output_image': outputimg}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)"
      ],
      "metadata": {
        "id": "w6PAklAu0hNJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utilities\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)"
      ],
      "metadata": {
        "id": "3NAulroG0i29"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, net, optimizer, scheduler, criterion, criterion_MSE, epoch, args):\n",
        "\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    psnr = AverageMeter('PSNR', ':.4f')\n",
        "    ssim = AverageMeter('SSIM', ':.4f')\n",
        "    progress = ProgressMeter(len(dataloader), [batch_time, psnr, ssim], prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    end = time.time()\n",
        "    for i, data in enumerate(dataloader):\n",
        "        inputs = data['input_image']\n",
        "        inputs = inputs.float()\n",
        "        inputs = inputs.cuda(args.gpu)\n",
        "        target = data['output_image']\n",
        "        target = target.float()\n",
        "        target = target.cuda(args.gpu)\n",
        "\n",
        "        output = net(inputs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if args.scheduler == \"cyclic-lr\" or args.scheduler == \"one-cycle-lr\":\n",
        "            scheduler.step()\n",
        "\n",
        "        loss_MSE = criterion_MSE(output, target)\n",
        "        losses.update(loss_MSE.item(), inputs.size(0))\n",
        "\n",
        "        psnr_batch = calc_psnr(output, target)\n",
        "        psnr.update(psnr_batch, inputs.size(0))\n",
        "\n",
        "        ssim_batch = calc_ssim(output, target)\n",
        "        ssim.update(ssim_batch, inputs.size(0))\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 20 == 0:\n",
        "            progress.display(i)\n",
        "    return losses.avg, psnr.avg, ssim.avg\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "smrIJIsQDp3c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(dataloader, net, criterion_MSE, args):\n",
        "\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    psnr = AverageMeter('PSNR', ':.4f')\n",
        "    ssim = AverageMeter('SSIM', ':.4f')\n",
        "    progress = ProgressMeter(len(dataloader), [batch_time, psnr, ssim], prefix='Validation: ')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, data in enumerate(dataloader):\n",
        "            inputs = data['input_image']\n",
        "            inputs = inputs.float()\n",
        "            inputs = inputs.cuda(args.gpu)\n",
        "            target = data['output_image']\n",
        "            target = target.float()\n",
        "            target = target.cuda(args.gpu)\n",
        "\n",
        "            output = net(inputs)\n",
        "\n",
        "            loss_MSE = criterion_MSE(output, target)\n",
        "            losses.update(loss_MSE.item(), inputs.size(0))\n",
        "\n",
        "            psnr_batch = calc_psnr(output, target)\n",
        "            psnr.update(psnr_batch, inputs.size(0))\n",
        "\n",
        "            ssim_batch = calc_ssim(output, target)\n",
        "            ssim.update(ssim_batch, inputs.size(0))\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % 20 == 0:\n",
        "                progress.display(i)\n",
        "\n",
        "    return losses.avg, psnr.avg, ssim.avg"
      ],
      "metadata": {
        "id": "ZvHvb_vkD2Wm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_noKmeans(args):\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "\n",
        "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "    ngpus_per_node = torch.cuda.device_count()\n",
        "\n",
        "    gpu = args.gpu\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create model(s) and send to device(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    scale = args.hr_image_size // args.lr_image_size\n",
        "    net = Hyperspectral_RCAN(args.spectrum_len, scale).float()\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.gpu is not None:\n",
        "            torch.cuda.set_device(args.gpu)\n",
        "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.gpu])\n",
        "        else:\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "    elif args.gpu is not None:\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "        net.cuda(args.gpu)\n",
        "    else:\n",
        "        net = nn.DataParallel(net).cuda()\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define dataset path and data splits\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    dataset_path = \"Dataset/\"\n",
        "    image_ids_csv = pd.read_csv(dataset_path + \"Image_IDs.csv\")\n",
        "\n",
        "    image_ids = image_ids_csv[\"id\"].values\n",
        "\n",
        "    train_split = round(0.85 * len(image_ids))\n",
        "    val_split = round(0.10 * len(image_ids))\n",
        "    test_split = round(0.05 * len(image_ids))\n",
        "    train_ids = image_ids[:train_split]\n",
        "    val_ids = image_ids[train_split:train_split+val_split]\n",
        "    test_ids = image_ids[train_split+val_split:]\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create datasets and dataloaders\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Raman_Dataset_Train = torch.utils.data.ConcatDataset([RamanImageDataset(train_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len, spectrum_shift = 0.1, spectrum_flip = True,\n",
        "                                                    horizontal_flip = True, vertical_flip = True, rotate = True, patch = True, mixup = True),\n",
        "                                                    RamanImageDataset(train_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len, spectrum_shift = -0.2, spectrum_flip = True,\n",
        "                                                    horizontal_flip = True, vertical_flip = True, rotate = True, patch = True, mixup = True)])\n",
        "\n",
        "    Raman_Dataset_Val = RamanImageDataset(val_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len)\n",
        "\n",
        "    train_loader = DataLoader(Raman_Dataset_Train, batch_size = args.batch_size, shuffle = False, num_workers = args.workers)\n",
        "    val_loader = DataLoader(Raman_Dataset_Val, batch_size = args.batch_size, shuffle = False, num_workers = args.workers)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define criterion(s), optimizer(s), and scheduler(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "\n",
        "    # ------------Criterion------------\n",
        "    criterion = nn.L1Loss().cuda(args.gpu)\n",
        "    criterion_MSE = nn.MSELoss().cuda(args.gpu)\n",
        "\n",
        "    # ------------Optimizer------------\n",
        "    if args.optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(net.parameters(), lr = args.lr)\n",
        "    elif args.optimizer == \"adamW\":\n",
        "        optimizer = optim.AdamW(net.parameters(), lr = args.lr)\n",
        "    else: # Adam\n",
        "        optimizer = optim.Adam(net.parameters(), lr = args.lr)\n",
        "\n",
        "    # ------------Scheduler------------\n",
        "    if args.scheduler == \"decay-lr\":\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.2)\n",
        "    elif args.scheduler == \"multiplicative-lr\":\n",
        "        lmbda = lambda epoch: 0.985\n",
        "        scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
        "    elif args.scheduler == \"cyclic-lr\":\n",
        "        scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr = args.base_lr, max_lr = args.lr, mode = 'triangular2', cycle_momentum = False)\n",
        "    elif args.scheduler == \"one-cycle-lr\":\n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr = args.lr, steps_per_epoch=len(train_loader), epochs=args.epochs, cycle_momentum = False)\n",
        "    else: # constant-lr\n",
        "        scheduler = None\n",
        "\n",
        "    print('Started Training')\n",
        "    print('Training Details:')\n",
        "    print('Network:         {}'.format(args.network))\n",
        "    print('Epochs:          {}'.format(args.epochs))\n",
        "    print('Batch Size:      {}'.format(args.batch_size))\n",
        "    print('Optimizer:       {}'.format(args.optimizer))\n",
        "    print('Scheduler:       {}'.format(args.scheduler))\n",
        "    print('Learning Rate:   {}'.format(args.lr))\n",
        "    print('Spectrum Length: {}'.format(args.spectrum_len))\n",
        "\n",
        "    date = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
        "\n",
        "    formatted_lr = '{:_.6f}'.format(float(args.lr)).rstrip('0').rstrip('.')\n",
        "    losses_dir = \"losses/{}_{}_{}_{}_{}.csv\".format(date, args.optimizer, args.scheduler, formatted_lr, args.network)\n",
        "    models_dir = \"{}_{}_{}_{}_{}.pt\".format(date, args.optimizer, args.scheduler, formatted_lr, args.network)\n",
        "\n",
        "    df = pd.DataFrame(columns=['epoch', 'train_loss', 'val_loss'])\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        train_loss, train_psnr, train_ssim = train(train_loader, net, optimizer, scheduler, criterion, criterion_MSE, epoch, args)\n",
        "        val_loss, valid_psnr, valid_ssim = validate(val_loader, net, criterion_MSE, args)\n",
        "        if args.scheduler != \"cyclic-lr\" and args.scheduler != \"one-cycle-lr\" and args.scheduler != \"constant-lr\":\n",
        "            scheduler.step()\n",
        "\n",
        "        new_row = pd.DataFrame({'epoch': [epoch], 'train_loss': [train_loss], 'val_loss': [val_loss]})\n",
        "\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        print('Epoch {} done'.format(epoch))\n",
        "        print('Loss/train: {}'.format(train_loss))\n",
        "        print('Loss/val: {}'.format(val_loss))\n",
        "        print('PSNR/train: {}'.format(train_psnr))\n",
        "        print('PSNR/val: {}'.format(valid_psnr))\n",
        "        print('SSIM/train: {}'.format(train_ssim))\n",
        "        print('SSIM/val: {}'.format(valid_ssim))\n",
        "\n",
        "    df.to_csv(losses_dir, index=False)\n",
        "    torch.save(net.state_dict(), models_dir)\n",
        "    print('Finished Training')"
      ],
      "metadata": {
        "id": "SNYVMuYwCfwP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_kmeans(args, k_folds = 5):\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "\n",
        "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "    ngpus_per_node = torch.cuda.device_count()\n",
        "\n",
        "    gpu = args.gpu\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define dataset path and data splits\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    dataset_path = \"Dataset/\"\n",
        "    image_ids_csv = pd.read_csv(dataset_path + \"Image_IDs.csv\")\n",
        "\n",
        "    image_ids = image_ids_csv[\"id\"].values\n",
        "\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create datasets and dataloaders\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Raman_Dataset_Train = torch.utils.data.ConcatDataset([RamanImageDataset(image_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len, spectrum_shift = 0.1, spectrum_flip = True,\n",
        "                                                    horizontal_flip = True, vertical_flip = True, rotate = True, patch = True, mixup = True),\n",
        "                                                    RamanImageDataset(image_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len, spectrum_shift = -0.2, spectrum_flip = True,\n",
        "                                                    horizontal_flip = True, vertical_flip = True, rotate = True, patch = True, mixup = True),\n",
        "                                                    RamanImageDataset(image_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len)])\n",
        "\n",
        "# From here down per fold\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(Raman_Dataset_Train)):\n",
        "      print(f\"Fold {fold + 1}\")\n",
        "      print(\"-------\")\n",
        "\n",
        "      train_loader = DataLoader(Raman_Dataset_Train, batch_size = args.batch_size, shuffle = False, num_workers = 0, pin_memory = True, sampler=torch.utils.data.SubsetRandomSampler(train_idx))\n",
        "      val_loader = DataLoader(Raman_Dataset_Train, batch_size = args.batch_size, shuffle = False, num_workers = 0, pin_memory = True, sampler=torch.utils.data.SubsetRandomSampler(test_idx))\n",
        "\n",
        "      # ----------------------------------------------------------------------------------------\n",
        "      # Create model(s) and send to device(s)\n",
        "      # ----------------------------------------------------------------------------------------\n",
        "      scale = args.hr_image_size // args.lr_image_size\n",
        "      net = Hyperspectral_RCAN(args.spectrum_len, scale).float()\n",
        "\n",
        "      if args.distributed:\n",
        "          if args.gpu is not None:\n",
        "              torch.cuda.set_device(args.gpu)\n",
        "              args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "              args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "\n",
        "              net.cuda(args.gpu)\n",
        "              net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.gpu])\n",
        "          else:\n",
        "              net.cuda(args.gpu)\n",
        "              net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "      elif args.gpu is not None:\n",
        "          torch.cuda.set_device(args.gpu)\n",
        "          net.cuda(args.gpu)\n",
        "      else:\n",
        "          net = nn.DataParallel(net).cuda()\n",
        "\n",
        "\n",
        "      # ----------------------------------------------------------------------------------------\n",
        "      # Define criterion(s), optimizer(s), and scheduler(s)\n",
        "      # ----------------------------------------------------------------------------------------\n",
        "\n",
        "      # ------------Criterion------------\n",
        "      criterion = nn.L1Loss().cuda(args.gpu)\n",
        "      criterion_MSE = nn.MSELoss().cuda(args.gpu)\n",
        "\n",
        "      # ------------Optimizer------------\n",
        "      if args.optimizer == \"sgd\":\n",
        "          optimizer = optim.SGD(net.parameters(), lr = args.lr)\n",
        "      elif args.optimizer == \"adamW\":\n",
        "          optimizer = optim.AdamW(net.parameters(), lr = args.lr)\n",
        "      else: # Adam\n",
        "          optimizer = optim.Adam(net.parameters(), lr = args.lr)\n",
        "\n",
        "      # ------------Scheduler------------\n",
        "      if args.scheduler == \"decay-lr\":\n",
        "          scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.2)\n",
        "      elif args.scheduler == \"multiplicative-lr\":\n",
        "          lmbda = lambda epoch: 0.985\n",
        "          scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
        "      elif args.scheduler == \"cyclic-lr\":\n",
        "          scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr = args.base_lr, max_lr = args.lr, mode = 'triangular2', cycle_momentum = False)\n",
        "      elif args.scheduler == \"one-cycle-lr\":\n",
        "          scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr = args.lr, steps_per_epoch=len(train_loader), epochs=args.epochs, cycle_momentum = False)\n",
        "      else: # constant-lr\n",
        "          scheduler = None\n",
        "\n",
        "      print('Started Training')\n",
        "      print('Training Details:')\n",
        "      print('Network:         {}'.format(args.network))\n",
        "      print('Epochs:          {}'.format(args.epochs))\n",
        "      print('Batch Size:      {}'.format(args.batch_size))\n",
        "      print('Optimizer:       {}'.format(args.optimizer))\n",
        "      print('Scheduler:       {}'.format(args.scheduler))\n",
        "      print('Learning Rate:   {}'.format(args.lr))\n",
        "      print('Spectrum Length: {}'.format(args.spectrum_len))\n",
        "\n",
        "      date = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
        "\n",
        "      formatted_lr = '{:_.6f}'.format(float(args.lr)).rstrip('0').rstrip('.')\n",
        "      losses_dir = \"losses/{}_{}_{}_{}_{}.csv\".format(date, args.optimizer, args.scheduler, formatted_lr, args.network)\n",
        "      models_dir = \"{}_{}_{}_{}_{}.pt\".format(date, args.optimizer, args.scheduler, formatted_lr, args.network)\n",
        "\n",
        "      df = pd.DataFrame(columns=['epoch', 'train_loss', 'val_loss'])\n",
        "\n",
        "      for epoch in range(args.epochs):\n",
        "          train_loss, train_psnr, train_ssim = train(train_loader, net, optimizer, scheduler, criterion, criterion_MSE, epoch, args)\n",
        "          val_loss, valid_psnr, valid_ssim = validate(val_loader, net, criterion_MSE, args)\n",
        "          if args.scheduler != \"cyclic-lr\" and args.scheduler != \"one-cycle-lr\" and args.scheduler != \"constant-lr\":\n",
        "              scheduler.step()\n",
        "\n",
        "          new_row = pd.DataFrame({'epoch': [epoch], 'train_loss': [train_loss], 'val_loss': [val_loss]})\n",
        "\n",
        "          df = pd.concat([df, new_row], ignore_index=True)\n",
        "          print('Epoch {} done'.format(epoch))\n",
        "          print('Loss/train: {}'.format(train_loss))\n",
        "          print('Loss/val: {}'.format(val_loss))\n",
        "          print('PSNR/train: {}'.format(train_psnr))\n",
        "          print('PSNR/val: {}'.format(valid_psnr))\n",
        "          print('SSIM/train: {}'.format(train_ssim))\n",
        "          print('SSIM/val: {}'.format(valid_ssim))\n",
        "\n",
        "\n",
        "      df.to_csv(losses_dir, index=False)\n",
        "      torch.save(net.state_dict(), models_dir)\n",
        "      print('Finished Training')"
      ],
      "metadata": {
        "id": "pxNZr11BjHvF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "qtYI_0qkGa3n",
        "outputId": "15d47b14-5f3b-4723-9cb4-dd14c1ac8290",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "O1sjKibTHrPN",
        "outputId": "202f990f-5bb2-45f1-8d1f-3c8c788a1f30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset.py  model.py  ResUNet.pt  utilities.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J1ZBnz0lm1w",
        "outputId": "99b3f189-5b52-47ce-ce9e-c8d710287ae5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/My\\ Drive/Colab\\ Notebooks/DeepeR-master/Hyperspectral Super-Resolution"
      ],
      "metadata": {
        "id": "RAHNfyHBGcGD",
        "outputId": "bb3cddd4-e13c-4cf5-9815-e9f974b86e39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/DeepeR-master/Hyperspectral Super-Resolution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_psnr(output, target):\n",
        "    psnr = 0.\n",
        "    mse = nn.MSELoss()(output, target)\n",
        "    psnr = 10 * math.log10(torch.max(output)/mse)\n",
        "    return psnr\n",
        "\n",
        "def calc_ssim(output, target):\n",
        "    ssim = 0.\n",
        "    output = output.cpu().detach().numpy()\n",
        "    target = target.cpu().detach().numpy()\n",
        "\n",
        "    if output.ndim == 4:\n",
        "        for i in range(output.shape[0]):\n",
        "            output_i = np.squeeze(output[i,:,:,:])\n",
        "            output_i = np.moveaxis(output_i, 0, -1)\n",
        "            target_i = np.squeeze(target[i,:,:,:])\n",
        "            target_i = np.moveaxis(target_i, 0, -1)\n",
        "            batch_size = output.shape[0]\n",
        "            ssim += sk_ssim(output_i, target_i, data_range = output_i.max() - target_i.max(), multichannel=True)\n",
        "    else:\n",
        "        output_i = np.squeeze(output)\n",
        "        output_i = np.moveaxis(output_i, 0, -1)\n",
        "        target_i = np.squeeze(target)\n",
        "        target_i = np.moveaxis(target_i, 0, -1)\n",
        "        batch_size = 1\n",
        "        ssim += sk_ssim(output_i, target_i, data_range = output_i.max() - target_i.max(), multichannel=True)\n",
        "\n",
        "    ssim = ssim / batch_size\n",
        "    return ssim"
      ],
      "metadata": {
        "id": "6sQ6cib9zsW5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Default args from original code\n",
        "\n",
        "class Arguments:\n",
        "    pass\n",
        "\n",
        "args = Arguments()\n",
        "args.workers = 0\n",
        "args.epochs = 200\n",
        "args.start_epoch = 0\n",
        "args.batch_size = 2\n",
        "args.network = \"Hyperspectral_RCAN\"\n",
        "args.lam = 100\n",
        "args.optimizer = \"adam\"\n",
        "args.lr = 1e-5\n",
        "args.base_lr = 1e-7\n",
        "args.scheduler = \"constant-lr\"\n",
        "args.lr_image_size = 16\n",
        "args.hr_image_size = 64\n",
        "args.batch_norm = False\n",
        "args.spectrum_len = 500\n",
        "args.seed = None\n",
        "args.gpu = 0\n",
        "args.world_size = -1\n",
        "args.rank = -1\n",
        "args.dist_url = \"tcp://224.66.41.62:23456\"\n",
        "args.dist_backend = \"nccl\"\n",
        "args.multiprocessing_distributed = False\n",
        "\n",
        "\n",
        "args.epochs=1\n",
        "train_noKmeans(args)\n",
        "# train_kmeans(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqEw59PrCzKG",
        "outputId": "324c5a73-58d8-4579-f3e9-a48435ce295c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: 0 for training\n",
            "Started Training\n",
            "Training Details:\n",
            "Network:         Hyperspectral_RCAN\n",
            "Epochs:          1\n",
            "Batch Size:      2\n",
            "Optimizer:       adam\n",
            "Scheduler:       constant-lr\n",
            "Learning Rate:   1e-05\n",
            "Spectrum Length: 500\n",
            "Epoch: [0][  0/144]\tTime  4.314 ( 4.314)\tPSNR 17.0118 (17.0118)\tSSIM 0.0091 (0.0091)\n",
            "Epoch: [0][ 20/144]\tTime  3.955 ( 4.006)\tPSNR 21.3919 (18.9938)\tSSIM 0.5112 (0.2983)\n",
            "Epoch: [0][ 40/144]\tTime  6.891 ( 4.418)\tPSNR 23.8109 (19.6847)\tSSIM 0.7322 (0.4311)\n",
            "Epoch: [0][ 60/144]\tTime  3.102 ( 4.563)\tPSNR 19.5123 (20.0745)\tSSIM 0.4864 (0.4894)\n",
            "Epoch: [0][ 80/144]\tTime  0.953 ( 3.948)\tPSNR 19.1198 (20.5232)\tSSIM 0.5198 (0.5326)\n",
            "Epoch: [0][100/144]\tTime  1.460 ( 3.412)\tPSNR 23.5244 (20.7842)\tSSIM 0.7869 (0.5575)\n",
            "Epoch: [0][120/144]\tTime  2.096 ( 3.094)\tPSNR 25.1095 (20.9567)\tSSIM 0.7709 (0.5743)\n",
            "Epoch: [0][140/144]\tTime  1.408 ( 2.848)\tPSNR 22.1812 (21.2061)\tSSIM 0.7031 (0.5935)\n",
            "Validation: [0/9]\tTime  6.219 ( 6.219)\tPSNR 22.0376 (22.0376)\tSSIM 0.7771 (0.7771)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-fa1df1fe9e9e>:139: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df, new_row], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 done\n",
            "Loss/train: 0.0013049501615872334\n",
            "Loss/val: 0.0005181702812585761\n",
            "PSNR/train: 21.23250354722852\n",
            "PSNR/val: 22.650568044232074\n",
            "SSIM/train: 0.5964688510471379\n",
            "SSIM/val: 0.7837556513237935\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "q-Nymahoae4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "def evaluate(dataloader, net, scale, args):\n",
        "\n",
        "    psnr = AverageMeter('PSNR', ':.4f')\n",
        "    ssim = AverageMeter('SSIM', ':.4f')\n",
        "    mse_NN = AverageMeter('MSE', ':.4f')\n",
        "    psnr_bicubic = AverageMeter('PSNR_Bicubic', ':.4f')\n",
        "    ssim_bicubic = AverageMeter('SSIM_Bicubic', ':.4f')\n",
        "    mse_bicubic = AverageMeter('MSE_Bicubic', ':.4f')\n",
        "    psnr_nearest_neighbours = AverageMeter('PSNR_Nearest_Neighbours', ':.4f')\n",
        "    ssim_nearest_neighbours = AverageMeter('SSIM_Nearest_Neighbours', ':.4f')\n",
        "    mse_nearest_neighbours = AverageMeter('MSE_Nearest_Neighbours', ':.4f')\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(dataloader):\n",
        "            # measure data loading time\n",
        "            x = data['input_image']\n",
        "            inputs = x.float()\n",
        "            inputs = inputs.cuda(args.gpu)\n",
        "            y = data['output_image']\n",
        "            target = y.float()\n",
        "            target = target.cuda(args.gpu)\n",
        "\n",
        "            # compute output\n",
        "            output = net(inputs)\n",
        "\n",
        "            x2 = np.squeeze(x.numpy())\n",
        "            y2 = np.squeeze(y.numpy())\n",
        "\n",
        "            nearest_neighbours = scipy.ndimage.zoom(x2,(1,scale,scale), order=0)\n",
        "            bicubic = scipy.ndimage.zoom(x2,(1,scale,scale), order=3)\n",
        "\n",
        "            bicubic = torch.from_numpy(bicubic)\n",
        "            bicubic = bicubic.cuda(args.gpu)\n",
        "\n",
        "            nearest_neighbours = torch.from_numpy(nearest_neighbours)\n",
        "            nearest_neighbours = nearest_neighbours.cuda(args.gpu)\n",
        "\n",
        "            # Nearest neighbours\n",
        "            psnr_batch_nearest_neighbours = calc_psnr(nearest_neighbours, target)\n",
        "            psnr_nearest_neighbours.update(psnr_batch_nearest_neighbours, inputs.size(0))\n",
        "\n",
        "            ssim_batch_nearest_neighbours = calc_ssim(nearest_neighbours, target)\n",
        "            ssim_nearest_neighbours.update(ssim_batch_nearest_neighbours, inputs.size(0))\n",
        "\n",
        "            mse_batch_nearest_neighbours = nn.MSELoss()(nearest_neighbours, target)\n",
        "            mse_nearest_neighbours.update(mse_batch_nearest_neighbours, inputs.size(0))\n",
        "\n",
        "            # Bicubic\n",
        "            psnr_batch_bicubic = calc_psnr(bicubic, target)\n",
        "            psnr_bicubic.update(psnr_batch_bicubic, inputs.size(0))\n",
        "\n",
        "            ssim_batch_bicubic = calc_ssim(bicubic, target)\n",
        "            ssim_bicubic.update(ssim_batch_bicubic, inputs.size(0))\n",
        "\n",
        "            mse_batch_bicubic = nn.MSELoss()(bicubic, target)\n",
        "            mse_bicubic.update(mse_batch_bicubic, inputs.size(0))\n",
        "\n",
        "            # Neural network\n",
        "            psnr_batch = calc_psnr(output, target)\n",
        "            psnr.update(psnr_batch, inputs.size(0))\n",
        "\n",
        "            ssim_batch = calc_ssim(output, target)\n",
        "            ssim.update(ssim_batch, inputs.size(0))\n",
        "\n",
        "            mse_batch = nn.MSELoss()(output, target)\n",
        "            mse_NN.update(mse_batch, inputs.size(0))\n",
        "\n",
        "    print(\"RCAN PSNR: {}    Bicubic PSNR: {}    Nearest Neighbours PSNR: {}\".format(psnr.avg, psnr_bicubic.avg, psnr_nearest_neighbours.avg))\n",
        "    print(\"RCAN SSIM: {}    Bicubic SSIM: {}    Nearest Neighbours SSIM: {}\".format(ssim.avg, ssim_bicubic.avg, ssim_nearest_neighbours.avg))\n",
        "    print(\"RCAN MSE:  {}    Bicubic MSE:  {}    Nearest Neighbours MSE:  {}\".format(mse_NN.avg, mse_bicubic.avg, mse_nearest_neighbours.avg))\n",
        "    return psnr.avg, psnr_bicubic.avg, psnr_nearest_neighbours.avg, ssim.avg, ssim_bicubic.avg, ssim_nearest_neighbours.avg, mse_NN.avg, mse_bicubic.avg, mse_nearest_neighbours.avg"
      ],
      "metadata": {
        "id": "8ujeP58VmTxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_test(args):\n",
        "\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "\n",
        "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "    ngpus_per_node = torch.cuda.device_count()\n",
        "\n",
        "    gpu = args.gpu\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create model(s) and send to device(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    scale = args.hr_image_size // args.lr_image_size\n",
        "    net = Hyperspectral_RCAN(args.spectrum_len, scale).float()\n",
        "\n",
        "    net.load_state_dict(torch.load(args.model))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.gpu is not None:\n",
        "            torch.cuda.set_device(args.gpu)\n",
        "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.gpu])\n",
        "        else:\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "    elif args.gpu is not None:\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "        net.cuda(args.gpu)\n",
        "    else:\n",
        "        net = nn.DataParallel(net).cuda()\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define dataset path and data splits\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    dataset_path = \"Dataset/\"\n",
        "    image_ids_csv = pd.read_csv(dataset_path + \"Image_IDs.csv\")\n",
        "\n",
        "    image_ids = image_ids_csv[\"id\"].values\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create datasets and dataloaders\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Raman_Dataset_Test = RamanImageDataset(image_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len)\n",
        "\n",
        "    test_loader = DataLoader(Raman_Dataset_Test, batch_size = args.batch_size, shuffle = False, num_workers = args.workers)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Evaluate\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    RCAN_PSNR, Bicubic_PSNR, Nearest_PSNR, RCAN_SSIM, Bicubic_SSIM, Nearest_SSIM, RCAN_MSE, Bicubic_MSE, Nearest_MSE = evaluate(test_loader, net, scale, args)"
      ],
      "metadata": {
        "id": "yqtQlVwwK3GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Arguments:\n",
        "    pass\n",
        "\n",
        "args = Arguments()\n",
        "args.workers = 0\n",
        "args.batch_size = 1\n",
        "args.spectrum_len = 500\n",
        "args.network = \"Hyperspectral_RCAN\"\n",
        "args.lr_image_size = 16\n",
        "args.hr_image_size = 64\n",
        "args.spectrum_len = 500\n",
        "args.seed = None\n",
        "args.gpu = 0\n",
        "args.world_size = -1\n",
        "args.rank = -1\n",
        "args.dist_url = \"tcp://224.66.41.62:23456\"\n",
        "args.dist_backend = \"nccl\"\n",
        "args.multiprocessing_distributed = False\n",
        "args.batch_norm = True\n",
        "args.model = \"2024_05_22_adam_constant-lr_Hyperspectral_RCAN_4x.pt\"\n",
        "\n",
        "\n",
        "main_test(args)"
      ],
      "metadata": {
        "id": "I97mxHSasvCS",
        "outputId": "51579610-7f34-4ab3-eb83-d6f77d1a3f6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: 0 for testing\n",
            "Neural Network MSE: 0.00260821853749535\n",
            "Neural Network PSNR: 26.159036795137737\n",
            "Neural Network SSIM: 0.2734942460924173\n",
            "Savitzky-Golay MSE: 0.027660622850368643\n",
            "Neural Network performed 10.61x better than Savitzky-Golay\n"
          ]
        }
      ]
    }
  ]
}
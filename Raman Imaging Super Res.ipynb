{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1GHdCjqh84Xexa4GTzb8qntXQ08wgcIgd",
      "authorship_tag": "ABX9TyO3AlxfNpXA3B/FcMBXogH9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiverBotham/Raman/blob/main/Raman%20Imaging%20Super%20Res.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO:\n",
        "\n",
        "\n",
        "*   Add utilities to github\n",
        "*   Update this notebook to clone repo\n",
        "*   Add updates to this notbook to run a train & test for de-noising using images from google drive but utilities from github\n",
        "*   Add in k-means & testing framework\n",
        "*   Repeat with second notebook for hyper-spectral super sesolution\n",
        "\n"
      ],
      "metadata": {
        "id": "D077tOtY-b6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To save forst clone the repo\n",
        "!git config --global user.name \"RiverBotham\"\n",
        "!git config --global user.email \"river.botham@gmail.com\"\n",
        "!git config --global user.password \"MY_PASSWORD\"\n",
        "\n",
        "token = 'MY_TOKEN'\n",
        "username = 'RiverBotham'\n",
        "repo = 'Raman'\n",
        "\n",
        "!git clone https://{token}@github.com/{username}/{repo}"
      ],
      "metadata": {
        "id": "q-lUBSc7BMNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5766f84e-4444-4efa-d9c7-7c37677b699a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Raman'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 56 (delta 27), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (56/56), 7.42 MiB | 9.69 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move into the cloned repo, then File -> Save copy in GitHub\n",
        "%cd {repo}/Denoising"
      ],
      "metadata": {
        "id": "zzLakAJlDBnY",
        "outputId": "827e2aa8-f469-4428-c996-659494590025",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Raman/Denoising\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import datetime\n",
        "import time\n",
        "import shutil\n",
        "import argparse\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import scipy.signal\n",
        "import math\n",
        "from skimage.metrics import structural_similarity as sk_ssim\n",
        "from sklearn.model_selection import KFold\n",
        "from skimage.transform import resize\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.distributed as dist\n",
        "import torch.utils.data.distributed\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# import model, dataset, utilities"
      ],
      "metadata": {
        "id": "KWeXO7nYDF8j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "\n",
        "\n",
        "class ChannelAttentionBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttentionBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.chan_attn = nn.Sequential(\n",
        "                nn.Conv2d(channels, channels // reduction, 1, padding=0, bias=True),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channels // reduction, channels, 1, padding=0, bias=True),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.chan_attn(y)\n",
        "        return x * y\n",
        "\n",
        "class ResidualChannelAttentionBlock(nn.Module):\n",
        "    def __init__(self, channels=500, kernel_size=3, reduction=16, bias=True, act=nn.ReLU(True)):\n",
        "        super(ResidualChannelAttentionBlock, self).__init__()\n",
        "        modules_body = []\n",
        "        for i in range(2):\n",
        "            modules_body.append(nn.Conv2d(channels, channels, kernel_size, padding=(kernel_size//2), bias=bias))\n",
        "            if i == 0: modules_body.append(act)\n",
        "        modules_body.append(ChannelAttentionBlock(channels, reduction))\n",
        "\n",
        "        self.body = nn.Sequential(*modules_body)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.body(x)\n",
        "        res += x\n",
        "        return res\n",
        "\n",
        "class ResidualGroup(nn.Module):\n",
        "    def __init__(self, channels=500, kernel_size=3, reduction=16, bias=True, act=nn.ReLU(True), n_resblocks=6):\n",
        "        super(ResidualGroup, self).__init__()\n",
        "        modules_body = []\n",
        "        modules_body = [ResidualChannelAttentionBlock(channels, kernel_size, reduction, bias=bias, act=nn.ReLU(True)) for _ in range(n_resblocks)]\n",
        "        modules_body.append(nn.Conv2d(channels, channels, kernel_size, padding=(kernel_size//2), bias=bias))\n",
        "\n",
        "        self.body = nn.Sequential(*modules_body)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.body(x)\n",
        "        res += x\n",
        "        return res\n",
        "\n",
        "class Upsampler(nn.Sequential):\n",
        "    def __init__(self, scale, channels, kernel_size, bn=False, act=False, bias=True):\n",
        "\n",
        "        m = []\n",
        "        if (scale & (scale - 1)) == 0:\n",
        "            for _ in range(int(math.log(scale, 2))):\n",
        "                conv = nn.Conv2d(channels, 4*channels, kernel_size, padding=(kernel_size//2), bias=bias)\n",
        "                m.append(conv)\n",
        "                m.append(nn.PixelShuffle(2))\n",
        "                if bn: m.append(nn.BatchNorm2d(channels))\n",
        "                if act: m.append(nn.ReLU(True))\n",
        "        elif scale == 3:\n",
        "            m.append(nn.Conv2d(channels, 9*channels, kernel_size, padding=(kernel_size//2), bias=bias))\n",
        "            m.append(nn.PixelShuffle(3))\n",
        "            if bn: m.append(nn.BatchNorm2d(channels))\n",
        "            if act: m.append(nn.ReLU(True))\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        super(Upsampler, self).__init__(*m)\n",
        "\n",
        "class Hyperspectral_RCAN(nn.Module):\n",
        "    def __init__(self, spectrum_len, scale=4, kernel_size=3, reduction=16, bias=True, act=nn.ReLU(True), n_resblocks=16, n_resgroups=18):\n",
        "        super(Hyperspectral_RCAN, self).__init__()\n",
        "        modules_head1 = [Upsampler(scale, spectrum_len, kernel_size, act=False), nn.Conv2d(spectrum_len, spectrum_len, kernel_size, padding=(kernel_size//2), bias=bias)]\n",
        "        modules_head2 = [nn.Conv2d(spectrum_len, int(spectrum_len/2), kernel_size, padding=(kernel_size//2), bias=bias)]\n",
        "\n",
        "        modules_body = [ResidualGroup(int(spectrum_len/2), kernel_size, reduction, act, n_resblocks) for _ in range(n_resgroups)]\n",
        "        modules_body.append(nn.Conv2d(int(spectrum_len/2), int(spectrum_len/2), kernel_size, padding=(kernel_size//2), bias=bias))\n",
        "\n",
        "        modules_tail = [nn.Conv2d(int(spectrum_len/2), int(spectrum_len/2), kernel_size, padding=(kernel_size//2), bias=bias)]\n",
        "        modules_tail.append(nn.Conv2d(int(spectrum_len/2), spectrum_len, kernel_size, padding=(kernel_size//2), bias=bias))\n",
        "\n",
        "        self.head1 = nn.Sequential(*modules_head1)\n",
        "        self.head2 = nn.Sequential(*modules_head2)\n",
        "        self.body = nn.Sequential(*modules_body)\n",
        "        self.tail = nn.Sequential(*modules_tail)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.head1(x)\n",
        "        x1 = self.head2(x)\n",
        "\n",
        "        res1 = self.body(x1)\n",
        "        res1 += x1\n",
        "\n",
        "        res2 = self.tail(res1)\n",
        "        res2 += x\n",
        "\n",
        "        return res2"
      ],
      "metadata": {
        "id": "Mei5k_ff0fci"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data set\n",
        "\n",
        "\n",
        "class RamanImageDataset(Dataset):\n",
        "    def __init__(self, image_ids, path, batch_size=2, hr_image_size=64, lr_image_size=16, spectrum_len=500,\n",
        "                spectrum_shift = 0., spectrum_flip = False, horizontal_flip = False, vertical_flip = False,\n",
        "                 rotate = False, patch = False, mixup = False):\n",
        "        self.image_ids = image_ids\n",
        "        self.path = path\n",
        "        self.batch_size = batch_size\n",
        "        self.hr_image_size = hr_image_size\n",
        "        self.lr_image_size = lr_image_size\n",
        "        self.spectrum_len = spectrum_len\n",
        "        self.spectrum_shift = spectrum_shift\n",
        "        self.spectrum_flip = spectrum_flip\n",
        "        self.horizontal_flip = horizontal_flip\n",
        "        self.vertical_flip = vertical_flip\n",
        "        self.rotate = rotate\n",
        "        self.patch = patch\n",
        "        self.mixup = mixup\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def load_image(self, id_name):\n",
        "        input_path =self.path + id_name + \".mat\"\n",
        "\n",
        "        output_data = scipy.io.loadmat(input_path)\n",
        "        output_values = list(output_data.values())\n",
        "        output_image = output_values[3]\n",
        "        return output_image\n",
        "\n",
        "    def pad_image(self, image, size, patch):\n",
        "        if image.shape[0] == size and image.shape[1] == size:\n",
        "            padded_image = image\n",
        "        elif image.shape[0] > size and image.shape[1] > size:\n",
        "            if patch:\n",
        "                padded_image = self.get_image_patch(image, size)\n",
        "            else:\n",
        "                padded_image = self.center_crop_image(image, size)\n",
        "        else:\n",
        "            padded_image = image\n",
        "            if padded_image.shape[0] > size:\n",
        "                if patch:\n",
        "                    padded_image = self.get_image_patch(padded_image, size)\n",
        "                else:\n",
        "                    padded_image = self.center_crop_image(padded_image, size)\n",
        "            else:\n",
        "                pad_before = int(np.floor((size - padded_image.shape[0])/2))\n",
        "                pad_after = int(np.ceil((size - padded_image.shape[0])/2))\n",
        "                padded_image = np.pad(padded_image, ((pad_before, pad_after), (0,0), (0, 0)), 'reflect')\n",
        "\n",
        "            if padded_image.shape[1] > size:\n",
        "                if patch:\n",
        "                    padded_image = self.get_image_patch(padded_image, size)\n",
        "                else:\n",
        "                    padded_image = self.center_crop_image(padded_image, size)\n",
        "            else:\n",
        "                pad_before = int(np.floor((size - padded_image.shape[1])/2))\n",
        "                pad_after = int(np.ceil((size - padded_image.shape[1])/2))\n",
        "                padded_image = np.pad(padded_image, ((0,0), (pad_before, pad_after), (0, 0)), 'reflect')\n",
        "\n",
        "        return padded_image\n",
        "\n",
        "    def get_image_patch(self, image, patch_size):\n",
        "        if image.shape[0] > patch_size:\n",
        "            start_idx_x = int(np.round(np.random.random() * (image.shape[0]-patch_size)))\n",
        "            end_idx_x = start_idx_x + patch_size\n",
        "        else:\n",
        "            start_idx_x = 0\n",
        "            end_idx_x = image.shape[0]\n",
        "\n",
        "        if image.shape[1] > patch_size:\n",
        "            start_idx_y = int(np.round(np.random.random() * (image.shape[1]-patch_size)))\n",
        "            end_idx_y = start_idx_y + patch_size\n",
        "        else:\n",
        "            start_idx_y = 0\n",
        "            end_idx_y = image.shape[1]\n",
        "\n",
        "        image_patch = image[start_idx_x:end_idx_x,start_idx_y:end_idx_y,:]\n",
        "        return image_patch\n",
        "\n",
        "    def center_crop_image(self, image, image_size):\n",
        "        cropped_image = image\n",
        "        if image.shape[0] > image_size:\n",
        "            dif = int(np.floor((image.shape[0] - image_size)/2))\n",
        "            cropped_image = cropped_image[dif:image_size+dif,:,:]\n",
        "\n",
        "        if image.shape[1] > image_size:\n",
        "            dif = int(np.floor((image.shape[1] - image_size)/2))\n",
        "            cropped_image = cropped_image[:,dif:image_size+dif,:]\n",
        "        return cropped_image\n",
        "\n",
        "    def flip_axis(self, image, axis):\n",
        "        if np.random.random() < 0.5:\n",
        "            image = np.asarray(image).swapaxes(axis, 0)\n",
        "            image = image[::-1, ...]\n",
        "            image = image.swapaxes(0, axis)\n",
        "        return image\n",
        "\n",
        "    def rotate_spectral_image(self, image):\n",
        "        rotation_extent = np.random.random()\n",
        "        if rotation_extent < 0.25:\n",
        "            rotation = 1\n",
        "        elif rotation_extent < 0.5:\n",
        "            rotation = 2\n",
        "        elif rotation_extent < 0.75:\n",
        "            rotation = 3\n",
        "        else:\n",
        "            rotation = 0\n",
        "        image = np.rot90(image, rotation)\n",
        "        return image\n",
        "\n",
        "    def shift_spectrum(self, image, shift_range):\n",
        "        shifted_spectrum_image = image\n",
        "        spectrum_shift_range = int(np.round(shift_range*image.shape[2]))\n",
        "        if spectrum_shift_range > 0:\n",
        "            shifted_spectrum_image = np.pad(image[:,:,spectrum_shift_range:], ((0,0), (0,0), (0,abs(spectrum_shift_range))), 'reflect')\n",
        "        elif spectrum_shift_range < 0:\n",
        "            shifted_spectrum_image = np.pad(image[:,:,:spectrum_shift_range], ((0,0), (0,0), (abs(spectrum_shift_range), 0)), 'reflect')\n",
        "        return shifted_spectrum_image\n",
        "\n",
        "    def spectrum_padding(self, image, spectrum_length):\n",
        "        if image.shape[-1] == spectrum_length:\n",
        "            padded_spectrum_image = image\n",
        "        elif image.shape[-1] > spectrum_length:\n",
        "            padded_spectrum_image = image[:,:,0:spectrum_length]\n",
        "        else:\n",
        "            padded_spectrum_image = np.pad(image, ((0,0), (0,0), (0, spectrum_length - image.shape[-1])), 'reflect')\n",
        "        return padded_spectrum_image\n",
        "\n",
        "    def image_mixup(self, image1, image2, alpha):\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        image = (lam * image1) + ((1 - lam) * image2)\n",
        "        return image\n",
        "\n",
        "    def normalise_image(self, image):\n",
        "        image_max = np.tile(np.amax(image),image.shape)\n",
        "        normalised_image = np.divide(image,image_max)\n",
        "        return normalised_image\n",
        "\n",
        "    def downsample_image(self, image, scale = 4):\n",
        "        if scale >= 4:\n",
        "            start_idx = np.random.randint(1,scale-1)\n",
        "        else:\n",
        "            start_idx = 1\n",
        "        downsampled_image = image[start_idx::scale,start_idx::scale,:]\n",
        "        return downsampled_image\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_size_ratio = self.hr_image_size // self.lr_image_size\n",
        "\n",
        "        outputimg = self.load_image(self.image_ids[idx])\n",
        "\n",
        "        mixup_on = False\n",
        "        if self.mixup:\n",
        "            if np.random.random() < 0.5:\n",
        "                image_idx = int(np.round(np.random.random() * (len(self.image_ids)-1)))\n",
        "                image2 = self.load_image(self.image_ids[image_idx])\n",
        "                mixup_on = True\n",
        "\n",
        "        # --------------- Image Data Augmentations ---------------\n",
        "        outputimg = self.pad_image(outputimg, self.hr_image_size, self.patch)\n",
        "        if mixup_on:\n",
        "            image2 = self.pad_image(image2, self.hr_image_size, self.patch)\n",
        "\n",
        "        if self.horizontal_flip:\n",
        "            outputimg = self.flip_axis(outputimg, 1)\n",
        "            if mixup_on:\n",
        "                image2 = self.flip_axis(image2, 1)\n",
        "\n",
        "        if self.vertical_flip:\n",
        "            outputimg = self.flip_axis(outputimg, 0)\n",
        "            if mixup_on:\n",
        "                image2 = self.flip_axis(image2, 0)\n",
        "\n",
        "        if self.rotate:\n",
        "            outputimg = self.rotate_spectral_image(outputimg)\n",
        "            if mixup_on:\n",
        "                image2 = self.rotate_spectral_image(image2)\n",
        "\n",
        "        # --------------- Spectral Data Augmentations ---------------\n",
        "        if self.spectrum_shift != 0.0:\n",
        "            shift_range = np.random.uniform(-self.spectrum_shift, self.spectrum_shift)\n",
        "            outputimg = self.shift_spectrum(outputimg, shift_range)\n",
        "            if mixup_on:\n",
        "                image2 = self.shift_spectrum(image2, shift_range)\n",
        "\n",
        "        outputimg = self.spectrum_padding(outputimg, self.spectrum_len)\n",
        "        if mixup_on:\n",
        "            image2 = self.spectrum_padding(image2, self.spectrum_len)\n",
        "\n",
        "        if self.spectrum_flip:\n",
        "            if np.random.random() < 0.5:\n",
        "                outputimg = self.flip_axis(outputimg, 2)\n",
        "                if mixup_on:\n",
        "                    image2 = self.flip_axis(image2, 2)\n",
        "\n",
        "        # --------------- Mixup ---------------\n",
        "        if mixup_on:\n",
        "            outputimg = self.image_mixup(outputimg, image2, 0.2)\n",
        "\n",
        "        # --------------- Normalisation and Downsampling ---------------\n",
        "        outputimg = self.normalise_image(outputimg)\n",
        "        inputimg = self.downsample_image(outputimg, image_size_ratio)\n",
        "\n",
        "        outputimg = np.moveaxis(outputimg, -1, 0)\n",
        "        inputimg = np.moveaxis(inputimg, -1, 0)\n",
        "\n",
        "        sample = {'input_image': inputimg, 'output_image': outputimg}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)"
      ],
      "metadata": {
        "id": "w6PAklAu0hNJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utilities\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)"
      ],
      "metadata": {
        "id": "3NAulroG0i29"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, net, optimizer, scheduler, criterion, criterion_MSE, epoch, args):\n",
        "\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    psnr = AverageMeter('PSNR', ':.4f')\n",
        "    ssim = AverageMeter('SSIM', ':.4f')\n",
        "    progress = ProgressMeter(len(dataloader), [batch_time, psnr, ssim], prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    end = time.time()\n",
        "    for i, data in enumerate(dataloader):\n",
        "        inputs = data['input_image']\n",
        "        inputs = inputs.float()\n",
        "        inputs = inputs.cuda(args.gpu)\n",
        "        target = data['output_image']\n",
        "        target = target.float()\n",
        "        target = target.cuda(args.gpu)\n",
        "\n",
        "        output = net(inputs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if args.scheduler == \"cyclic-lr\" or args.scheduler == \"one-cycle-lr\":\n",
        "            scheduler.step()\n",
        "\n",
        "        loss_MSE = criterion_MSE(output, target)\n",
        "        losses.update(loss_MSE.item(), inputs.size(0))\n",
        "\n",
        "        psnr_batch = calc_psnr(output, target)\n",
        "        psnr.update(psnr_batch, inputs.size(0))\n",
        "\n",
        "        ssim_batch = calc_ssim(output, target)\n",
        "        ssim.update(ssim_batch, inputs.size(0))\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 20 == 0:\n",
        "            progress.display(i)\n",
        "    return losses.avg, psnr.avg, ssim.avg\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "smrIJIsQDp3c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(dataloader, net, criterion_MSE, args):\n",
        "\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    psnr = AverageMeter('PSNR', ':.4f')\n",
        "    ssim = AverageMeter('SSIM', ':.4f')\n",
        "    progress = ProgressMeter(len(dataloader), [batch_time, psnr, ssim], prefix='Validation: ')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, data in enumerate(dataloader):\n",
        "            inputs = data['input_image']\n",
        "            inputs = inputs.float()\n",
        "            inputs = inputs.cuda(args.gpu)\n",
        "            target = data['output_image']\n",
        "            target = target.float()\n",
        "            target = target.cuda(args.gpu)\n",
        "\n",
        "            output = net(inputs)\n",
        "\n",
        "            loss_MSE = criterion_MSE(output, target)\n",
        "            losses.update(loss_MSE.item(), inputs.size(0))\n",
        "\n",
        "            psnr_batch = calc_psnr(output, target)\n",
        "            psnr.update(psnr_batch, inputs.size(0))\n",
        "\n",
        "            ssim_batch = calc_ssim(output, target)\n",
        "            ssim.update(ssim_batch, inputs.size(0))\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % 20 == 0:\n",
        "                progress.display(i)\n",
        "\n",
        "    return losses.avg, psnr.avg, ssim.avg"
      ],
      "metadata": {
        "id": "ZvHvb_vkD2Wm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_noKmeans(args):\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "\n",
        "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "    ngpus_per_node = torch.cuda.device_count()\n",
        "\n",
        "    gpu = args.gpu\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create model(s) and send to device(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    scale = args.hr_image_size // args.lr_image_size\n",
        "    net = Hyperspectral_RCAN(args.spectrum_len, scale).float()\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.gpu is not None:\n",
        "            torch.cuda.set_device(args.gpu)\n",
        "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.gpu])\n",
        "        else:\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "    elif args.gpu is not None:\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "        net.cuda(args.gpu)\n",
        "    else:\n",
        "        net = nn.DataParallel(net).cuda()\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define dataset path and data splits\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    dataset_path = \"Dataset/\"\n",
        "    image_ids_csv = pd.read_csv(dataset_path + \"Image_IDs.csv\")\n",
        "\n",
        "    image_ids = image_ids_csv[\"id\"].values\n",
        "\n",
        "    train_split = round(0.85 * len(image_ids))\n",
        "    val_split = round(0.10 * len(image_ids))\n",
        "    test_split = round(0.05 * len(image_ids))\n",
        "    train_ids = image_ids[:train_split]\n",
        "    val_ids = image_ids[train_split:train_split+val_split]\n",
        "    test_ids = image_ids[train_split+val_split:]\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create datasets and dataloaders\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Raman_Dataset_Train = torch.utils.data.ConcatDataset([RamanImageDataset(train_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len, spectrum_shift = 0.1, spectrum_flip = True,\n",
        "                                                    horizontal_flip = True, vertical_flip = True, rotate = True, patch = True, mixup = True),\n",
        "                                                    RamanImageDataset(train_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len, spectrum_shift = -0.2, spectrum_flip = True,\n",
        "                                                    horizontal_flip = True, vertical_flip = True, rotate = True, patch = True, mixup = True)])\n",
        "\n",
        "    Raman_Dataset_Val = RamanImageDataset(val_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len)\n",
        "\n",
        "    train_loader = DataLoader(Raman_Dataset_Train, batch_size = args.batch_size, shuffle = False, num_workers = args.workers)\n",
        "    val_loader = DataLoader(Raman_Dataset_Val, batch_size = args.batch_size, shuffle = False, num_workers = args.workers)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define criterion(s), optimizer(s), and scheduler(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "\n",
        "    # ------------Criterion------------\n",
        "    criterion = nn.L1Loss().cuda(args.gpu)\n",
        "    criterion_MSE = nn.MSELoss().cuda(args.gpu)\n",
        "\n",
        "    # ------------Optimizer------------\n",
        "    if args.optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(net.parameters(), lr = args.lr)\n",
        "    elif args.optimizer == \"adamW\":\n",
        "        optimizer = optim.AdamW(net.parameters(), lr = args.lr)\n",
        "    else: # Adam\n",
        "        optimizer = optim.Adam(net.parameters(), lr = args.lr)\n",
        "\n",
        "    # ------------Scheduler------------\n",
        "    if args.scheduler == \"decay-lr\":\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.2)\n",
        "    elif args.scheduler == \"multiplicative-lr\":\n",
        "        lmbda = lambda epoch: 0.985\n",
        "        scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
        "    elif args.scheduler == \"cyclic-lr\":\n",
        "        scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr = args.base_lr, max_lr = args.lr, mode = 'triangular2', cycle_momentum = False)\n",
        "    elif args.scheduler == \"one-cycle-lr\":\n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr = args.lr, steps_per_epoch=len(train_loader), epochs=args.epochs, cycle_momentum = False)\n",
        "    else: # constant-lr\n",
        "        scheduler = None\n",
        "\n",
        "    print('Started Training')\n",
        "    print('Training Details:')\n",
        "    print('Network:         {}'.format(args.network))\n",
        "    print('Epochs:          {}'.format(args.epochs))\n",
        "    print('Batch Size:      {}'.format(args.batch_size))\n",
        "    print('Optimizer:       {}'.format(args.optimizer))\n",
        "    print('Scheduler:       {}'.format(args.scheduler))\n",
        "    print('Learning Rate:   {}'.format(args.lr))\n",
        "    print('Spectrum Length: {}'.format(args.spectrum_len))\n",
        "\n",
        "    date = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
        "\n",
        "    log_dir = \"runs/{}_{}_{}_{}_{}x\".format(date, args.optimizer, args.scheduler, args.network, scale)\n",
        "    models_dir = \"{}_{}_{}_{}_{}x.pt\".format(date, args.optimizer, args.scheduler, args.network, scale)\n",
        "\n",
        "    writer = SummaryWriter(log_dir = log_dir)\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        train_loss, train_psnr, train_ssim = train(train_loader, net, optimizer, scheduler, criterion, criterion_MSE, epoch, args)\n",
        "        valid_loss, valid_psnr, valid_ssim = validate(val_loader, net, criterion_MSE, args)\n",
        "        if args.scheduler != \"cyclic-lr\" and args.scheduler != \"one-cycle-lr\" and args.scheduler != \"constant-lr\":\n",
        "            scheduler.step()\n",
        "\n",
        "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "        writer.add_scalar('Loss/val', valid_loss, epoch)\n",
        "        writer.add_scalar('PSNR/train', train_psnr, epoch)\n",
        "        writer.add_scalar('PSNR/val', valid_psnr, epoch)\n",
        "        writer.add_scalar('SSIM/train', train_ssim, epoch)\n",
        "        writer.add_scalar('SSIM/val', valid_ssim, epoch)\n",
        "\n",
        "    torch.save(net.state_dict(), models_dir)\n",
        "    print('Finished Training')"
      ],
      "metadata": {
        "id": "SNYVMuYwCfwP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_kmeans(args, k_folds = 5):\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "\n",
        "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "    ngpus_per_node = torch.cuda.device_count()\n",
        "\n",
        "    gpu = args.gpu\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define dataset path and data splits\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    dataset_path = \"Dataset/\"\n",
        "    image_ids_csv = pd.read_csv(dataset_path + \"Image_IDs.csv\")\n",
        "\n",
        "    image_ids = image_ids_csv[\"id\"].values\n",
        "\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create datasets and dataloaders\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Raman_Dataset_Train = torch.utils.data.ConcatDataset([RamanImageDataset(image_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len, spectrum_shift = 0.1, spectrum_flip = True,\n",
        "                                                    horizontal_flip = True, vertical_flip = True, rotate = True, patch = True, mixup = True),\n",
        "                                                    RamanImageDataset(image_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len, spectrum_shift = -0.2, spectrum_flip = True,\n",
        "                                                    horizontal_flip = True, vertical_flip = True, rotate = True, patch = True, mixup = True),\n",
        "                                                    RamanImageDataset(image_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len)])\n",
        "\n",
        "# From here down per fold\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(Raman_Dataset_Train)):\n",
        "      print(f\"Fold {fold + 1}\")\n",
        "      print(\"-------\")\n",
        "\n",
        "      train_loader = DataLoader(Raman_Dataset_Train, batch_size = args.batch_size, shuffle = False, num_workers = 0, pin_memory = True, sampler=torch.utils.data.SubsetRandomSampler(train_idx))\n",
        "      val_loader = DataLoader(Raman_Dataset_Train, batch_size = args.batch_size, shuffle = False, num_workers = 0, pin_memory = True, sampler=torch.utils.data.SubsetRandomSampler(test_idx))\n",
        "\n",
        "      # ----------------------------------------------------------------------------------------\n",
        "      # Create model(s) and send to device(s)\n",
        "      # ----------------------------------------------------------------------------------------\n",
        "      scale = args.hr_image_size // args.lr_image_size\n",
        "      net = Hyperspectral_RCAN(args.spectrum_len, scale).float()\n",
        "\n",
        "      if args.distributed:\n",
        "          if args.gpu is not None:\n",
        "              torch.cuda.set_device(args.gpu)\n",
        "              args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "              args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "\n",
        "              net.cuda(args.gpu)\n",
        "              net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.gpu])\n",
        "          else:\n",
        "              net.cuda(args.gpu)\n",
        "              net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "      elif args.gpu is not None:\n",
        "          torch.cuda.set_device(args.gpu)\n",
        "          net.cuda(args.gpu)\n",
        "      else:\n",
        "          net = nn.DataParallel(net).cuda()\n",
        "\n",
        "\n",
        "      # ----------------------------------------------------------------------------------------\n",
        "      # Define criterion(s), optimizer(s), and scheduler(s)\n",
        "      # ----------------------------------------------------------------------------------------\n",
        "\n",
        "      # ------------Criterion------------\n",
        "      criterion = nn.L1Loss().cuda(args.gpu)\n",
        "      criterion_MSE = nn.MSELoss().cuda(args.gpu)\n",
        "\n",
        "      # ------------Optimizer------------\n",
        "      if args.optimizer == \"sgd\":\n",
        "          optimizer = optim.SGD(net.parameters(), lr = args.lr)\n",
        "      elif args.optimizer == \"adamW\":\n",
        "          optimizer = optim.AdamW(net.parameters(), lr = args.lr)\n",
        "      else: # Adam\n",
        "          optimizer = optim.Adam(net.parameters(), lr = args.lr)\n",
        "\n",
        "      # ------------Scheduler------------\n",
        "      if args.scheduler == \"decay-lr\":\n",
        "          scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.2)\n",
        "      elif args.scheduler == \"multiplicative-lr\":\n",
        "          lmbda = lambda epoch: 0.985\n",
        "          scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
        "      elif args.scheduler == \"cyclic-lr\":\n",
        "          scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr = args.base_lr, max_lr = args.lr, mode = 'triangular2', cycle_momentum = False)\n",
        "      elif args.scheduler == \"one-cycle-lr\":\n",
        "          scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr = args.lr, steps_per_epoch=len(train_loader), epochs=args.epochs, cycle_momentum = False)\n",
        "      else: # constant-lr\n",
        "          scheduler = None\n",
        "\n",
        "      print('Started Training')\n",
        "      print('Training Details:')\n",
        "      print('Network:         {}'.format(args.network))\n",
        "      print('Epochs:          {}'.format(args.epochs))\n",
        "      print('Batch Size:      {}'.format(args.batch_size))\n",
        "      print('Optimizer:       {}'.format(args.optimizer))\n",
        "      print('Scheduler:       {}'.format(args.scheduler))\n",
        "      print('Learning Rate:   {}'.format(args.lr))\n",
        "      print('Spectrum Length: {}'.format(args.spectrum_len))\n",
        "\n",
        "      date = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
        "\n",
        "      log_dir = \"runs/{}_{}_{}_{}_{}_{}x\".format(date, args.optimizer, args.scheduler, args.network, scale, fold + 1)\n",
        "      models_dir = \"{}_{}_{}_{}_{}_{}x.pt\".format(date, args.optimizer, args.scheduler, args.network, scale, fold + 1)\n",
        "\n",
        "      writer = SummaryWriter(log_dir = log_dir)\n",
        "\n",
        "      for epoch in range(args.epochs):\n",
        "          train_loss, train_psnr, train_ssim = train(train_loader, net, optimizer, scheduler, criterion, criterion_MSE, epoch, args)\n",
        "          valid_loss, valid_psnr, valid_ssim = validate(val_loader, net, criterion_MSE, args)\n",
        "          if args.scheduler != \"cyclic-lr\" and args.scheduler != \"one-cycle-lr\" and args.scheduler != \"constant-lr\":\n",
        "              scheduler.step()\n",
        "\n",
        "          writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "          writer.add_scalar('Loss/val', valid_loss, epoch)\n",
        "          writer.add_scalar('PSNR/train', train_psnr, epoch)\n",
        "          writer.add_scalar('PSNR/val', valid_psnr, epoch)\n",
        "          writer.add_scalar('SSIM/train', train_ssim, epoch)\n",
        "          writer.add_scalar('SSIM/val', valid_ssim, epoch)\n",
        "          print('Epoch {} done'.format(epoch))\n",
        "          print('Loss/train: {}'.format(train_loss))\n",
        "          print('Loss/val: {}'.format(valid_loss))\n",
        "          print('PSNR/train: {}'.format(train_psnr))\n",
        "          print('PSNR/val: {}'.format(valid_psnr))\n",
        "          print('SSIM/train: {}'.format(train_ssim))\n",
        "          print('SSIM/val: {}'.format(valid_ssim))\n",
        "\n",
        "\n",
        "      torch.save(net.state_dict(), models_dir)\n",
        "      print('Finished Training')"
      ],
      "metadata": {
        "id": "pxNZr11BjHvF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "qtYI_0qkGa3n",
        "outputId": "1002a142-fcf9-454a-b590-a76dab8040c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "O1sjKibTHrPN",
        "outputId": "f4a48bd7-74cc-419e-8cc6-8b03735e000d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset.py  model.py  ResUNet.pt  utilities.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J1ZBnz0lm1w",
        "outputId": "012a4d54-a2c8-4622-918c-efd1727319a3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/My\\ Drive/Colab\\ Notebooks/DeepeR-master/Hyperspectral Super-Resolution"
      ],
      "metadata": {
        "id": "RAHNfyHBGcGD",
        "outputId": "879cb7ea-e221-4130-e9a9-e66f08e74cc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/DeepeR-master/Hyperspectral Super-Resolution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_psnr(output, target):\n",
        "    psnr = 0.\n",
        "    mse = nn.MSELoss()(output, target)\n",
        "    psnr = 10 * math.log10(torch.max(output)/mse)\n",
        "    return psnr\n",
        "\n",
        "def calc_ssim(output, target):\n",
        "    ssim = 0.\n",
        "    output = output.cpu().detach().numpy()\n",
        "    target = target.cpu().detach().numpy()\n",
        "\n",
        "    if output.ndim == 4:\n",
        "        for i in range(output.shape[0]):\n",
        "            output_i = np.squeeze(output[i,:,:,:])\n",
        "            output_i = np.moveaxis(output_i, 0, -1)\n",
        "            target_i = np.squeeze(target[i,:,:,:])\n",
        "            target_i = np.moveaxis(target_i, 0, -1)\n",
        "            batch_size = output.shape[0]\n",
        "            ssim += sk_ssim(output_i, target_i, data_range = output_i.max() - target_i.max(), multichannel=True)\n",
        "    else:\n",
        "        output_i = np.squeeze(output)\n",
        "        output_i = np.moveaxis(output_i, 0, -1)\n",
        "        target_i = np.squeeze(target)\n",
        "        target_i = np.moveaxis(target_i, 0, -1)\n",
        "        batch_size = 1\n",
        "        ssim += sk_ssim(output_i, target_i, data_range = output_i.max() - target_i.max(), multichannel=True)\n",
        "\n",
        "    ssim = ssim / batch_size\n",
        "    return ssim"
      ],
      "metadata": {
        "id": "6sQ6cib9zsW5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Default args from original code\n",
        "\n",
        "class Arguments:\n",
        "    pass\n",
        "\n",
        "args = Arguments()\n",
        "args.workers = 0\n",
        "args.epochs = 200\n",
        "args.start_epoch = 0\n",
        "args.batch_size = 2\n",
        "args.network = \"Hyperspectral_RCAN\"\n",
        "args.lam = 100\n",
        "args.optimizer = \"adam\"\n",
        "args.lr = 1e-5\n",
        "args.base_lr = 1e-7\n",
        "args.scheduler = \"constant-lr\"\n",
        "args.lr_image_size = 16\n",
        "args.hr_image_size = 64\n",
        "args.batch_norm = True\n",
        "args.spectrum_len = 500\n",
        "args.seed = None\n",
        "args.gpu = 0\n",
        "args.world_size = -1\n",
        "args.rank = -1\n",
        "args.dist_url = \"tcp://224.66.41.62:23456\"\n",
        "args.dist_backend = \"nccl\"\n",
        "args.multiprocessing_distributed = False\n",
        "\n",
        "\n",
        "args.epochs=2\n",
        "# train_noKmeans(args)\n",
        "train_kmeans(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqEw59PrCzKG",
        "outputId": "38412d17-4612-4f27-e1bf-040da8ec4f15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: 0 for training\n",
            "Fold 1\n",
            "-------\n",
            "Started Training\n",
            "Training Details:\n",
            "Network:         Hyperspectral_RCAN\n",
            "Epochs:          2\n",
            "Batch Size:      2\n",
            "Optimizer:       adam\n",
            "Scheduler:       constant-lr\n",
            "Learning Rate:   1e-05\n",
            "Spectrum Length: 500\n",
            "Epoch: [0][  0/203]\tTime  8.108 ( 8.108)\tPSNR 16.2480 (16.2480)\tSSIM 0.0083 (0.0083)\n",
            "Epoch: [0][ 20/203]\tTime  7.491 ( 7.935)\tPSNR 20.0048 (18.4571)\tSSIM 0.5116 (0.2406)\n",
            "Epoch: [0][ 40/203]\tTime  2.601 ( 7.041)\tPSNR 20.5384 (19.2820)\tSSIM 0.5279 (0.3671)\n",
            "Epoch: [0][ 60/203]\tTime  3.924 ( 6.454)\tPSNR 22.5896 (20.4358)\tSSIM 0.6799 (0.4539)\n",
            "Epoch: [0][ 80/203]\tTime  4.132 ( 5.990)\tPSNR 24.4831 (21.1475)\tSSIM 0.7186 (0.5143)\n",
            "Epoch: [0][100/203]\tTime  2.781 ( 5.501)\tPSNR 26.1236 (21.5444)\tSSIM 0.6073 (0.5464)\n",
            "Epoch: [0][120/203]\tTime  1.934 ( 5.106)\tPSNR 24.7865 (21.7901)\tSSIM 0.7478 (0.5616)\n",
            "Epoch: [0][140/203]\tTime  2.555 ( 4.751)\tPSNR 26.1335 (22.0958)\tSSIM 0.7282 (0.5798)\n",
            "Epoch: [0][160/203]\tTime  2.552 ( 4.515)\tPSNR 29.0534 (22.5521)\tSSIM 0.8518 (0.5992)\n",
            "Epoch: [0][180/203]\tTime  2.614 ( 4.310)\tPSNR 24.1726 (22.8456)\tSSIM 0.7166 (0.6136)\n",
            "Epoch: [0][200/203]\tTime  2.440 ( 4.124)\tPSNR 23.0746 (23.0526)\tSSIM 0.7465 (0.6257)\n",
            "Validation: [ 0/51]\tTime  2.711 ( 2.711)\tPSNR 25.1021 (25.1021)\tSSIM 0.7665 (0.7665)\n",
            "Validation: [20/51]\tTime  1.740 ( 1.600)\tPSNR 21.9802 (26.0787)\tSSIM 0.7808 (0.7410)\n",
            "Validation: [40/51]\tTime  1.066 ( 1.615)\tPSNR 20.6055 (25.3265)\tSSIM 0.6462 (0.7261)\n",
            "Epoch 0 done\n",
            "Loss/train: 0.001204943273146062\n",
            "Loss/val: 0.0006846814075662918\n",
            "PSNR/train: 23.041722128370658\n",
            "PSNR/val: 25.15196911526407\n",
            "SSIM/train: 0.6260757177589054\n",
            "SSIM/val: 0.7294332624451068\n",
            "Epoch: [1][  0/203]\tTime  2.368 ( 2.368)\tPSNR 27.1557 (27.1557)\tSSIM 0.7587 (0.7587)\n",
            "Epoch: [1][ 20/203]\tTime  2.259 ( 2.423)\tPSNR 25.0090 (25.9022)\tSSIM 0.7837 (0.7434)\n",
            "Epoch: [1][ 40/203]\tTime  2.674 ( 2.426)\tPSNR 22.9056 (26.2321)\tSSIM 0.8134 (0.7579)\n",
            "Epoch: [1][ 60/203]\tTime  2.331 ( 2.368)\tPSNR 21.6700 (25.8084)\tSSIM 0.5401 (0.7516)\n",
            "Epoch: [1][ 80/203]\tTime  1.953 ( 2.380)\tPSNR 31.0063 (25.9903)\tSSIM 0.8176 (0.7556)\n",
            "Epoch: [1][100/203]\tTime  2.662 ( 2.371)\tPSNR 24.1686 (26.2195)\tSSIM 0.7684 (0.7558)\n",
            "Epoch: [1][120/203]\tTime  2.264 ( 2.362)\tPSNR 23.9626 (26.2587)\tSSIM 0.6830 (0.7530)\n",
            "Epoch: [1][140/203]\tTime  2.535 ( 2.361)\tPSNR 22.1397 (26.2998)\tSSIM 0.6794 (0.7536)\n",
            "Epoch: [1][160/203]\tTime  1.883 ( 2.361)\tPSNR 29.2687 (26.3574)\tSSIM 0.8408 (0.7536)\n",
            "Epoch: [1][180/203]\tTime  2.741 ( 2.351)\tPSNR 22.4434 (26.3443)\tSSIM 0.7152 (0.7534)\n",
            "Epoch: [1][200/203]\tTime  1.951 ( 2.337)\tPSNR 28.4995 (26.3614)\tSSIM 0.6883 (0.7542)\n",
            "Validation: [ 0/51]\tTime  2.029 ( 2.029)\tPSNR 27.6588 (27.6588)\tSSIM 0.7649 (0.7649)\n",
            "Validation: [20/51]\tTime  1.719 ( 1.655)\tPSNR 22.7104 (26.4298)\tSSIM 0.7626 (0.7761)\n",
            "Validation: [40/51]\tTime  1.806 ( 1.597)\tPSNR 24.6981 (26.5340)\tSSIM 0.8072 (0.7792)\n",
            "Epoch 1 done\n",
            "Loss/train: 0.0006224806683158134\n",
            "Loss/val: 0.000517902361239334\n",
            "PSNR/train: 26.39923134042087\n",
            "PSNR/val: 26.800227295106527\n",
            "SSIM/train: 0.7552337506955458\n",
            "SSIM/val: 0.7795611542145631\n",
            "Finished Training\n",
            "Fold 2\n",
            "-------\n",
            "Started Training\n",
            "Training Details:\n",
            "Network:         Hyperspectral_RCAN\n",
            "Epochs:          2\n",
            "Batch Size:      2\n",
            "Optimizer:       adam\n",
            "Scheduler:       constant-lr\n",
            "Learning Rate:   1e-05\n",
            "Spectrum Length: 500\n",
            "Epoch: [0][  0/203]\tTime  2.420 ( 2.420)\tPSNR 15.6179 (15.6179)\tSSIM 0.0048 (0.0048)\n",
            "Epoch: [0][ 20/203]\tTime  2.800 ( 2.424)\tPSNR 13.5197 (18.2643)\tSSIM 0.1501 (0.2824)\n",
            "Epoch: [0][ 40/203]\tTime  2.551 ( 2.387)\tPSNR 20.3460 (19.2538)\tSSIM 0.6553 (0.4110)\n",
            "Epoch: [0][ 60/203]\tTime  2.090 ( 2.381)\tPSNR 21.2581 (20.0541)\tSSIM 0.5666 (0.4857)\n",
            "Epoch: [0][ 80/203]\tTime  2.028 ( 2.387)\tPSNR 23.3208 (20.5915)\tSSIM 0.6591 (0.5327)\n",
            "Epoch: [0][100/203]\tTime  2.063 ( 2.383)\tPSNR 20.5304 (21.0901)\tSSIM 0.6610 (0.5686)\n",
            "Epoch: [0][120/203]\tTime  2.107 ( 2.375)\tPSNR 20.2738 (21.3348)\tSSIM 0.6293 (0.5857)\n",
            "Epoch: [0][140/203]\tTime  1.900 ( 2.377)\tPSNR 25.9436 (21.7823)\tSSIM 0.8098 (0.6062)\n",
            "Epoch: [0][160/203]\tTime  2.371 ( 2.377)\tPSNR 27.2734 (22.2766)\tSSIM 0.6210 (0.6257)\n",
            "Epoch: [0][180/203]\tTime  1.899 ( 2.389)\tPSNR 23.1292 (22.5120)\tSSIM 0.6660 (0.6339)\n",
            "Epoch: [0][200/203]\tTime  2.082 ( 2.384)\tPSNR 29.2601 (22.8506)\tSSIM 0.8154 (0.6454)\n",
            "Validation: [ 0/51]\tTime  1.130 ( 1.130)\tPSNR 26.7262 (26.7262)\tSSIM 0.8067 (0.8067)\n",
            "Validation: [20/51]\tTime  2.033 ( 1.543)\tPSNR 27.4156 (24.7337)\tSSIM 0.7878 (0.7112)\n",
            "Validation: [40/51]\tTime  2.637 ( 1.562)\tPSNR 27.7803 (24.9027)\tSSIM 0.7080 (0.7204)\n",
            "Epoch 0 done\n",
            "Loss/train: 0.0011381078557108448\n",
            "Loss/val: 0.0008391563483290191\n",
            "PSNR/train: 22.87980330413163\n",
            "PSNR/val: 24.90278428151989\n",
            "SSIM/train: 0.646739983821975\n",
            "SSIM/val: 0.7192023325429798\n",
            "Epoch: [1][  0/203]\tTime  2.066 ( 2.066)\tPSNR 25.9030 (25.9030)\tSSIM 0.7744 (0.7744)\n",
            "Epoch: [1][ 20/203]\tTime  2.607 ( 2.337)\tPSNR 21.8930 (25.8511)\tSSIM 0.7440 (0.7586)\n",
            "Epoch: [1][ 40/203]\tTime  2.824 ( 2.322)\tPSNR 23.1183 (25.2910)\tSSIM 0.7985 (0.7523)\n",
            "Epoch: [1][ 60/203]\tTime  2.300 ( 2.296)\tPSNR 31.1830 (25.7276)\tSSIM 0.8135 (0.7564)\n",
            "Epoch: [1][ 80/203]\tTime  2.379 ( 2.321)\tPSNR 24.2557 (25.8207)\tSSIM 0.7464 (0.7630)\n",
            "Epoch: [1][100/203]\tTime  2.321 ( 2.368)\tPSNR 31.6841 (25.8620)\tSSIM 0.8827 (0.7645)\n",
            "Epoch: [1][120/203]\tTime  2.066 ( 2.367)\tPSNR 29.9645 (26.0091)\tSSIM 0.6475 (0.7635)\n",
            "Epoch: [1][140/203]\tTime  2.116 ( 2.362)\tPSNR 22.0684 (26.1582)\tSSIM 0.7610 (0.7632)\n",
            "Epoch: [1][160/203]\tTime  2.223 ( 2.360)\tPSNR 30.3273 (26.1600)\tSSIM 0.8724 (0.7593)\n",
            "Epoch: [1][180/203]\tTime  2.538 ( 2.349)\tPSNR 30.9525 (26.4223)\tSSIM 0.8034 (0.7661)\n",
            "Epoch: [1][200/203]\tTime  2.515 ( 2.352)\tPSNR 30.9734 (26.4772)\tSSIM 0.8662 (0.7671)\n",
            "Validation: [ 0/51]\tTime  1.283 ( 1.283)\tPSNR 27.3646 (27.3646)\tSSIM 0.6930 (0.6930)\n",
            "Validation: [20/51]\tTime  1.226 ( 1.542)\tPSNR 31.7605 (27.2870)\tSSIM 0.7702 (0.7272)\n",
            "Validation: [40/51]\tTime  1.429 ( 1.553)\tPSNR 24.4881 (27.4150)\tSSIM 0.8297 (0.7472)\n",
            "Epoch 1 done\n",
            "Loss/train: 0.0005504655109195292\n",
            "Loss/val: 0.0005612179540513156\n",
            "PSNR/train: 26.454176046506287\n",
            "PSNR/val: 27.48761914235313\n",
            "SSIM/train: 0.7661872155978612\n",
            "SSIM/val: 0.7391836142967492\n",
            "Finished Training\n",
            "Fold 3\n",
            "-------\n",
            "Started Training\n",
            "Training Details:\n",
            "Network:         Hyperspectral_RCAN\n",
            "Epochs:          2\n",
            "Batch Size:      2\n",
            "Optimizer:       adam\n",
            "Scheduler:       constant-lr\n",
            "Learning Rate:   1e-05\n",
            "Spectrum Length: 500\n",
            "Epoch: [0][  0/203]\tTime  2.008 ( 2.008)\tPSNR 15.3142 (15.3142)\tSSIM 0.0018 (0.0018)\n",
            "Epoch: [0][ 20/203]\tTime  2.047 ( 2.439)\tPSNR 17.3209 (18.3897)\tSSIM 0.2881 (0.2513)\n",
            "Epoch: [0][ 40/203]\tTime  1.995 ( 2.397)\tPSNR 21.7338 (19.2522)\tSSIM 0.6436 (0.3891)\n",
            "Epoch: [0][ 60/203]\tTime  1.982 ( 2.369)\tPSNR 23.6779 (20.0175)\tSSIM 0.6774 (0.4668)\n",
            "Epoch: [0][ 80/203]\tTime  2.463 ( 2.375)\tPSNR 20.2055 (20.6525)\tSSIM 0.5374 (0.5195)\n",
            "Epoch: [0][100/203]\tTime  2.033 ( 2.385)\tPSNR 23.8850 (21.1001)\tSSIM 0.6706 (0.5561)\n",
            "Epoch: [0][120/203]\tTime  3.290 ( 2.383)\tPSNR 25.7326 (21.5652)\tSSIM 0.7717 (0.5847)\n",
            "Epoch: [0][140/203]\tTime  2.886 ( 2.383)\tPSNR 24.6690 (21.9252)\tSSIM 0.7433 (0.6006)\n",
            "Epoch: [0][160/203]\tTime  2.316 ( 2.389)\tPSNR 24.2994 (22.2398)\tSSIM 0.7230 (0.6134)\n",
            "Epoch: [0][180/203]\tTime  2.582 ( 2.374)\tPSNR 25.5789 (22.6296)\tSSIM 0.7725 (0.6257)\n",
            "Epoch: [0][200/203]\tTime  1.958 ( 2.380)\tPSNR 22.1448 (22.8734)\tSSIM 0.6706 (0.6373)\n",
            "Validation: [ 0/51]\tTime  1.321 ( 1.321)\tPSNR 30.0790 (30.0790)\tSSIM 0.7979 (0.7979)\n",
            "Validation: [20/51]\tTime  1.330 ( 1.479)\tPSNR 27.3728 (24.7571)\tSSIM 0.7837 (0.7293)\n",
            "Validation: [40/51]\tTime  1.574 ( 1.488)\tPSNR 19.5256 (25.1498)\tSSIM 0.6575 (0.7399)\n",
            "Epoch 0 done\n",
            "Loss/train: 0.0011602782389422044\n",
            "Loss/val: 0.0006952258783604692\n",
            "PSNR/train: 22.883208889950538\n",
            "PSNR/val: 25.15149662771313\n",
            "SSIM/train: 0.6380554757428157\n",
            "SSIM/val: 0.7443243073163399\n",
            "Epoch: [1][  0/203]\tTime  2.100 ( 2.100)\tPSNR 22.0581 (22.0581)\tSSIM 0.7172 (0.7172)\n",
            "Epoch: [1][ 20/203]\tTime  2.081 ( 2.329)\tPSNR 30.5878 (25.0091)\tSSIM 0.8390 (0.7192)\n",
            "Epoch: [1][ 40/203]\tTime  2.384 ( 2.301)\tPSNR 20.9789 (24.9869)\tSSIM 0.6715 (0.7359)\n",
            "Epoch: [1][ 60/203]\tTime  2.429 ( 2.324)\tPSNR 24.6012 (25.4311)\tSSIM 0.7240 (0.7384)\n",
            "Epoch: [1][ 80/203]\tTime  3.148 ( 2.377)\tPSNR 23.6049 (25.3348)\tSSIM 0.7154 (0.7310)\n",
            "Epoch: [1][100/203]\tTime  1.912 ( 2.342)\tPSNR 28.2073 (25.3850)\tSSIM 0.6664 (0.7307)\n",
            "Epoch: [1][120/203]\tTime  2.121 ( 2.354)\tPSNR 29.9900 (25.6671)\tSSIM 0.8237 (0.7379)\n",
            "Epoch: [1][140/203]\tTime  2.127 ( 2.343)\tPSNR 27.0002 (25.8410)\tSSIM 0.7810 (0.7391)\n",
            "Epoch: [1][160/203]\tTime  1.809 ( 2.336)\tPSNR 23.6922 (26.0339)\tSSIM 0.6446 (0.7413)\n",
            "Epoch: [1][180/203]\tTime  2.402 ( 2.347)\tPSNR 29.7691 (26.2894)\tSSIM 0.8424 (0.7458)\n",
            "Epoch: [1][200/203]\tTime  2.155 ( 2.353)\tPSNR 23.3641 (26.3838)\tSSIM 0.6457 (0.7493)\n",
            "Validation: [ 0/51]\tTime  2.035 ( 2.035)\tPSNR 23.9616 (23.9616)\tSSIM 0.7188 (0.7188)\n",
            "Validation: [20/51]\tTime  1.529 ( 1.766)\tPSNR 29.4610 (26.9991)\tSSIM 0.8558 (0.7582)\n",
            "Validation: [40/51]\tTime  1.558 ( 1.596)\tPSNR 21.7252 (27.0954)\tSSIM 0.6600 (0.7677)\n",
            "Epoch 1 done\n",
            "Loss/train: 0.0006147104811085058\n",
            "Loss/val: 0.0004723348364056466\n",
            "PSNR/train: 26.377821340149513\n",
            "PSNR/val: 27.355921013984315\n",
            "SSIM/train: 0.7494948763470114\n",
            "SSIM/val: 0.7720261879078588\n",
            "Finished Training\n",
            "Fold 4\n",
            "-------\n",
            "Started Training\n",
            "Training Details:\n",
            "Network:         Hyperspectral_RCAN\n",
            "Epochs:          2\n",
            "Batch Size:      2\n",
            "Optimizer:       adam\n",
            "Scheduler:       constant-lr\n",
            "Learning Rate:   1e-05\n",
            "Spectrum Length: 500\n",
            "Epoch: [0][  0/203]\tTime  2.382 ( 2.382)\tPSNR 17.1870 (17.1870)\tSSIM 0.0010 (0.0010)\n",
            "Epoch: [0][ 20/203]\tTime  2.269 ( 2.436)\tPSNR 18.6865 (18.3860)\tSSIM 0.3802 (0.2634)\n",
            "Epoch: [0][ 40/203]\tTime  2.492 ( 2.373)\tPSNR 22.0065 (19.8422)\tSSIM 0.6440 (0.4129)\n",
            "Epoch: [0][ 60/203]\tTime  2.220 ( 2.369)\tPSNR 24.9031 (20.8724)\tSSIM 0.7502 (0.4906)\n",
            "Epoch: [0][ 80/203]\tTime  1.983 ( 2.340)\tPSNR 21.2304 (21.4389)\tSSIM 0.6119 (0.5306)\n",
            "Epoch: [0][100/203]\tTime  2.661 ( 2.318)\tPSNR 23.6480 (22.1330)\tSSIM 0.7748 (0.5646)\n",
            "Epoch: [0][120/203]\tTime  1.898 ( 2.295)\tPSNR 27.4816 (22.6497)\tSSIM 0.7685 (0.5865)\n",
            "Epoch: [0][140/203]\tTime  2.381 ( 2.300)\tPSNR 22.8147 (22.8088)\tSSIM 0.7548 (0.6026)\n",
            "Epoch: [0][160/203]\tTime  2.007 ( 2.303)\tPSNR 22.2560 (23.0127)\tSSIM 0.7351 (0.6141)\n",
            "Epoch: [0][180/203]\tTime  2.073 ( 2.315)\tPSNR 22.7651 (23.1264)\tSSIM 0.6777 (0.6239)\n",
            "Epoch: [0][200/203]\tTime  2.137 ( 2.309)\tPSNR 25.3158 (23.4201)\tSSIM 0.8061 (0.6355)\n",
            "Validation: [ 0/51]\tTime  1.429 ( 1.429)\tPSNR 24.4960 (24.4960)\tSSIM 0.7496 (0.7496)\n",
            "Validation: [20/51]\tTime  1.207 ( 1.563)\tPSNR 29.6543 (24.5371)\tSSIM 0.8278 (0.7588)\n",
            "Validation: [40/51]\tTime  1.997 ( 1.558)\tPSNR 23.0813 (24.5167)\tSSIM 0.7238 (0.7476)\n",
            "Epoch 0 done\n",
            "Loss/train: 0.00111617896396333\n",
            "Loss/val: 0.0007043482364319025\n",
            "PSNR/train: 23.43367829393187\n",
            "PSNR/val: 24.731177161409207\n",
            "SSIM/train: 0.6370758531253929\n",
            "SSIM/val: 0.7521248104735085\n",
            "Epoch: [1][  0/203]\tTime  2.151 ( 2.151)\tPSNR 23.1596 (23.1596)\tSSIM 0.7715 (0.7715)\n",
            "Epoch: [1][ 20/203]\tTime  2.135 ( 2.223)\tPSNR 29.5775 (25.7333)\tSSIM 0.7264 (0.7366)\n",
            "Epoch: [1][ 40/203]\tTime  2.032 ( 2.319)\tPSNR 29.4496 (25.8544)\tSSIM 0.8461 (0.7359)\n",
            "Epoch: [1][ 60/203]\tTime  2.078 ( 2.341)\tPSNR 28.5227 (26.1086)\tSSIM 0.7457 (0.7503)\n",
            "Epoch: [1][ 80/203]\tTime  2.774 ( 2.342)\tPSNR 24.5689 (26.4038)\tSSIM 0.5990 (0.7527)\n",
            "Epoch: [1][100/203]\tTime  2.880 ( 2.341)\tPSNR 22.8187 (26.1081)\tSSIM 0.6269 (0.7449)\n",
            "Epoch: [1][120/203]\tTime  2.350 ( 2.319)\tPSNR 30.1374 (26.3911)\tSSIM 0.7937 (0.7484)\n",
            "Epoch: [1][140/203]\tTime  2.440 ( 2.301)\tPSNR 21.1927 (26.3477)\tSSIM 0.5280 (0.7487)\n",
            "Epoch: [1][160/203]\tTime  1.907 ( 2.308)\tPSNR 28.7779 (26.5472)\tSSIM 0.7866 (0.7509)\n",
            "Epoch: [1][180/203]\tTime  2.127 ( 2.314)\tPSNR 28.7709 (26.6214)\tSSIM 0.6916 (0.7508)\n",
            "Epoch: [1][200/203]\tTime  1.921 ( 2.313)\tPSNR 25.1672 (26.6139)\tSSIM 0.5998 (0.7492)\n",
            "Validation: [ 0/51]\tTime  2.533 ( 2.533)\tPSNR 25.8484 (25.8484)\tSSIM 0.7874 (0.7874)\n",
            "Validation: [20/51]\tTime  1.656 ( 1.607)\tPSNR 25.7903 (26.6638)\tSSIM 0.8227 (0.7410)\n",
            "Validation: [40/51]\tTime  1.168 ( 1.518)\tPSNR 23.3990 (26.2871)\tSSIM 0.8099 (0.7445)\n",
            "Epoch 1 done\n",
            "Loss/train: 0.0006240966919496463\n",
            "Loss/val: 0.0006087605873443242\n",
            "PSNR/train: 26.60667295491114\n",
            "PSNR/val: 26.486619841840632\n",
            "SSIM/train: 0.7486578702517386\n",
            "SSIM/val: 0.7538313482207283\n",
            "Finished Training\n",
            "Fold 5\n",
            "-------\n",
            "Started Training\n",
            "Training Details:\n",
            "Network:         Hyperspectral_RCAN\n",
            "Epochs:          2\n",
            "Batch Size:      2\n",
            "Optimizer:       adam\n",
            "Scheduler:       constant-lr\n",
            "Learning Rate:   1e-05\n",
            "Spectrum Length: 500\n",
            "Epoch: [0][  0/203]\tTime  3.472 ( 3.472)\tPSNR 16.4438 (16.4438)\tSSIM 0.0102 (0.0102)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "def evaluate(dataloader, net, scale, args):\n",
        "\n",
        "    psnr = AverageMeter('PSNR', ':.4f')\n",
        "    ssim = AverageMeter('SSIM', ':.4f')\n",
        "    mse_NN = AverageMeter('MSE', ':.4f')\n",
        "    psnr_bicubic = AverageMeter('PSNR_Bicubic', ':.4f')\n",
        "    ssim_bicubic = AverageMeter('SSIM_Bicubic', ':.4f')\n",
        "    mse_bicubic = AverageMeter('MSE_Bicubic', ':.4f')\n",
        "    psnr_nearest_neighbours = AverageMeter('PSNR_Nearest_Neighbours', ':.4f')\n",
        "    ssim_nearest_neighbours = AverageMeter('SSIM_Nearest_Neighbours', ':.4f')\n",
        "    mse_nearest_neighbours = AverageMeter('MSE_Nearest_Neighbours', ':.4f')\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(dataloader):\n",
        "            # measure data loading time\n",
        "            x = data['input_image']\n",
        "            inputs = x.float()\n",
        "            inputs = inputs.cuda(args.gpu)\n",
        "            y = data['output_image']\n",
        "            target = y.float()\n",
        "            target = target.cuda(args.gpu)\n",
        "\n",
        "            # compute output\n",
        "            output = net(inputs)\n",
        "\n",
        "            x2 = np.squeeze(x.numpy())\n",
        "            y2 = np.squeeze(y.numpy())\n",
        "\n",
        "            nearest_neighbours = scipy.ndimage.zoom(x2,(1,scale,scale), order=0)\n",
        "            bicubic = scipy.ndimage.zoom(x2,(1,scale,scale), order=3)\n",
        "\n",
        "            bicubic = torch.from_numpy(bicubic)\n",
        "            bicubic = bicubic.cuda(args.gpu)\n",
        "\n",
        "            nearest_neighbours = torch.from_numpy(nearest_neighbours)\n",
        "            nearest_neighbours = nearest_neighbours.cuda(args.gpu)\n",
        "\n",
        "            # Nearest neighbours\n",
        "            psnr_batch_nearest_neighbours = calc_psnr(nearest_neighbours, target)\n",
        "            psnr_nearest_neighbours.update(psnr_batch_nearest_neighbours, inputs.size(0))\n",
        "\n",
        "            ssim_batch_nearest_neighbours = calc_ssim(nearest_neighbours, target)\n",
        "            ssim_nearest_neighbours.update(ssim_batch_nearest_neighbours, inputs.size(0))\n",
        "\n",
        "            mse_batch_nearest_neighbours = nn.MSELoss()(nearest_neighbours, target)\n",
        "            mse_nearest_neighbours.update(mse_batch_nearest_neighbours, inputs.size(0))\n",
        "\n",
        "            # Bicubic\n",
        "            psnr_batch_bicubic = calc_psnr(bicubic, target)\n",
        "            psnr_bicubic.update(psnr_batch_bicubic, inputs.size(0))\n",
        "\n",
        "            ssim_batch_bicubic = calc_ssim(bicubic, target)\n",
        "            ssim_bicubic.update(ssim_batch_bicubic, inputs.size(0))\n",
        "\n",
        "            mse_batch_bicubic = nn.MSELoss()(bicubic, target)\n",
        "            mse_bicubic.update(mse_batch_bicubic, inputs.size(0))\n",
        "\n",
        "            # Neural network\n",
        "            psnr_batch = calc_psnr(output, target)\n",
        "            psnr.update(psnr_batch, inputs.size(0))\n",
        "\n",
        "            ssim_batch = calc_ssim(output, target)\n",
        "            ssim.update(ssim_batch, inputs.size(0))\n",
        "\n",
        "            mse_batch = nn.MSELoss()(output, target)\n",
        "            mse_NN.update(mse_batch, inputs.size(0))\n",
        "\n",
        "    print(\"RCAN PSNR: {}    Bicubic PSNR: {}    Nearest Neighbours PSNR: {}\".format(psnr.avg, psnr_bicubic.avg, psnr_nearest_neighbours.avg))\n",
        "    print(\"RCAN SSIM: {}    Bicubic SSIM: {}    Nearest Neighbours SSIM: {}\".format(ssim.avg, ssim_bicubic.avg, ssim_nearest_neighbours.avg))\n",
        "    print(\"RCAN MSE:  {}    Bicubic MSE:  {}    Nearest Neighbours MSE:  {}\".format(mse_NN.avg, mse_bicubic.avg, mse_nearest_neighbours.avg))\n",
        "    return psnr.avg, psnr_bicubic.avg, psnr_nearest_neighbours.avg, ssim.avg, ssim_bicubic.avg, ssim_nearest_neighbours.avg, mse_NN.avg, mse_bicubic.avg, mse_nearest_neighbours.avg"
      ],
      "metadata": {
        "id": "8ujeP58VmTxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_test(args):\n",
        "\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "\n",
        "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "    ngpus_per_node = torch.cuda.device_count()\n",
        "\n",
        "    gpu = args.gpu\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create model(s) and send to device(s)\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    scale = args.hr_image_size // args.lr_image_size\n",
        "    net = Hyperspectral_RCAN(args.spectrum_len, scale).float()\n",
        "\n",
        "    net.load_state_dict(torch.load(args.model))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.gpu is not None:\n",
        "            torch.cuda.set_device(args.gpu)\n",
        "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.gpu])\n",
        "        else:\n",
        "            net.cuda(args.gpu)\n",
        "            net = torch.nn.parallel.DistributedDataParallel(net)\n",
        "    elif args.gpu is not None:\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "        net.cuda(args.gpu)\n",
        "    else:\n",
        "        net = nn.DataParallel(net).cuda()\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Define dataset path and data splits\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    dataset_path = \"Dataset/\"\n",
        "    image_ids_csv = pd.read_csv(dataset_path + \"Image_IDs.csv\")\n",
        "\n",
        "    image_ids = image_ids_csv[\"id\"].values\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Create datasets and dataloaders\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    Raman_Dataset_Test = RamanImageDataset(image_ids, dataset_path, batch_size = args.batch_size,\n",
        "                                                    hr_image_size = args.hr_image_size, lr_image_size = args.lr_image_size,\n",
        "                                                    spectrum_len = args.spectrum_len)\n",
        "\n",
        "    test_loader = DataLoader(Raman_Dataset_Test, batch_size = args.batch_size, shuffle = False, num_workers = args.workers)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    # Evaluate\n",
        "    # ----------------------------------------------------------------------------------------\n",
        "    RCAN_PSNR, Bicubic_PSNR, Nearest_PSNR, RCAN_SSIM, Bicubic_SSIM, Nearest_SSIM, RCAN_MSE, Bicubic_MSE, Nearest_MSE = evaluate(test_loader, net, scale, args)"
      ],
      "metadata": {
        "id": "yqtQlVwwK3GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Arguments:\n",
        "    pass\n",
        "\n",
        "args = Arguments()\n",
        "args.workers = 0\n",
        "args.batch_size = 1\n",
        "args.spectrum_len = 500\n",
        "args.network = \"Hyperspectral_RCAN\"\n",
        "args.lr_image_size = 16\n",
        "args.hr_image_size = 64\n",
        "args.spectrum_len = 500\n",
        "args.seed = None\n",
        "args.gpu = 0\n",
        "args.world_size = -1\n",
        "args.rank = -1\n",
        "args.dist_url = \"tcp://224.66.41.62:23456\"\n",
        "args.dist_backend = \"nccl\"\n",
        "args.multiprocessing_distributed = False\n",
        "args.batch_norm = True\n",
        "args.model = \"2024_05_22_adam_constant-lr_Hyperspectral_RCAN_4x.pt\"\n",
        "\n",
        "\n",
        "main_test(args)"
      ],
      "metadata": {
        "id": "I97mxHSasvCS",
        "outputId": "51579610-7f34-4ab3-eb83-d6f77d1a3f6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: 0 for testing\n",
            "Neural Network MSE: 0.00260821853749535\n",
            "Neural Network PSNR: 26.159036795137737\n",
            "Neural Network SSIM: 0.2734942460924173\n",
            "Savitzky-Golay MSE: 0.027660622850368643\n",
            "Neural Network performed 10.61x better than Savitzky-Golay\n"
          ]
        }
      ]
    }
  ]
}